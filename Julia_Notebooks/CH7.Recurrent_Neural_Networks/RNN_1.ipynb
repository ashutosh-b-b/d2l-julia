{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Sequences\n",
    "\n",
    "Up until now, we have focused on models whose inputs\n",
    "consisted of a single feature vector $\\mathbf{x} \\in \\mathbb{R}^d$.\n",
    "The main change of perspective when developing models\n",
    "capable of processing sequences is that we now\n",
    "focus on inputs that consist of an ordered list\n",
    "of feature vectors $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$,\n",
    "where each feature vector $\\mathbf{x}_t$ is\n",
    "indexed by a time step $t \\in \\mathbb{Z}^+$\n",
    "lying in $\\mathbb{R}^d$.\n",
    "\n",
    "Some datasets consist of a single massive sequence.\n",
    "Consider, for example, the extremely long streams\n",
    "of sensor readings that might be available to climate scientists.\n",
    "In such cases, we might create training datasets\n",
    "by randomly sampling subsequences of some predetermined length.\n",
    "More often, our data arrives as a collection of sequences.\n",
    "Consider the following examples:\n",
    "(i) a collection of documents,\n",
    "each represented as its own sequence of words,\n",
    "and each having its own length $T_i$;\n",
    "(ii) sequence representation of\n",
    "patient stays in the hospital,\n",
    "where each stay consists of a number of events\n",
    "and the sequence length depends roughly\n",
    "on the length of the stay.\n",
    "\n",
    "\n",
    "Previously, when dealing with individual inputs,\n",
    "we assumed that they were sampled independently\n",
    "from the same underlying distribution $P(X)$.\n",
    "While we still assume that entire sequences\n",
    "(e.g., entire documents or patient trajectories)\n",
    "are sampled independently,\n",
    "we cannot assume that the data arriving\n",
    "at each time step are independent of each other.\n",
    "For example, the words that likely to appear later in a document\n",
    "depend heavily on words occurring earlier in the document.\n",
    "The medicine a patient is likely to receive\n",
    "on the 10th day of a hospital visit\n",
    "depends heavily on what transpired\n",
    "in the previous nine days.\n",
    "\n",
    "This should come as no surprise.\n",
    "If we did not believe that the elements in a sequence were related,\n",
    "we would not have bothered to model them as a sequence in the first place.\n",
    "Consider the usefulness of the auto-fill features\n",
    "that are popular on search tools and modern email clients.\n",
    "They are useful precisely because it is often possible\n",
    "to predict (imperfectly, but better than random guessing)\n",
    "what the likely continuations of a sequence might be,\n",
    "given some initial prefix.\n",
    "For most sequence models,\n",
    "we do not require independence,\n",
    "or even stationarity, of our sequences.\n",
    "Instead, we require only that\n",
    "the sequences themselves are sampled\n",
    "from some fixed underlying distribution\n",
    "over entire sequences.\n",
    "\n",
    "This flexible approach allows for such phenomena\n",
    "as (i) documents looking significantly different\n",
    "at the beginning than at the end;\n",
    "or (ii) patient status evolving either\n",
    "towards recovery or towards death\n",
    "over the course of a hospital stay;\n",
    "or (iii) customer taste evolving in predictable ways\n",
    "over the course of continued interaction with a recommender system.\n",
    "\n",
    "\n",
    "We sometimes wish to predict a fixed target $y$\n",
    "given sequentially structured input\n",
    "(e.g., sentiment classification based on a movie review).\n",
    "At other times, we wish to predict a sequentially structured target\n",
    "($y_1, \\ldots, y_T$)\n",
    "given a fixed input (e.g., image captioning).\n",
    "Still other times, our goal is to predict sequentially structured targets\n",
    "based on sequentially structured inputs\n",
    "(e.g., machine translation or video captioning).\n",
    "Such sequence-to-sequence tasks take two forms:\n",
    "(i) *aligned*: where the input at each time step\n",
    "aligns with a corresponding target (e.g., part of speech tagging);\n",
    "(ii) *unaligned*: where the input and target\n",
    "do not necessarily exhibit a step-for-step correspondence\n",
    "(e.g., machine translation).\n",
    "\n",
    "Before we worry about handling targets of any kind,\n",
    "we can tackle the most straightforward problem:\n",
    "unsupervised density modeling (also called *sequence modeling*).\n",
    "Here, given a collection of sequences,\n",
    "our goal is to estimate the probability mass function\n",
    "that tells us how likely we are to see any given sequence,\n",
    "i.e., $p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Projects/D2L/d2lai`\n"
     ]
    },
    {
     "ename": "InitError",
     "evalue": "InitError: KeyError: key Base.PkgId(Base.UUID(\"033835bb-8acc-5ee8-8aae-3f567f8a3819\"), \"JLD2\") not found\nduring initialization of module JLD2",
     "output_type": "error",
     "traceback": [
      "InitError: KeyError: key Base.PkgId(Base.UUID(\"033835bb-8acc-5ee8-8aae-3f567f8a3819\"), \"JLD2\") not found\n",
      "during initialization of module JLD2\n",
      "\n",
      "Stacktrace:\n",
      "  [1] getindex\n",
      "    @ ./dict.jl:477 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ ./lock.jl:273 [inlined]\n",
      "  [3] root_module(key::Base.PkgId)\n",
      "    @ Base ./loading.jl:2471\n",
      "  [4] parse_pkg_files(id::Base.PkgId)\n",
      "    @ Revise ~/.julia/packages/Revise/FaTes/src/loading.jl:38\n",
      "  [5] watch_package(id::Base.PkgId)\n",
      "    @ Revise ~/.julia/packages/Revise/FaTes/src/pkgs.jl:349\n",
      "  [6] add_require(sourcefile::String, modcaller::Module, idmod::String, modname::String, expr::Expr)\n",
      "    @ Revise ~/.julia/packages/Revise/FaTes/src/pkgs.jl:188\n",
      "  [7] withnotifications(::Any, ::Vararg{Any})\n",
      "    @ Requires ~/.julia/packages/Requires/Z8rfN/src/require.jl:70\n",
      "  [8] (::JLD2.var\"#134#137\")()\n",
      "    @ JLD2 ~/.julia/packages/Requires/Z8rfN/src/require.jl:106\n",
      "  [9] listenpkg(f::Any, pkg::Base.PkgId)\n",
      "    @ Requires ~/.julia/packages/Requires/Z8rfN/src/require.jl:20\n",
      " [10] macro expansion\n",
      "    @ ~/.julia/packages/Requires/Z8rfN/src/require.jl:98 [inlined]\n",
      " [11] __init__()\n",
      "    @ JLD2 ~/.julia/packages/JLD2/OP0XX/src/JLD2.jl:523\n",
      " [12] run_module_init(mod::Module, i::Int64)\n",
      "    @ Base ./loading.jl:1378\n",
      " [13] register_restored_modules(sv::Core.SimpleVector, pkg::Base.PkgId, path::String)\n",
      "    @ Base ./loading.jl:1366\n",
      " [14] _include_from_serialized(pkg::Base.PkgId, path::String, ocachepath::String, depmods::Vector{Any}, ignore_native::Nothing; register::Bool)\n",
      "    @ Base ./loading.jl:1254\n",
      " [15] _include_from_serialized (repeats 2 times)\n",
      "    @ ./loading.jl:1210 [inlined]\n",
      " [16] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128, stalecheck::Bool; reasons::Dict{String, Int64}, DEPOT_PATH::Vector{String})\n",
      "    @ Base ./loading.jl:2041\n",
      " [17] _require(pkg::Base.PkgId, env::String)\n",
      "    @ Base ./loading.jl:2527\n",
      " [18] __require_prelocked(uuidkey::Base.PkgId, env::String)\n",
      "    @ Base ./loading.jl:2388\n",
      " [19] #invoke_in_world#3\n",
      "    @ ./essentials.jl:1089 [inlined]\n",
      " [20] invoke_in_world\n",
      "    @ ./essentials.jl:1086 [inlined]\n",
      " [21] _require_prelocked(uuidkey::Base.PkgId, env::String)\n",
      "    @ Base ./loading.jl:2375\n",
      " [22] macro expansion\n",
      "    @ ./loading.jl:2314 [inlined]\n",
      " [23] macro expansion\n",
      "    @ ./lock.jl:273 [inlined]\n",
      " [24] __require(into::Module, mod::Symbol)\n",
      "    @ Base ./loading.jl:2271\n",
      " [25] #invoke_in_world#3\n",
      "    @ ./essentials.jl:1089 [inlined]\n",
      " [26] invoke_in_world\n",
      "    @ ./essentials.jl:1086 [inlined]\n",
      " [27] require(into::Module, mod::Symbol)\n",
      "    @ Base ./loading.jl:2260\n",
      " [28] eval\n",
      "    @ ./boot.jl:430 [inlined]\n",
      " [29] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:2734\n",
      " [30] #invokelatest#2\n",
      "    @ ./essentials.jl:1055 [inlined]\n",
      " [31] invokelatest\n",
      "    @ ./essentials.jl:1052 [inlined]\n",
      " [32] (::VSCodeServer.var\"#217#218\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.124.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:24\n",
      " [33] withpath(f::VSCodeServer.var\"#217#218\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.124.2/scripts/packages/VSCodeServer/src/repl.jl:276\n",
      " [34] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.124.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:13\n",
      " [35] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.124.2/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [36] serve_notebook(pipename::String, debugger_pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; error_handler::var\"#5#10\"{String})\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.124.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:147\n",
      " [37] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.124.2/scripts/notebook/notebook.jl:35"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"../../d2lai\")\n",
    "using d2lai\n",
    "using Flux \n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Models\n",
    "\n",
    "\n",
    "Before introducing specialized neural networks\n",
    "designed to handle sequentially structured data,\n",
    "let's take a look at some actual sequence data\n",
    "and build up some basic intuitions and statistical tools.\n",
    "In particular, we will focus on stock price data\n",
    "from the FTSE 100 index (:numref:`fig_ftse100`).\n",
    "At each *time step* $t \\in \\mathbb{Z}^+$, we observe\n",
    "the price, $x_t$, of the index at that time.\n",
    "\n",
    "\n",
    "![FTSE 100 index over about 30 years.](../img/ftse100.png)\n",
    ":width:`400px`\n",
    ":label:`fig_ftse100`\n",
    "\n",
    "\n",
    "Now suppose that a trader would like to make short-term trades,\n",
    "strategically getting into or out of the index,\n",
    "depending on whether they believe\n",
    "that it will rise or decline\n",
    "in the subsequent time step.\n",
    "Absent any other features\n",
    "(news, financial reporting data, etc.),\n",
    "the only available signal for predicting\n",
    "the subsequent value is the history of prices to date.\n",
    "The trader is thus interested in knowing\n",
    "the probability distribution\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1)$$\n",
    "\n",
    "over prices that the index might take\n",
    "in the subsequent time step.\n",
    "While estimating the entire distribution\n",
    "over a continuously valued random variable\n",
    "can be difficult, the trader would be happy\n",
    "to focus on a few key statistics of the distribution,\n",
    "particularly the expected value and the variance.\n",
    "One simple strategy for estimating the conditional expectation\n",
    "\n",
    "$$\\mathbb{E}[(x_t \\mid x_{t-1}, \\ldots, x_1)],$$\n",
    "\n",
    "would be to apply a linear regression model\n",
    "(recall :numref:`sec_linear_regression`).\n",
    "Such models that regress the value of a signal\n",
    "on the previous values of that same signal\n",
    "are naturally called *autoregressive models*.\n",
    "There is just one major problem: the number of inputs,\n",
    "$x_{t-1}, \\ldots, x_1$ varies, depending on $t$.\n",
    "In other words, the number of inputs increases\n",
    "with the amount of data that we encounter.\n",
    "Thus if we want to treat our historical data\n",
    "as a training set, we are left with the problem\n",
    "that each example has a different number of features.\n",
    "Much of what follows in this chapter\n",
    "will revolve around techniques\n",
    "for overcoming these challenges\n",
    "when engaging in such *autoregressive* modeling problems\n",
    "where the object of interest is\n",
    "$P(x_t \\mid x_{t-1}, \\ldots, x_1)$\n",
    "or some statistic(s) of this distribution.\n",
    "\n",
    "A few strategies recur frequently.\n",
    "First of all,\n",
    "we might believe that although long sequences\n",
    "$x_{t-1}, \\ldots, x_1$ are available,\n",
    "it may not be necessary\n",
    "to look back so far in the history\n",
    "when predicting the near future.\n",
    "In this case we might content ourselves\n",
    "to condition on some window of length $\\tau$\n",
    "and only use $x_{t-1}, \\ldots, x_{t-\\tau}$ observations.\n",
    "The immediate benefit is that now the number of arguments\n",
    "is always the same, at least for $t > \\tau$.\n",
    "This allows us to train any linear model or deep network\n",
    "that requires fixed-length vectors as inputs.\n",
    "Second, we might develop models that maintain\n",
    "some summary $h_t$ of the past observations\n",
    "(see :numref:`fig_sequence-model`)\n",
    "and at the same time update $h_t$\n",
    "in addition to the prediction $\\hat{x}_t$.\n",
    "This leads to models that estimate not only $x_t$\n",
    "with $\\hat{x}_t = P(x_t \\mid h_{t})$\n",
    "but also updates of the form\n",
    "$h_t = g(h_{t-1}, x_{t-1})$.\n",
    "Since $h_t$ is never observed,\n",
    "these models are also called\n",
    "*latent autoregressive models*.\n",
    "\n",
    "![A latent autoregressive model.](../img/sequence-model.svg)\n",
    ":label:`fig_sequence-model`\n",
    "\n",
    "To construct training data from historical data, one\n",
    "typically creates examples by sampling windows randomly.\n",
    "In general, we do not expect time to stand still.\n",
    "However, we often assume that while\n",
    "the specific values of $x_t$ might change,\n",
    "the dynamics according to which each subsequent\n",
    "observation is generated given the previous observations do not.\n",
    "Statisticians call dynamics that do not change *stationary*.\n",
    "\n",
    "\n",
    "\n",
    "## Sequence Models\n",
    "\n",
    "Sometimes, especially when working with language,\n",
    "we wish to estimate the joint probability\n",
    "of an entire sequence.\n",
    "This is a common task when working with sequences\n",
    "composed of discrete *tokens*, such as words.\n",
    "Generally, these estimated functions are called *sequence models*\n",
    "and for natural language data, they are called *language models*.\n",
    "The field of sequence modeling has been driven so much by natural language processing,\n",
    "that we often describe sequence models as \"language models\",\n",
    "even when dealing with non-language data.\n",
    "Language models prove useful for all sorts of reasons.\n",
    "Sometimes we want to evaluate the likelihood of sentences.\n",
    "For example, we might wish to compare\n",
    "the naturalness of two candidate outputs\n",
    "generated by a machine translation system\n",
    "or by a speech recognition system.\n",
    "But language modeling gives us not only\n",
    "the capacity to *evaluate* likelihood,\n",
    "but the ability to *sample* sequences,\n",
    "and even to optimize for the most likely sequences.\n",
    "\n",
    "While language modeling might not, at first glance, look\n",
    "like an autoregressive problem,\n",
    "we can reduce language modeling to autoregressive prediction\n",
    "by decomposing the joint density  of a sequence $p(x_1, \\ldots, x_T)$\n",
    "into the product of conditional densities\n",
    "in a left-to-right fashion\n",
    "by applying the chain rule of probability:\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = P(x_1) \\prod_{t=2}^T P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n",
    "\n",
    "Note that if we are working with discrete signals such as words,\n",
    "then the autoregressive model must be a probabilistic classifier,\n",
    "outputting a full probability distribution\n",
    "over the vocabulary for whatever word will come next,\n",
    "given the leftwards context.\n",
    "\n",
    "\n",
    "\n",
    "### Markov Models\n",
    "\n",
    "\n",
    "Now suppose that we wish to employ the strategy mentioned above,\n",
    "where we condition only on the $\\tau$ previous time steps,\n",
    "i.e., $x_{t-1}, \\ldots, x_{t-\\tau}$, rather than\n",
    "the entire sequence history $x_{t-1}, \\ldots, x_1$.\n",
    "Whenever we can throw away the history\n",
    "beyond the previous $\\tau$ steps\n",
    "without any loss in predictive power,\n",
    "we say that the sequence satisfies a *Markov condition*,\n",
    "i.e., *that the future is conditionally independent of the past,\n",
    "given the recent history*.\n",
    "When $\\tau = 1$, we say that the data is characterized\n",
    "by a *first-order Markov model*,\n",
    "and when $\\tau = k$, we say that the data is characterized\n",
    "by a $k^{\\textrm{th}}$-order Markov model.\n",
    "For when the first-order Markov condition holds ($\\tau = 1$)\n",
    "the factorization of our joint probability becomes a product\n",
    "of probabilities of each word given the previous *word*:\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = P(x_1) \\prod_{t=2}^T P(x_t \\mid x_{t-1}).$$\n",
    "\n",
    "We often find it useful to work with models that proceed\n",
    "as though a Markov condition were satisfied,\n",
    "even when we know that this is only *approximately* true.\n",
    "With real text documents we continue to gain information\n",
    "as we include more and more leftwards context.\n",
    "But these gains diminish rapidly.\n",
    "Thus, sometimes we compromise, obviating computational and statistical difficulties\n",
    "by training models whose validity depends\n",
    "on a $k^{\\textrm{th}}$-order Markov condition.\n",
    "Even today's massive RNN- and Transformer-based language models\n",
    "seldom incorporate more than thousands of words of context.\n",
    "\n",
    "\n",
    "With discrete data, a true Markov model\n",
    "simply counts the number of times\n",
    "that each word has occurred in each context, producing\n",
    "the relative frequency estimate of $P(x_t \\mid x_{t-1})$.\n",
    "Whenever the data assumes only discrete values\n",
    "(as in language),\n",
    "the most likely sequence of words can be computed efficiently\n",
    "using dynamic programming.\n",
    "\n",
    "\n",
    "### The Order of Decoding\n",
    "\n",
    "You may be wondering why we represented\n",
    "the factorization of a text sequence $P(x_1, \\ldots, x_T)$\n",
    "as a left-to-right chain of conditional probabilities.\n",
    "Why not right-to-left or some other, seemingly random order?\n",
    "In principle, there is nothing wrong with unfolding\n",
    "$P(x_1, \\ldots, x_T)$ in reverse order.\n",
    "The result is a valid factorization:\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = P(x_T) \\prod_{t=T-1}^1 P(x_t \\mid x_{t+1}, \\ldots, x_T).$$\n",
    "\n",
    "\n",
    "However, there are many reasons why factorizing text\n",
    "in the same direction in which we read it\n",
    "(left-to-right for most languages,\n",
    "but right-to-left for Arabic and Hebrew)\n",
    "is preferred for the task of language modeling.\n",
    "First, this is just a more natural direction for us to think about.\n",
    "After all we all read text every day,\n",
    "and this process is guided by our ability\n",
    "to anticipate which words and phrases\n",
    "are likely to come next.\n",
    "Just think of how many times you have completed\n",
    "someone else's sentence.\n",
    "Thus, even if we had no other reason to prefer such in-order decodings,\n",
    "they would be useful if only because we have better intuitions\n",
    "for what should be likely when predicting in this order.\n",
    "\n",
    "Second, by factorizing in order,\n",
    "we can assign probabilities to arbitrarily long sequences\n",
    "using the same language model.\n",
    "To convert a probability over steps $1$ through $t$\n",
    "into one that extends to word $t+1$ we simply\n",
    "multiply by the conditional probability\n",
    "of the additional token given the previous ones:\n",
    "$P(x_{t+1}, \\ldots, x_1) = P(x_{t}, \\ldots, x_1) \\cdot P(x_{t+1} \\mid x_{t}, \\ldots, x_1)$.\n",
    "\n",
    "Third, we have stronger predictive models\n",
    "for predicting adjacent words than\n",
    "words at arbitrary other locations.\n",
    "While all orders of factorization are valid,\n",
    "they do not necessarily all represent equally easy\n",
    "predictive modeling problems.\n",
    "This is true not only for language,\n",
    "but for other kinds of data as well,\n",
    "e.g., when the data is causally structured.\n",
    "For example, we believe that future events cannot influence the past.\n",
    "Hence, if we change $x_t$, we may be able to influence\n",
    "what happens for $x_{t+1}$ going forward but not the converse.\n",
    "That is, if we change $x_t$, the distribution over past events will not change.\n",
    "In some contexts, this makes it easier to predict $P(x_{t+1} \\mid x_t)$\n",
    "than to predict $P(x_t \\mid x_{t+1})$.\n",
    "For instance, in some cases, we can find $x_{t+1} = f(x_t) + \\epsilon$\n",
    "for some additive noise $\\epsilon$,\n",
    "whereas the converse is not true :cite:`Hoyer.Janzing.Mooij.ea.2009`.\n",
    "This is great news, since it is typically the forward direction\n",
    "that we are interested in estimating.\n",
    "The book by :citet:`Peters.Janzing.Scholkopf.2017` contains more on this topic.\n",
    "We barely scratch the surface of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `AbstractData` not defined in `d2lai`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `AbstractData` not defined in `d2lai`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] getproperty(x::Module, f::Symbol)\n",
      "   @ Base ./Base.jl:42\n",
      " [2] top-level scope\n",
      "   @ ~/Projects/D2L/Julia_Notebooks/CH7.Recurrent_Neural_Networks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "struct SineData{A} <: d2lai.AbstractData \n",
    "    X::AbstractArray\n",
    "    time::AbstractArray\n",
    "    args::A\n",
    "    function SineData(batchsize = 16, T = 1000, num_train = 600, tau = 4)\n",
    "        time = collect(1:T+1)\n",
    "        X = sin.(0.01*time) .+ (randn(T).*0.2)\n",
    "        SineData(X, time, (batchsize = batchsize, num_train = 600, tau = tau, T = T))\n",
    "    end\n",
    "end\n",
    "\n",
    "data = SineData()\n",
    "plot(data.time, data.X, xlabel = \"time\", ylabel = \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_dataloader(data::SineData; train = true)\n",
    "    features = [data.X[i : data.args.T - data.args.tau + i] for i in 1:data.args.tau]\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
