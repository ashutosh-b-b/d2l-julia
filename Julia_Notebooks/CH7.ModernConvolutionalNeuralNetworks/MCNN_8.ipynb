{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37ace54-1ef2-4309-b95b-15a80f8ee9d1",
   "metadata": {},
   "source": [
    "# Designing Convolution Network Architectures\n",
    "\n",
    "The previous sections have taken us on a tour of modern network design for computer vision. Common to all the work we covered was that it greatly relied on the intuition of scientists. Many of the architectures are heavily informed by human creativity and to a much lesser extent by systematic exploration of the design space that deep networks offer. Nonetheless, this *network engineering* approach has been tremendously successful. \n",
    "\n",
    "Ever since AlexNet (:numref:`sec_alexnet`)\n",
    "beat conventional computer vision models on ImageNet,\n",
    "it has become popular to construct very deep networks\n",
    "by stacking blocks of convolutions, all designed according to the same pattern. \n",
    "In particular, $3 \\times 3$ convolutions were \n",
    "popularized by VGG networks (:numref:`sec_vgg`).\n",
    "NiN (:numref:`sec_nin`) showed that even $1 \\times 1$ convolutions could \n",
    "be beneficial by adding local nonlinearities. \n",
    "Moreover, NiN solved the problem of aggregating information at the head of a network \n",
    "by aggregating across all locations. \n",
    "GoogLeNet (:numref:`sec_googlenet`) added multiple branches of different convolution width, \n",
    "combining the advantages of VGG and NiN in its Inception block. \n",
    "ResNets (:numref:`sec_resnet`) \n",
    "changed the inductive bias towards the identity mapping (from $f(x) = 0$). This allowed for very deep networks. Almost a decade later, the ResNet design is still popular, a testament to its design. Lastly, ResNeXt (:numref:`subsec_resnext`) added grouped convolutions, offering a better trade-off between parameters and computation. A precursor to Transformers for vision, the Squeeze-and-Excitation Networks (SENets) allow for efficient information transfer between locations\n",
    ":cite:`Hu.Shen.Sun.2018`. This was accomplished by computing a per-channel global attention function. \n",
    "\n",
    "Up to now we have omitted networks obtained via *neural architecture search* (NAS) :cite:`zoph2016neural,liu2018darts`. We chose to do so since their cost is usually enormous, relying on brute-force search, genetic algorithms, reinforcement learning, or some other form of hyperparameter optimization. Given a fixed search space,\n",
    "NAS uses a search strategy to automatically select\n",
    "an architecture based on the returned performance estimation.\n",
    "The outcome of NAS\n",
    "is a single network instance. EfficientNets are a notable outcome of this search :cite:`tan2019efficientnet`.\n",
    "\n",
    "In the following we discuss an idea that is quite different to the quest for the *single best network*. It is computationally relatively inexpensive, it leads to scientific insights on the way, and it is quite effective in terms of the quality of outcomes. Let's review the strategy by :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` to *design network design spaces*. The strategy combines the strength of manual design and NAS. It accomplishes this by operating on *distributions of networks* and optimizing the distributions in a way to obtain good performance for entire families of networks. The outcome of it are *RegNets*, specifically RegNetX and RegNetY, plus a range of guiding principles for the design of performant CNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75621ee0-1379-436d-81af-78d9d0f72a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `/workspace/d2l-julia/d2lai`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"../../d2lai\")\n",
    "using d2lai\n",
    "using Flux \n",
    "using CUDA, cuDNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a720774-03f8-49c3-9596-4f5e9870fa3b",
   "metadata": {},
   "source": [
    "\n",
    "## The AnyNet Design Space\n",
    "\n",
    "The description below closely follows the reasoning in :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` with some abbreviations to make it fit in the scope of the book. \n",
    "To begin, we need a template for the family of networks to explore. One of the commonalities of the designs in this chapter is that the networks consist of a *stem*, a *body* and a *head*. The stem performs initial image processing, often through convolutions with a larger window size. The body consists of multiple blocks, carrying out the bulk of the transformations needed to go from raw images to object representations. Lastly, the head converts this into the desired outputs, such as via a softmax regressor for multiclass classification. \n",
    "The body, in turn, consists of multiple stages, operating on the image at decreasing resolutions. In fact, both the stem and each subsequent stage quarter the spatial resolution. Lastly, each stage consists of one or more blocks. This pattern is common to all networks, from VGG to ResNeXt. Indeed, for the design of generic AnyNet networks, :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` used the ResNeXt block of :numref:`fig_resnext_block`.\n",
    "\n",
    "\n",
    "![The AnyNet design space. The numbers $(\\mathit{c}, \\mathit{r})$ along each arrow indicate the number of channels $c$ and the resolution $\\mathit{r} \\times \\mathit{r}$ of the images at that point. From left to right: generic network structure composed of stem, body, and head; body composed of four stages; detailed structure of a stage; two alternative structures for blocks, one without downsampling and one that halves the resolution in each dimension. Design choices include depth $\\mathit{d_i}$, the number of output channels $\\mathit{c_i}$, the number of groups $\\mathit{g_i}$, and bottleneck ratio $\\mathit{k_i}$ for any stage $\\mathit{i}$.](../img/anynet.svg)\n",
    ":label:`fig_anynet_full`\n",
    "\n",
    "Let's review the structure outlined in :numref:`fig_anynet_full` in detail. As mentioned, an AnyNet consists of a stem, body, and head. The stem takes as its input RGB images (3 channels), using a $3 \\times 3$ convolution with a stride of $2$, followed by a batch norm, to halve the resolution from $r \\times r$ to $r/2 \\times r/2$. Moreover, it generates $c_0$ channels that serve as input to the body. \n",
    "\n",
    "Since the network is designed to work well with ImageNet images of shape $224 \\times 224 \\times 3$, the body serves to reduce this to $7 \\times 7 \\times c_4$ through 4 stages (recall that $224 / 2^{1+4} = 7$), each with an eventual stride of $2$. Lastly, the head employs an entirely standard design via global average pooling, similar to NiN (:numref:`sec_nin`), followed by a fully connected layer to emit an $n$-dimensional vector for $n$-class classification. \n",
    "\n",
    "Most of the relevant design decisions are inherent to the body of the network. It proceeds in stages, where each stage is composed of the same type of ResNeXt blocks as we discussed in :numref:`subsec_resnext`. The design there is again entirely generic: we begin with a block that halves the resolution by using a stride of $2$ (the rightmost in :numref:`fig_anynet_full`). To match this, the residual branch of the ResNeXt block needs to pass through a $1 \\times 1$ convolution. This block is followed by a variable number of additional ResNeXt blocks that leave both resolution and the number of channels unchanged. Note that a common design practice is to add a slight bottleneck in the design of convolutional blocks. \n",
    "As such, with bottleneck ratio $k_i \\geq 1$ we afford some number of channels, $c_i/k_i$,  within each block for stage $i$ (as the experiments show, this is not really effective and should be skipped). Lastly, since we are dealing with ResNeXt blocks, we also need to pick the number of groups $g_i$ for grouped convolutions at stage $i$. \n",
    "\n",
    "This seemingly generic design space provides us nonetheless with many parameters: we can set the block width (number of channels) $c_0, \\ldots c_4$, the depth (number of blocks) per stage $d_1, \\ldots d_4$, the bottleneck ratios $k_1, \\ldots k_4$, and the group widths (numbers of groups) $g_1, \\ldots g_4$. \n",
    "In total this adds up to 17 parameters, resulting in an unreasonably large number of configurations that would warrant exploring. We need some tools to reduce this huge design space effectively. This is where the conceptual beauty of design spaces comes in. Before we do so, let's implement the generic design first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08cfb0-2c1f-430b-9354-f1713824b2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (16 threads) 1.11.3",
   "language": "julia",
   "name": "julia-_16-threads_-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
