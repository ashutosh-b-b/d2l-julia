<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Concise Implementation of Recurrent Neural Networks · d2l Julia</title><meta name="title" content="Concise Implementation of Recurrent Neural Networks · d2l Julia"/><meta property="og:title" content="Concise Implementation of Recurrent Neural Networks · d2l Julia"/><meta property="twitter:title" content="Concise Implementation of Recurrent Neural Networks · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../RNN_3/">Language Models</a></li><li><a class="tocitem" href="../RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li class="is-active"><a class="tocitem" href>Concise Implementation of Recurrent Neural Networks</a><ul class="internal"><li><a class="tocitem" href="#Defining-the-Model"><span>Defining the Model</span></a></li><li><a class="tocitem" href="#Training-and-Predicting"><span>Training and Predicting</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Recurrent Neural Networks</a></li><li class="is-active"><a href>Concise Implementation of Recurrent Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Concise Implementation of Recurrent Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Concise-Implementation-of-Recurrent-Neural-Networks"><a class="docs-heading-anchor" href="#Concise-Implementation-of-Recurrent-Neural-Networks">Concise Implementation of Recurrent Neural Networks</a><a id="Concise-Implementation-of-Recurrent-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Concise-Implementation-of-Recurrent-Neural-Networks" title="Permalink"></a></h1><p>Like most of our from-scratch implementations, :numref:<code>sec_rnn-scratch</code> was designed  to provide insight into how each component works. But when you are using RNNs every day  or writing production code, you will want to rely more on libraries that cut down on both implementation time  (by supplying library code for common models and functions) and computation time  (by optimizing the heck out of these library implementations). This section will show you how to implement  the same language model more efficiently using the high-level API provided  by your deep learning framework. We begin, as before, by loading  <em>The Time Machine</em> dataset.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Flux 
using Downloads
using StatsBase
using Plots
using CUDA, cuDNN</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/d2l-julia/d2lai`</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Precompiling d2lai [749b8817-cd67-416c-8a57-830ea19f3cc4] (cache misses: include_dependency fsize change (2))</pre>
</div><h2 id="Defining-the-Model"><a class="docs-heading-anchor" href="#Defining-the-Model">Defining the Model</a><a id="Defining-the-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-Model" title="Permalink"></a></h2><p>We define the following class using the RNN implemented by high-level APIs.</p><pre><code class="language-julia hljs">num_hiddens = 32 
input_size = 28
rnn = Flux.RNN(input_size =&gt; num_hiddens, return_state = true)</code></pre><pre><code class="nohighlight hljs">RNN(28 =&gt; 32, tanh)  # 1_952 parameters</code></pre><p>Inheriting from the <code>AbstractRNNClassifier</code> class in :numref:<code>sec_rnn-scratch</code>,  the following <code>RNNLM</code> class defines a complete RNN-based language model. Note that we need to create a separate fully connected output layer.</p><pre><code class="language-julia hljs">struct RNNLM{N,R,A} &lt;: d2lai.AbstractRNNClassifier 
    net::N 
    rnn::R 
    args::A
end

Flux.@layer RNNLM trainable = (net, rnn)
function RNNLM(rnn::Flux.RNN, num_hiddens::Int, vocab_size::Int)
    net = Dense(num_hiddens =&gt; vocab_size)
    return RNNLM(net, rnn, (num_hiddens = num_hiddens, vocab_size = vocab_size))
end

function d2lai.output_layer(m::RNNLM, out)
    m.net(out)
end

function (m::RNNLM)(x)
    out = m.rnn(x)[1]
    return d2lai.output_layer(m, out)
end
</code></pre><h2 id="Training-and-Predicting"><a class="docs-heading-anchor" href="#Training-and-Predicting">Training and Predicting</a><a id="Training-and-Predicting-1"></a><a class="docs-heading-anchor-permalink" href="#Training-and-Predicting" title="Permalink"></a></h2><p>Before training the model, let&#39;s make a prediction  with a model initialized with random weights. Given that we have not trained the network,  it will generate nonsensical predictions.</p><pre><code class="language-julia hljs">num_hiddens = 32
data = d2lai.TimeMachine(1024, 32) |&gt; f64
rnn = Flux.RNN(length(data.vocab) =&gt; num_hiddens, return_state = true)
model = RNNLM(rnn, num_hiddens, length(data.vocab)) |&gt; f64
prefix = &quot;it has&quot;
d2lai.prediction(prefix, model, data.vocab, 20, state = zeros(num_hiddens))</code></pre><pre><code class="nohighlight hljs">&quot;it hasnfjqoqvsoqvszqk&lt;unk&gt;zbgs&quot;</code></pre><pre><code class="language-julia hljs">opt = Descent(1.)
trainer = Trainer(model, data, opt; max_epochs = 100, gpu = true, board_yscale = :identity, gradient_clip_val = 1.)
m, _ = d2lai.fit(trainer)</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 2.8086967, Val Loss: 2.8176572
    [ Info: Train Loss: 2.5578668, Val Loss: 2.5525599
    [ Info: Train Loss: 2.4050918, Val Loss: 2.4460049
    [ Info: Train Loss: 2.316194, Val Loss: 2.368135
    [ Info: Train Loss: 2.2381124, Val Loss: 2.332166
    [ Info: Train Loss: 2.1993399, Val Loss: 2.3082745
    [ Info: Train Loss: 2.1650236, Val Loss: 2.2931821
    [ Info: Train Loss: 2.1162374, Val Loss: 2.2517207
    [ Info: Train Loss: 2.1146615, Val Loss: 2.2625127
    [ Info: Train Loss: 2.0888648, Val Loss: 2.2390275
    [ Info: Train Loss: 2.0716052, Val Loss: 2.230474
    [ Info: Train Loss: 2.0240247, Val Loss: 2.2290487
    [ Info: Train Loss: 2.0295258, Val Loss: 2.2020535
    [ Info: Train Loss: 1.9899576, Val Loss: 2.1624563
    [ Info: Train Loss: 2.004797, Val Loss: 2.1773639
    [ Info: Train Loss: 1.9721513, Val Loss: 2.1735885
    [ Info: Train Loss: 1.9659573, Val Loss: 2.1581883
    [ Info: Train Loss: 1.9484462, Val Loss: 2.1393445
    [ Info: Train Loss: 1.930977, Val Loss: 2.1594217
    [ Info: Train Loss: 1.9451939, Val Loss: 2.1503987
    [ Info: Train Loss: 1.9346877, Val Loss: 2.114789
    [ Info: Train Loss: 1.8983876, Val Loss: 2.1232784
    [ Info: Train Loss: 1.8946356, Val Loss: 2.1232986
    [ Info: Train Loss: 1.8931116, Val Loss: 2.0921307
    [ Info: Train Loss: 1.8924788, Val Loss: 2.113529
    [ Info: Train Loss: 1.8815277, Val Loss: 2.0992854
    [ Info: Train Loss: 1.8664699, Val Loss: 2.0718553
    [ Info: Train Loss: 1.8620858, Val Loss: 2.0682604
    [ Info: Train Loss: 1.8730401, Val Loss: 2.0869172
    [ Info: Train Loss: 1.8470776, Val Loss: 2.06626
    [ Info: Train Loss: 1.8517028, Val Loss: 2.082669
    [ Info: Train Loss: 1.839059, Val Loss: 2.0598977
    [ Info: Train Loss: 1.8291104, Val Loss: 2.0723095
    [ Info: Train Loss: 1.8454365, Val Loss: 2.0628722
    [ Info: Train Loss: 1.8226379, Val Loss: 2.0716498
    [ Info: Train Loss: 1.818457, Val Loss: 2.0575447
    [ Info: Train Loss: 1.8064052, Val Loss: 2.0661533
    [ Info: Train Loss: 1.7888063, Val Loss: 2.0482213
    [ Info: Train Loss: 1.8107212, Val Loss: 2.0582194
    [ Info: Train Loss: 1.8009559, Val Loss: 2.0606222
    [ Info: Train Loss: 1.8003632, Val Loss: 2.062232
    [ Info: Train Loss: 1.793633, Val Loss: 2.0593336
    [ Info: Train Loss: 1.7663118, Val Loss: 2.0475314
    [ Info: Train Loss: 1.776362, Val Loss: 2.0510414
    [ Info: Train Loss: 1.7802719, Val Loss: 2.0557463
    [ Info: Train Loss: 1.7893523, Val Loss: 2.061776
    [ Info: Train Loss: 1.7992761, Val Loss: 2.0471537
    [ Info: Train Loss: 1.7724043, Val Loss: 2.0530665
    [ Info: Train Loss: 1.7678832, Val Loss: 2.043026
    [ Info: Train Loss: 1.7517827, Val Loss: 2.0457454
    [ Info: Train Loss: 1.7520797, Val Loss: 2.0534086
    [ Info: Train Loss: 1.735371, Val Loss: 2.0413203
    [ Info: Train Loss: 1.741371, Val Loss: 2.0482664
    [ Info: Train Loss: 1.7486898, Val Loss: 2.0389254
    [ Info: Train Loss: 1.7638668, Val Loss: 2.0467787
    [ Info: Train Loss: 1.7438501, Val Loss: 2.0521975
    [ Info: Train Loss: 1.7357632, Val Loss: 2.043563
    [ Info: Train Loss: 1.7333704, Val Loss: 2.0436978
    [ Info: Train Loss: 1.762108, Val Loss: 2.0362566
    [ Info: Train Loss: 1.7176626, Val Loss: 2.0604687
    [ Info: Train Loss: 1.717186, Val Loss: 2.0518222
    [ Info: Train Loss: 1.7292625, Val Loss: 2.0569487
    [ Info: Train Loss: 1.7006977, Val Loss: 2.0418108
    [ Info: Train Loss: 1.7316309, Val Loss: 2.0493767
    [ Info: Train Loss: 1.69463, Val Loss: 2.054217
    [ Info: Train Loss: 1.720155, Val Loss: 2.0379403
    [ Info: Train Loss: 1.7145555, Val Loss: 2.0562189
    [ Info: Train Loss: 1.7215083, Val Loss: 2.0379372
    [ Info: Train Loss: 1.7383448, Val Loss: 2.0564325
    [ Info: Train Loss: 1.7068391, Val Loss: 2.0735323
    [ Info: Train Loss: 1.7159855, Val Loss: 2.06642
    [ Info: Train Loss: 1.709202, Val Loss: 2.0742066
    [ Info: Train Loss: 1.7023453, Val Loss: 2.0456624
    [ Info: Train Loss: 1.6985327, Val Loss: 2.0623722
    [ Info: Train Loss: 1.6987822, Val Loss: 2.0451224
    [ Info: Train Loss: 1.7177141, Val Loss: 2.044373
    [ Info: Train Loss: 1.7015309, Val Loss: 2.0406048
    [ Info: Train Loss: 1.6836351, Val Loss: 2.0618813
    [ Info: Train Loss: 1.6850606, Val Loss: 2.0612392
    [ Info: Train Loss: 1.7006621, Val Loss: 2.047501
    [ Info: Train Loss: 1.6999978, Val Loss: 2.052405
    [ Info: Train Loss: 1.7009304, Val Loss: 2.0556684
    [ Info: Train Loss: 1.6887848, Val Loss: 2.0540125
    [ Info: Train Loss: 1.6855419, Val Loss: 2.0616734
    [ Info: Train Loss: 1.6916366, Val Loss: 2.0426404
    [ Info: Train Loss: 1.6893617, Val Loss: 2.0559156
    [ Info: Train Loss: 1.6977744, Val Loss: 2.0574336
    [ Info: Train Loss: 1.6968977, Val Loss: 2.0456219
    [ Info: Train Loss: 1.6823503, Val Loss: 2.062682
    [ Info: Train Loss: 1.6991038, Val Loss: 2.051137
    [ Info: Train Loss: 1.6861258, Val Loss: 2.032179
    [ Info: Train Loss: 1.6727321, Val Loss: 2.0421462
    [ Info: Train Loss: 1.6626164, Val Loss: 2.0548327
    [ Info: Train Loss: 1.6945255, Val Loss: 2.041975
    [ Info: Train Loss: 1.6579713, Val Loss: 2.0641537
    [ Info: Train Loss: 1.6617924, Val Loss: 2.058827
    [ Info: Train Loss: 1.6782551, Val Loss: 2.0603712
    [ Info: Train Loss: 1.6792405, Val Loss: 2.0408056
    [ Info: Train Loss: 1.6815602, Val Loss: 2.0537863
    [ Info: Train Loss: 1.6723149, Val Loss: 2.039872</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip730">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip730)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip731">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip730)" d="M139.446 1423.18 L2352.76 1423.18 L2352.76 47.2441 L139.446 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip732">
    <rect x="139" y="47" width="2214" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="180.995,1423.18 180.995,47.2441 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="708.275,1423.18 708.275,47.2441 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1235.56,1423.18 1235.56,47.2441 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1762.84,1423.18 1762.84,47.2441 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2290.12,1423.18 2290.12,47.2441 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,1321.9 2352.76,1321.9 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,1128.86 2352.76,1128.86 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,935.816 2352.76,935.816 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,742.773 2352.76,742.773 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,549.731 2352.76,549.731 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,356.688 2352.76,356.688 "/>
<polyline clip-path="url(#clip732)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="139.446,163.646 2352.76,163.646 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="180.995,1423.18 180.995,1404.28 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="708.275,1423.18 708.275,1404.28 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1235.56,1423.18 1235.56,1404.28 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1762.84,1423.18 1762.84,1404.28 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2290.12,1423.18 2290.12,1404.28 "/>
<path clip-path="url(#clip730)" d="M180.995 1454.1 Q177.384 1454.1 175.555 1457.66 Q173.75 1461.2 173.75 1468.33 Q173.75 1475.44 175.555 1479.01 Q177.384 1482.55 180.995 1482.55 Q184.629 1482.55 186.435 1479.01 Q188.264 1475.44 188.264 1468.33 Q188.264 1461.2 186.435 1457.66 Q184.629 1454.1 180.995 1454.1 M180.995 1450.39 Q186.805 1450.39 189.861 1455 Q192.94 1459.58 192.94 1468.33 Q192.94 1477.06 189.861 1481.67 Q186.805 1486.25 180.995 1486.25 Q175.185 1486.25 172.106 1481.67 Q169.051 1477.06 169.051 1468.33 Q169.051 1459.58 172.106 1455 Q175.185 1450.39 180.995 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M687.546 1481.64 L703.865 1481.64 L703.865 1485.58 L681.921 1485.58 L681.921 1481.64 Q684.583 1478.89 689.166 1474.26 Q693.773 1469.61 694.953 1468.27 Q697.199 1465.74 698.078 1464.01 Q698.981 1462.25 698.981 1460.56 Q698.981 1457.8 697.037 1456.07 Q695.116 1454.33 692.014 1454.33 Q689.815 1454.33 687.361 1455.09 Q684.93 1455.86 682.153 1457.41 L682.153 1452.69 Q684.977 1451.55 687.43 1450.97 Q689.884 1450.39 691.921 1450.39 Q697.291 1450.39 700.486 1453.08 Q703.68 1455.77 703.68 1460.26 Q703.68 1462.39 702.87 1464.31 Q702.083 1466.2 699.977 1468.8 Q699.398 1469.47 696.296 1472.69 Q693.194 1475.88 687.546 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M713.727 1451.02 L732.083 1451.02 L732.083 1454.96 L718.009 1454.96 L718.009 1463.43 Q719.027 1463.08 720.046 1462.92 Q721.064 1462.73 722.083 1462.73 Q727.87 1462.73 731.25 1465.9 Q734.629 1469.08 734.629 1474.49 Q734.629 1480.07 731.157 1483.17 Q727.685 1486.25 721.365 1486.25 Q719.189 1486.25 716.921 1485.88 Q714.676 1485.51 712.268 1484.77 L712.268 1480.07 Q714.352 1481.2 716.574 1481.76 Q718.796 1482.32 721.273 1482.32 Q725.277 1482.32 727.615 1480.21 Q729.953 1478.1 729.953 1474.49 Q729.953 1470.88 727.615 1468.77 Q725.277 1466.67 721.273 1466.67 Q719.398 1466.67 717.523 1467.08 Q715.671 1467.5 713.727 1468.38 L713.727 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1210.25 1451.02 L1228.61 1451.02 L1228.61 1454.96 L1214.54 1454.96 L1214.54 1463.43 Q1215.56 1463.08 1216.57 1462.92 Q1217.59 1462.73 1218.61 1462.73 Q1224.4 1462.73 1227.78 1465.9 Q1231.16 1469.08 1231.16 1474.49 Q1231.16 1480.07 1227.68 1483.17 Q1224.21 1486.25 1217.89 1486.25 Q1215.72 1486.25 1213.45 1485.88 Q1211.2 1485.51 1208.8 1484.77 L1208.8 1480.07 Q1210.88 1481.2 1213.1 1481.76 Q1215.32 1482.32 1217.8 1482.32 Q1221.81 1482.32 1224.14 1480.21 Q1226.48 1478.1 1226.48 1474.49 Q1226.48 1470.88 1224.14 1468.77 Q1221.81 1466.67 1217.8 1466.67 Q1215.93 1466.67 1214.05 1467.08 Q1212.2 1467.5 1210.25 1468.38 L1210.25 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1250.37 1454.1 Q1246.76 1454.1 1244.93 1457.66 Q1243.12 1461.2 1243.12 1468.33 Q1243.12 1475.44 1244.93 1479.01 Q1246.76 1482.55 1250.37 1482.55 Q1254 1482.55 1255.81 1479.01 Q1257.64 1475.44 1257.64 1468.33 Q1257.64 1461.2 1255.81 1457.66 Q1254 1454.1 1250.37 1454.1 M1250.37 1450.39 Q1256.18 1450.39 1259.24 1455 Q1262.31 1459.58 1262.31 1468.33 Q1262.31 1477.06 1259.24 1481.67 Q1256.18 1486.25 1250.37 1486.25 Q1244.56 1486.25 1241.48 1481.67 Q1238.43 1477.06 1238.43 1468.33 Q1238.43 1459.58 1241.48 1455 Q1244.56 1450.39 1250.37 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1736.69 1451.02 L1758.91 1451.02 L1758.91 1453.01 L1746.37 1485.58 L1741.48 1485.58 L1753.29 1454.96 L1736.69 1454.96 L1736.69 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1768.08 1451.02 L1786.43 1451.02 L1786.43 1454.96 L1772.36 1454.96 L1772.36 1463.43 Q1773.38 1463.08 1774.4 1462.92 Q1775.42 1462.73 1776.43 1462.73 Q1782.22 1462.73 1785.6 1465.9 Q1788.98 1469.08 1788.98 1474.49 Q1788.98 1480.07 1785.51 1483.17 Q1782.04 1486.25 1775.72 1486.25 Q1773.54 1486.25 1771.27 1485.88 Q1769.03 1485.51 1766.62 1484.77 L1766.62 1480.07 Q1768.7 1481.2 1770.93 1481.76 Q1773.15 1482.32 1775.62 1482.32 Q1779.63 1482.32 1781.97 1480.21 Q1784.3 1478.1 1784.3 1474.49 Q1784.3 1470.88 1781.97 1468.77 Q1779.63 1466.67 1775.62 1466.67 Q1773.75 1466.67 1771.87 1467.08 Q1770.02 1467.5 1768.08 1468.38 L1768.08 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2249.72 1481.64 L2257.36 1481.64 L2257.36 1455.28 L2249.05 1456.95 L2249.05 1452.69 L2257.31 1451.02 L2261.99 1451.02 L2261.99 1481.64 L2269.63 1481.64 L2269.63 1485.58 L2249.72 1485.58 L2249.72 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2289.07 1454.1 Q2285.46 1454.1 2283.63 1457.66 Q2281.83 1461.2 2281.83 1468.33 Q2281.83 1475.44 2283.63 1479.01 Q2285.46 1482.55 2289.07 1482.55 Q2292.71 1482.55 2294.51 1479.01 Q2296.34 1475.44 2296.34 1468.33 Q2296.34 1461.2 2294.51 1457.66 Q2292.71 1454.1 2289.07 1454.1 M2289.07 1450.39 Q2294.88 1450.39 2297.94 1455 Q2301.02 1459.58 2301.02 1468.33 Q2301.02 1477.06 2297.94 1481.67 Q2294.88 1486.25 2289.07 1486.25 Q2283.26 1486.25 2280.18 1481.67 Q2277.13 1477.06 2277.13 1468.33 Q2277.13 1459.58 2280.18 1455 Q2283.26 1450.39 2289.07 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2319.24 1454.1 Q2315.62 1454.1 2313.8 1457.66 Q2311.99 1461.2 2311.99 1468.33 Q2311.99 1475.44 2313.8 1479.01 Q2315.62 1482.55 2319.24 1482.55 Q2322.87 1482.55 2324.68 1479.01 Q2326.5 1475.44 2326.5 1468.33 Q2326.5 1461.2 2324.68 1457.66 Q2322.87 1454.1 2319.24 1454.1 M2319.24 1450.39 Q2325.05 1450.39 2328.1 1455 Q2331.18 1459.58 2331.18 1468.33 Q2331.18 1477.06 2328.1 1481.67 Q2325.05 1486.25 2319.24 1486.25 Q2313.43 1486.25 2310.35 1481.67 Q2307.29 1477.06 2307.29 1468.33 Q2307.29 1459.58 2310.35 1455 Q2313.43 1450.39 2319.24 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1166.29 1548.76 L1166.29 1551.62 L1139.36 1551.62 Q1139.75 1557.67 1142.99 1560.85 Q1146.27 1564 1152.1 1564 Q1155.47 1564 1158.62 1563.17 Q1161.8 1562.35 1164.92 1560.69 L1164.92 1566.23 Q1161.77 1567.57 1158.46 1568.27 Q1155.15 1568.97 1151.75 1568.97 Q1143.21 1568.97 1138.22 1564 Q1133.25 1559.04 1133.25 1550.57 Q1133.25 1541.82 1137.96 1536.69 Q1142.71 1531.54 1150.73 1531.54 Q1157.92 1531.54 1162.09 1536.18 Q1166.29 1540.8 1166.29 1548.76 M1160.43 1547.04 Q1160.37 1542.23 1157.73 1539.37 Q1155.12 1536.5 1150.79 1536.5 Q1145.89 1536.5 1142.93 1539.27 Q1140 1542.04 1139.55 1547.07 L1160.43 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1181.57 1562.7 L1181.57 1581.6 L1175.68 1581.6 L1175.68 1532.4 L1181.57 1532.4 L1181.57 1537.81 Q1183.41 1534.62 1186.22 1533.1 Q1189.05 1531.54 1192.96 1531.54 Q1199.46 1531.54 1203.5 1536.69 Q1207.57 1541.85 1207.57 1550.25 Q1207.57 1558.65 1203.5 1563.81 Q1199.46 1568.97 1192.96 1568.97 Q1189.05 1568.97 1186.22 1567.44 Q1183.41 1565.88 1181.57 1562.7 M1201.49 1550.25 Q1201.49 1543.79 1198.82 1540.13 Q1196.18 1536.44 1191.53 1536.44 Q1186.88 1536.44 1184.21 1540.13 Q1181.57 1543.79 1181.57 1550.25 Q1181.57 1556.71 1184.21 1560.4 Q1186.88 1564.07 1191.53 1564.07 Q1196.18 1564.07 1198.82 1560.4 Q1201.49 1556.71 1201.49 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1231.09 1536.5 Q1226.38 1536.5 1223.65 1540.19 Q1220.91 1543.85 1220.91 1550.25 Q1220.91 1556.65 1223.61 1560.34 Q1226.35 1564 1231.09 1564 Q1235.77 1564 1238.51 1560.31 Q1241.25 1556.62 1241.25 1550.25 Q1241.25 1543.92 1238.51 1540.23 Q1235.77 1536.5 1231.09 1536.5 M1231.09 1531.54 Q1238.73 1531.54 1243.09 1536.5 Q1247.45 1541.47 1247.45 1550.25 Q1247.45 1559 1243.09 1564 Q1238.73 1568.97 1231.09 1568.97 Q1223.42 1568.97 1219.06 1564 Q1214.73 1559 1214.73 1550.25 Q1214.73 1541.47 1219.06 1536.5 Q1223.42 1531.54 1231.09 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1282.81 1533.76 L1282.81 1539.24 Q1280.33 1537.87 1277.82 1537.2 Q1275.34 1536.5 1272.79 1536.5 Q1267.09 1536.5 1263.94 1540.13 Q1260.79 1543.73 1260.79 1550.25 Q1260.79 1556.78 1263.94 1560.4 Q1267.09 1564 1272.79 1564 Q1275.34 1564 1277.82 1563.33 Q1280.33 1562.63 1282.81 1561.26 L1282.81 1566.68 Q1280.36 1567.82 1277.72 1568.39 Q1275.11 1568.97 1272.15 1568.97 Q1264.1 1568.97 1259.36 1563.91 Q1254.61 1558.85 1254.61 1550.25 Q1254.61 1541.53 1259.39 1536.53 Q1264.2 1531.54 1272.53 1531.54 Q1275.24 1531.54 1277.82 1532.11 Q1280.4 1532.65 1282.81 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1322.63 1546.53 L1322.63 1568.04 L1316.78 1568.04 L1316.78 1546.72 Q1316.78 1541.66 1314.8 1539.14 Q1312.83 1536.63 1308.88 1536.63 Q1304.14 1536.63 1301.4 1539.65 Q1298.67 1542.68 1298.67 1547.9 L1298.67 1568.04 L1292.78 1568.04 L1292.78 1518.52 L1298.67 1518.52 L1298.67 1537.93 Q1300.77 1534.72 1303.6 1533.13 Q1306.46 1531.54 1310.19 1531.54 Q1316.33 1531.54 1319.48 1535.36 Q1322.63 1539.14 1322.63 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M1357.04 1533.45 L1357.04 1538.98 Q1354.56 1537.71 1351.88 1537.07 Q1349.21 1536.44 1346.34 1536.44 Q1341.98 1536.44 1339.79 1537.77 Q1337.62 1539.11 1337.62 1541.79 Q1337.62 1543.82 1339.18 1545 Q1340.74 1546.15 1345.45 1547.2 L1347.46 1547.64 Q1353.7 1548.98 1356.31 1551.43 Q1358.95 1553.85 1358.95 1558.21 Q1358.95 1563.17 1355 1566.07 Q1351.09 1568.97 1344.21 1568.97 Q1341.35 1568.97 1338.23 1568.39 Q1335.14 1567.85 1331.7 1566.74 L1331.7 1560.69 Q1334.95 1562.38 1338.1 1563.24 Q1341.25 1564.07 1344.34 1564.07 Q1348.48 1564.07 1350.71 1562.66 Q1352.93 1561.23 1352.93 1558.65 Q1352.93 1556.27 1351.31 1554.99 Q1349.72 1553.72 1344.28 1552.54 L1342.24 1552.07 Q1336.8 1550.92 1334.38 1548.56 Q1331.96 1546.18 1331.96 1542.04 Q1331.96 1537.01 1335.52 1534.27 Q1339.09 1531.54 1345.64 1531.54 Q1348.89 1531.54 1351.76 1532.01 Q1354.62 1532.49 1357.04 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,1423.18 139.446,47.2441 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,1321.9 158.343,1321.9 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,1128.86 158.343,1128.86 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,935.816 158.343,935.816 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,742.773 158.343,742.773 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,549.731 158.343,549.731 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,356.688 158.343,356.688 "/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="139.446,163.646 158.343,163.646 "/>
<path clip-path="url(#clip730)" d="M91.9178 1320.04 Q88.7697 1320.04 86.9179 1322.19 Q85.0892 1324.34 85.0892 1328.09 Q85.0892 1331.82 86.9179 1334 Q88.7697 1336.15 91.9178 1336.15 Q95.066 1336.15 96.8947 1334 Q98.7465 1331.82 98.7465 1328.09 Q98.7465 1324.34 96.8947 1322.19 Q95.066 1320.04 91.9178 1320.04 M101.2 1305.38 L101.2 1309.64 Q99.4409 1308.81 97.6354 1308.37 Q95.853 1307.93 94.0937 1307.93 Q89.4641 1307.93 87.0105 1311.06 Q84.5799 1314.18 84.2327 1320.5 Q85.5984 1318.49 87.6586 1317.42 Q89.7188 1316.33 92.1956 1316.33 Q97.4039 1316.33 100.413 1319.5 Q103.446 1322.65 103.446 1328.09 Q103.446 1333.42 100.297 1336.63 Q97.1493 1339.85 91.9178 1339.85 Q85.9225 1339.85 82.7512 1335.27 Q79.5799 1330.66 79.5799 1321.94 Q79.5799 1313.74 83.4688 1308.88 Q87.3577 1304 93.9086 1304 Q95.6678 1304 97.4502 1304.34 Q99.2558 1304.69 101.2 1305.38 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M91.5938 1129.73 Q88.2604 1129.73 86.3392 1131.51 Q84.441 1133.29 84.441 1136.42 Q84.441 1139.54 86.3392 1141.32 Q88.2604 1143.11 91.5938 1143.11 Q94.9271 1143.11 96.8484 1141.32 Q98.7696 1139.52 98.7696 1136.42 Q98.7696 1133.29 96.8484 1131.51 Q94.9502 1129.73 91.5938 1129.73 M86.9179 1127.74 Q83.9086 1126.99 82.2188 1124.93 Q80.5522 1122.87 80.5522 1119.91 Q80.5522 1115.77 83.492 1113.36 Q86.4549 1110.95 91.5938 1110.95 Q96.7558 1110.95 99.6956 1113.36 Q102.635 1115.77 102.635 1119.91 Q102.635 1122.87 100.946 1124.93 Q99.2789 1126.99 96.2928 1127.74 Q99.6724 1128.52 101.547 1130.81 Q103.446 1133.11 103.446 1136.42 Q103.446 1141.44 100.367 1144.12 Q97.3113 1146.81 91.5938 1146.81 Q85.8762 1146.81 82.7975 1144.12 Q79.742 1141.44 79.742 1136.42 Q79.742 1133.11 81.6401 1130.81 Q83.5382 1128.52 86.9179 1127.74 M85.2049 1120.35 Q85.2049 1123.04 86.8716 1124.54 Q88.5614 1126.05 91.5938 1126.05 Q94.603 1126.05 96.2928 1124.54 Q98.0058 1123.04 98.0058 1120.35 Q98.0058 1117.67 96.2928 1116.16 Q94.603 1114.66 91.5938 1114.66 Q88.5614 1114.66 86.8716 1116.16 Q85.2049 1117.67 85.2049 1120.35 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M52.1495 949.161 L59.7884 949.161 L59.7884 922.795 L51.4782 924.462 L51.4782 920.202 L59.7421 918.536 L64.418 918.536 L64.418 949.161 L72.0568 949.161 L72.0568 953.096 L52.1495 953.096 L52.1495 949.161 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M91.5012 921.614 Q87.8901 921.614 86.0614 925.179 Q84.2558 928.721 84.2558 935.851 Q84.2558 942.957 86.0614 946.522 Q87.8901 950.063 91.5012 950.063 Q95.1354 950.063 96.941 946.522 Q98.7696 942.957 98.7696 935.851 Q98.7696 928.721 96.941 925.179 Q95.1354 921.614 91.5012 921.614 M91.5012 917.911 Q97.3113 917.911 100.367 922.517 Q103.446 927.101 103.446 935.851 Q103.446 944.577 100.367 949.184 Q97.3113 953.767 91.5012 953.767 Q85.691 953.767 82.6123 949.184 Q79.5568 944.577 79.5568 935.851 Q79.5568 927.101 82.6123 922.517 Q85.691 917.911 91.5012 917.911 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M53.7467 756.118 L61.3856 756.118 L61.3856 729.753 L53.0754 731.419 L53.0754 727.16 L61.3393 725.493 L66.0152 725.493 L66.0152 756.118 L73.654 756.118 L73.654 760.053 L53.7467 760.053 L53.7467 756.118 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M87.1262 756.118 L103.446 756.118 L103.446 760.053 L81.5012 760.053 L81.5012 756.118 Q84.1632 753.364 88.7466 748.734 Q93.353 744.081 94.5336 742.739 Q96.7789 740.215 97.6585 738.479 Q98.5613 736.72 98.5613 735.03 Q98.5613 732.276 96.6169 730.54 Q94.6956 728.803 91.5938 728.803 Q89.3947 728.803 86.941 729.567 Q84.5105 730.331 81.7327 731.882 L81.7327 727.16 Q84.5568 726.026 87.0105 725.447 Q89.4641 724.868 91.5012 724.868 Q96.8715 724.868 100.066 727.554 Q103.26 730.239 103.26 734.729 Q103.26 736.859 102.45 738.78 Q101.663 740.678 99.5567 743.271 Q98.978 743.942 95.8761 747.16 Q92.7743 750.354 87.1262 756.118 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M51.6634 563.076 L59.3023 563.076 L59.3023 536.71 L50.9921 538.377 L50.9921 534.118 L59.256 532.451 L63.9319 532.451 L63.9319 563.076 L71.5707 563.076 L71.5707 567.011 L51.6634 567.011 L51.6634 563.076 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M93.8623 536.525 L82.0568 554.974 L93.8623 554.974 L93.8623 536.525 M92.6354 532.451 L98.515 532.451 L98.515 554.974 L103.446 554.974 L103.446 558.863 L98.515 558.863 L98.515 567.011 L93.8623 567.011 L93.8623 558.863 L78.2605 558.863 L78.2605 554.349 L92.6354 532.451 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M51.9875 370.033 L59.6263 370.033 L59.6263 343.668 L51.3162 345.334 L51.3162 341.075 L59.58 339.408 L64.2559 339.408 L64.2559 370.033 L71.8948 370.033 L71.8948 373.968 L51.9875 373.968 L51.9875 370.033 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M91.9178 354.825 Q88.7697 354.825 86.9179 356.978 Q85.0892 359.131 85.0892 362.881 Q85.0892 366.607 86.9179 368.783 Q88.7697 370.936 91.9178 370.936 Q95.066 370.936 96.8947 368.783 Q98.7465 366.607 98.7465 362.881 Q98.7465 359.131 96.8947 356.978 Q95.066 354.825 91.9178 354.825 M101.2 340.172 L101.2 344.432 Q99.4409 343.598 97.6354 343.158 Q95.853 342.719 94.0937 342.719 Q89.4641 342.719 87.0105 345.844 Q84.5799 348.969 84.2327 355.288 Q85.5984 353.274 87.6586 352.209 Q89.7188 351.121 92.1956 351.121 Q97.4039 351.121 100.413 354.293 Q103.446 357.441 103.446 362.881 Q103.446 368.205 100.297 371.422 Q97.1493 374.64 91.9178 374.64 Q85.9225 374.64 82.7512 370.056 Q79.5799 365.45 79.5799 356.723 Q79.5799 348.529 83.4688 343.668 Q87.3577 338.783 93.9086 338.783 Q95.6678 338.783 97.4502 339.131 Q99.2558 339.478 101.2 340.172 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M52.2421 176.991 L59.881 176.991 L59.881 150.625 L51.5708 152.292 L51.5708 148.033 L59.8347 146.366 L64.5106 146.366 L64.5106 176.991 L72.1494 176.991 L72.1494 180.926 L52.2421 180.926 L52.2421 176.991 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M91.5938 164.514 Q88.2604 164.514 86.3392 166.296 Q84.441 168.079 84.441 171.204 Q84.441 174.329 86.3392 176.111 Q88.2604 177.894 91.5938 177.894 Q94.9271 177.894 96.8484 176.111 Q98.7696 174.306 98.7696 171.204 Q98.7696 168.079 96.8484 166.296 Q94.9502 164.514 91.5938 164.514 M86.9179 162.523 Q83.9086 161.783 82.2188 159.722 Q80.5522 157.662 80.5522 154.699 Q80.5522 150.556 83.492 148.148 Q86.4549 145.741 91.5938 145.741 Q96.7558 145.741 99.6956 148.148 Q102.635 150.556 102.635 154.699 Q102.635 157.662 100.946 159.722 Q99.2789 161.783 96.2928 162.523 Q99.6724 163.31 101.547 165.602 Q103.446 167.894 103.446 171.204 Q103.446 176.227 100.367 178.912 Q97.3113 181.597 91.5938 181.597 Q85.8762 181.597 82.7975 178.912 Q79.742 176.227 79.742 171.204 Q79.742 167.894 81.6401 165.602 Q83.5382 163.31 86.9179 162.523 M85.2049 155.139 Q85.2049 157.824 86.8716 159.329 Q88.5614 160.833 91.5938 160.833 Q94.603 160.833 96.2928 159.329 Q98.0058 157.824 98.0058 155.139 Q98.0058 152.454 96.2928 150.949 Q94.603 149.445 91.5938 149.445 Q88.5614 149.445 86.8716 150.949 Q85.2049 152.454 85.2049 155.139 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip732)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="202.086,86.1857 223.178,536.561 244.269,774.031 265.36,887.684 286.451,954.358 307.542,1005.76 328.634,1037.98 349.725,1068.76 370.816,1090.12 391.907,1111.11 412.998,1130.82 434.09,1147.04 455.181,1161.33 476.272,1174.51 497.363,1183.9 518.454,1196.13 539.546,1205.95 560.637,1213.21 581.728,1221.44 602.819,1227.35 623.91,1237.07 645.002,1244.74 666.093,1250.92 687.184,1253.25 708.275,1262.25 729.366,1268.61 750.458,1270.25 771.549,1276.75 792.64,1281.29 813.731,1286.37 834.822,1288.14 855.914,1290.51 877.005,1295.69 898.096,1298.17 919.187,1304.08 940.278,1303.9 961.37,1309.59 982.461,1312.28 1003.55,1314.85 1024.64,1315.69 1045.73,1318.38 1066.83,1320.09 1087.92,1323.35 1109.01,1325.35 1130.1,1326.68 1151.19,1328.26 1172.28,1330.55 1193.37,1333.07 1214.46,1333.9 1235.56,1336.8 1256.65,1339.89 1277.74,1339.47 1298.83,1341.85 1319.92,1341.8 1341.01,1345.02 1362.1,1345.95 1383.19,1346.75 1404.28,1349.75 1425.38,1348.89 1446.47,1349.69 1467.56,1352.49 1488.65,1353.89 1509.74,1353.85 1530.83,1355.73 1551.92,1356.63 1573.01,1356.82 1594.11,1360.39 1615.2,1359.52 1636.29,1359.57 1657.38,1361.75 1678.47,1361.95 1699.56,1363.59 1720.65,1364.77 1741.74,1365.97 1762.84,1366.66 1783.93,1368.06 1805.02,1367.85 1826.11,1369.09 1847.2,1370.3 1868.29,1370.71 1889.38,1371.19 1910.47,1373.2 1931.56,1372.23 1952.66,1374.26 1973.75,1375.43 1994.84,1375.83 2015.93,1375.5 2037.02,1377.24 2058.11,1377.96 2079.2,1377.16 2100.29,1377.29 2121.39,1380.76 2142.48,1380.26 2163.57,1380.79 2184.66,1380.76 2205.75,1382.54 2226.84,1383.24 2247.93,1383.04 2269.02,1382.43 2290.12,1384.24 "/>
<polyline clip-path="url(#clip732)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="202.086,317.936 223.178,677.289 244.269,829.322 265.36,917.737 286.451,963.349 307.542,991.801 328.634,1007.43 349.725,1040.35 370.816,1038.25 391.907,1049.51 412.998,1053.63 434.09,1053.83 455.181,1072.9 476.272,1094.22 497.363,1083.36 518.454,1087.87 539.546,1093.57 560.637,1102.17 581.728,1113.46 602.819,1106.59 623.91,1125.59 645.002,1119.25 666.093,1118.38 687.184,1133.34 708.275,1129.94 729.366,1135.02 750.458,1151.85 771.549,1150.4 792.64,1149.47 813.731,1151.66 834.822,1144.59 855.914,1158.15 877.005,1161.39 898.096,1157.4 919.187,1155.42 940.278,1162.82 961.37,1163.87 982.461,1172.76 1003.55,1162.83 1024.64,1161.03 1045.73,1154.83 1066.83,1160.35 1087.92,1171.85 1109.01,1173.3 1130.1,1164.45 1151.19,1168.64 1172.28,1167.09 1193.37,1164.95 1214.46,1177.02 1235.56,1169.12 1256.65,1175.51 1277.74,1174.84 1298.83,1172.11 1319.92,1168.45 1341.01,1179.98 1362.1,1169.4 1383.19,1176.79 1404.28,1175.94 1425.38,1180.95 1446.47,1173.72 1467.56,1179.26 1488.65,1169.68 1509.74,1176.55 1530.83,1179.47 1551.92,1168.97 1573.01,1172.89 1594.11,1178.79 1615.2,1178.78 1636.29,1171.81 1657.38,1162.97 1678.47,1170.39 1699.56,1162.19 1720.65,1175.94 1741.74,1166.91 1762.84,1175.72 1783.93,1180.3 1805.02,1181.39 1826.11,1172.33 1847.2,1169.26 1868.29,1181.52 1889.38,1180.8 1910.47,1175.87 1931.56,1177.55 1952.66,1170.64 1973.75,1181.16 1994.84,1175.36 2015.93,1176.39 2037.02,1178.06 2058.11,1176.73 2079.2,1180.6 2100.29,1187.5 2121.39,1185.03 2142.48,1180.17 2163.57,1186.02 2184.66,1175.81 2205.75,1182.77 2226.84,1188.4 2247.93,1188.2 2269.02,1193.74 2290.12,1180.11 "/>
<path clip-path="url(#clip730)" d="M1855.96 248.629 L2278.98 248.629 L2278.98 93.1086 L1855.96 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip730)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1855.96,248.629 2278.98,248.629 2278.98,93.1086 1855.96,93.1086 1855.96,248.629 "/>
<polyline clip-path="url(#clip730)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1880.55,144.949 2028.11,144.949 "/>
<path clip-path="url(#clip730)" d="M2060.11 128.942 L2060.11 136.303 L2068.88 136.303 L2068.88 139.613 L2060.11 139.613 L2060.11 153.687 Q2060.11 156.858 2060.96 157.761 Q2061.84 158.664 2064.5 158.664 L2068.88 158.664 L2068.88 162.229 L2064.5 162.229 Q2059.57 162.229 2057.7 160.4 Q2055.82 158.548 2055.82 153.687 L2055.82 139.613 L2052.7 139.613 L2052.7 136.303 L2055.82 136.303 L2055.82 128.942 L2060.11 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2089.5 140.284 Q2088.79 139.868 2087.93 139.682 Q2087.1 139.474 2086.08 139.474 Q2082.47 139.474 2080.52 141.835 Q2078.6 144.173 2078.6 148.571 L2078.6 162.229 L2074.32 162.229 L2074.32 136.303 L2078.6 136.303 L2078.6 140.331 Q2079.94 137.969 2082.1 136.835 Q2084.25 135.678 2087.33 135.678 Q2087.77 135.678 2088.3 135.747 Q2088.83 135.794 2089.48 135.909 L2089.5 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2105.75 149.196 Q2100.59 149.196 2098.6 150.377 Q2096.61 151.557 2096.61 154.405 Q2096.61 156.673 2098.09 158.016 Q2099.6 159.335 2102.17 159.335 Q2105.71 159.335 2107.84 156.835 Q2109.99 154.312 2109.99 150.145 L2109.99 149.196 L2105.75 149.196 M2114.25 147.437 L2114.25 162.229 L2109.99 162.229 L2109.99 158.293 Q2108.53 160.655 2106.35 161.789 Q2104.18 162.9 2101.03 162.9 Q2097.05 162.9 2094.69 160.678 Q2092.35 158.432 2092.35 154.682 Q2092.35 150.307 2095.27 148.085 Q2098.21 145.863 2104.02 145.863 L2109.99 145.863 L2109.99 145.446 Q2109.99 142.507 2108.04 140.909 Q2106.12 139.289 2102.63 139.289 Q2100.41 139.289 2098.3 139.821 Q2096.19 140.354 2094.25 141.419 L2094.25 137.483 Q2096.59 136.581 2098.79 136.141 Q2100.98 135.678 2103.07 135.678 Q2108.69 135.678 2111.47 138.594 Q2114.25 141.511 2114.25 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2123.02 136.303 L2127.28 136.303 L2127.28 162.229 L2123.02 162.229 L2123.02 136.303 M2123.02 126.21 L2127.28 126.21 L2127.28 131.604 L2123.02 131.604 L2123.02 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2157.74 146.581 L2157.74 162.229 L2153.48 162.229 L2153.48 146.719 Q2153.48 143.039 2152.05 141.21 Q2150.61 139.382 2147.74 139.382 Q2144.29 139.382 2142.3 141.581 Q2140.31 143.78 2140.31 147.576 L2140.31 162.229 L2136.03 162.229 L2136.03 136.303 L2140.31 136.303 L2140.31 140.331 Q2141.84 137.993 2143.9 136.835 Q2145.98 135.678 2148.69 135.678 Q2153.16 135.678 2155.45 138.456 Q2157.74 141.21 2157.74 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2185.94 170.099 L2185.94 173.409 L2161.31 173.409 L2161.31 170.099 L2185.94 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2194.06 158.34 L2194.06 172.09 L2189.78 172.09 L2189.78 136.303 L2194.06 136.303 L2194.06 140.238 Q2195.41 137.923 2197.44 136.812 Q2199.5 135.678 2202.35 135.678 Q2207.07 135.678 2210.01 139.428 Q2212.97 143.178 2212.97 149.289 Q2212.97 155.4 2210.01 159.15 Q2207.07 162.9 2202.35 162.9 Q2199.5 162.9 2197.44 161.789 Q2195.41 160.655 2194.06 158.34 M2208.55 149.289 Q2208.55 144.59 2206.61 141.928 Q2204.69 139.243 2201.31 139.243 Q2197.93 139.243 2195.98 141.928 Q2194.06 144.59 2194.06 149.289 Q2194.06 153.988 2195.98 156.673 Q2197.93 159.335 2201.31 159.335 Q2204.69 159.335 2206.61 156.673 Q2208.55 153.988 2208.55 149.289 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2224.16 158.34 L2224.16 172.09 L2219.87 172.09 L2219.87 136.303 L2224.16 136.303 L2224.16 140.238 Q2225.5 137.923 2227.53 136.812 Q2229.6 135.678 2232.44 135.678 Q2237.16 135.678 2240.1 139.428 Q2243.07 143.178 2243.07 149.289 Q2243.07 155.4 2240.1 159.15 Q2237.16 162.9 2232.44 162.9 Q2229.6 162.9 2227.53 161.789 Q2225.5 160.655 2224.16 158.34 M2238.65 149.289 Q2238.65 144.59 2236.7 141.928 Q2234.78 139.243 2231.4 139.243 Q2228.02 139.243 2226.08 141.928 Q2224.16 144.59 2224.16 149.289 Q2224.16 153.988 2226.08 156.673 Q2228.02 159.335 2231.4 159.335 Q2234.78 159.335 2236.7 156.673 Q2238.65 153.988 2238.65 149.289 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2250.13 126.21 L2254.39 126.21 L2254.39 162.229 L2250.13 162.229 L2250.13 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip730)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1880.55,196.789 2028.11,196.789 "/>
<path clip-path="url(#clip730)" d="M2052.7 188.143 L2057.21 188.143 L2065.31 209.902 L2073.42 188.143 L2077.93 188.143 L2068.21 214.069 L2062.42 214.069 L2052.7 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2095.59 201.036 Q2090.43 201.036 2088.44 202.217 Q2086.45 203.397 2086.45 206.245 Q2086.45 208.513 2087.93 209.856 Q2089.43 211.175 2092 211.175 Q2095.54 211.175 2097.67 208.675 Q2099.83 206.152 2099.83 201.985 L2099.83 201.036 L2095.59 201.036 M2104.09 199.277 L2104.09 214.069 L2099.83 214.069 L2099.83 210.133 Q2098.37 212.495 2096.19 213.629 Q2094.02 214.74 2090.87 214.74 Q2086.89 214.74 2084.53 212.518 Q2082.19 210.272 2082.19 206.522 Q2082.19 202.147 2085.11 199.925 Q2088.04 197.703 2093.86 197.703 L2099.83 197.703 L2099.83 197.286 Q2099.83 194.347 2097.88 192.749 Q2095.96 191.129 2092.47 191.129 Q2090.24 191.129 2088.14 191.661 Q2086.03 192.194 2084.09 193.259 L2084.09 189.323 Q2086.42 188.421 2088.62 187.981 Q2090.82 187.518 2092.91 187.518 Q2098.53 187.518 2101.31 190.434 Q2104.09 193.351 2104.09 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2112.86 178.05 L2117.12 178.05 L2117.12 214.069 L2112.86 214.069 L2112.86 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2145.73 221.939 L2145.73 225.249 L2121.1 225.249 L2121.1 221.939 L2145.73 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2153.85 210.18 L2153.85 223.93 L2149.57 223.93 L2149.57 188.143 L2153.85 188.143 L2153.85 192.078 Q2155.2 189.763 2157.23 188.652 Q2159.29 187.518 2162.14 187.518 Q2166.86 187.518 2169.8 191.268 Q2172.77 195.018 2172.77 201.129 Q2172.77 207.24 2169.8 210.99 Q2166.86 214.74 2162.14 214.74 Q2159.29 214.74 2157.23 213.629 Q2155.2 212.495 2153.85 210.18 M2168.35 201.129 Q2168.35 196.43 2166.4 193.768 Q2164.48 191.083 2161.1 191.083 Q2157.72 191.083 2155.78 193.768 Q2153.85 196.43 2153.85 201.129 Q2153.85 205.828 2155.78 208.513 Q2157.72 211.175 2161.1 211.175 Q2164.48 211.175 2166.4 208.513 Q2168.35 205.828 2168.35 201.129 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2183.95 210.18 L2183.95 223.93 L2179.66 223.93 L2179.66 188.143 L2183.95 188.143 L2183.95 192.078 Q2185.29 189.763 2187.33 188.652 Q2189.39 187.518 2192.23 187.518 Q2196.96 187.518 2199.9 191.268 Q2202.86 195.018 2202.86 201.129 Q2202.86 207.24 2199.9 210.99 Q2196.96 214.74 2192.23 214.74 Q2189.39 214.74 2187.33 213.629 Q2185.29 212.495 2183.95 210.18 M2198.44 201.129 Q2198.44 196.43 2196.49 193.768 Q2194.57 191.083 2191.19 191.083 Q2187.81 191.083 2185.87 193.768 Q2183.95 196.43 2183.95 201.129 Q2183.95 205.828 2185.87 208.513 Q2187.81 211.175 2191.19 211.175 Q2194.57 211.175 2196.49 208.513 Q2198.44 205.828 2198.44 201.129 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip730)" d="M2209.92 178.05 L2214.18 178.05 L2214.18 214.069 L2209.92 214.069 L2209.92 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="nohighlight hljs">(RNNLM{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, RNN{true, RNNCell{typeof(tanh), Matrix{Float32}, Matrix{Float32}, Vector{Float32}}}, @NamedTuple{num_hiddens::Int64, vocab_size::Int64}}(Dense(32 =&gt; 28), RNN(28 =&gt; 32, tanh), (num_hiddens = 32, vocab_size = 28)), (val_loss = Float32[2.0083926, 1.9724069, 2.137811, 1.8953362, 2.039872], val_acc = nothing))</code></pre><p>Compared with :numref:<code>sec_rnn-scratch</code>, this model achieves comparable perplexity, but runs faster due to the optimized implementations. As before, we can generate predicted tokens  following the specified prefix string.</p><pre><code class="language-julia hljs">prefix = &quot;it has&quot;
d2lai.prediction(prefix, m, data.vocab, 20, state = zeros(num_hiddens))</code></pre><pre><code class="nohighlight hljs">┌ Warning: Layer with Float32 parameters got Float64 input.
│   The input will be converted, but any earlier layers may be very slow.
│   layer = Dense(32 =&gt; 28)     # 924 parameters
│   summary(x) = &quot;32×1 Matrix{Float64}&quot;
└ @ Flux ~/.julia/packages/Flux/3711C/src/layers/stateless.jl:60





&quot;it has dimension of the po&quot;</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>High-level APIs in deep learning frameworks provide implementations of standard RNNs. These libraries help you to avoid wasting time reimplementing standard models. Moreover, framework implementations are often highly optimized,    leading to significant (computational) performance gains    when compared with implementations from scratch.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Can you make the RNN model overfit using the high-level APIs?</li><li>Implement the autoregressive model of :numref:<code>sec_sequence</code> using an RNN.</li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../RNN_5/">« Recurrent Neural Network Implementation from Scratch</a><a class="docs-footer-nextpage" href="../RNN_7/">Backpropagation Through Time »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
