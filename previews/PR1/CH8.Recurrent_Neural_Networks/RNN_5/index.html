<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Recurrent Neural Network Implementation from Scratch · d2l Julia</title><meta name="title" content="Recurrent Neural Network Implementation from Scratch · d2l Julia"/><meta property="og:title" content="Recurrent Neural Network Implementation from Scratch · d2l Julia"/><meta property="twitter:title" content="Recurrent Neural Network Implementation from Scratch · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../RNN_3/">Language Models</a></li><li><a class="tocitem" href="../RNN_4/">Recurrent Neural Networks</a></li><li class="is-active"><a class="tocitem" href>Recurrent Neural Network Implementation from Scratch</a><ul class="internal"><li><a class="tocitem" href="#RNN-Model"><span>RNN Model</span></a></li><li><a class="tocitem" href="#RNN-Based-Language-Model"><span>RNN-Based Language Model</span></a></li><li><a class="tocitem" href="#Gradient-Clipping"><span>Gradient Clipping</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Decoding"><span>Decoding</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Recurrent Neural Networks</a></li><li class="is-active"><a href>Recurrent Neural Network Implementation from Scratch</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Recurrent Neural Network Implementation from Scratch</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Recurrent-Neural-Network-Implementation-from-Scratch"><a class="docs-heading-anchor" href="#Recurrent-Neural-Network-Implementation-from-Scratch">Recurrent Neural Network Implementation from Scratch</a><a id="Recurrent-Neural-Network-Implementation-from-Scratch-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent-Neural-Network-Implementation-from-Scratch" title="Permalink"></a></h1><p>We are now ready to implement an RNN from scratch. In particular, we will train this RNN to function as a character-level language model (see :numref:<code>sec_rnn</code>) and train it on a corpus consisting of  the entire text of H. G. Wells&#39; <em>The Time Machine</em>, following the data processing steps  outlined in :numref:<code>sec_text-sequence</code>. We start by loading the dataset.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Flux 
using Downloads
using StatsBase
using Plots
using CUDA, cuDNN</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/d2l-julia/d2lai`</code></pre><h2 id="RNN-Model"><a class="docs-heading-anchor" href="#RNN-Model">RNN Model</a><a id="RNN-Model-1"></a><a class="docs-heading-anchor-permalink" href="#RNN-Model" title="Permalink"></a></h2><p>We begin by defining a class      to implement the RNN model     (:numref:<code>subsec_rnn_w_hidden_states</code>).     Note that the number of hidden units <code>num_hiddens</code>      is a tunable hyperparameter.</p><pre><code class="language-julia hljs">
struct RNNScratch{Wx, Wh, Bh, A} &lt;: AbstractModel 
    Whx::Wx
    Whh::Wh
    b_h::Bh
    args::A
end
Flux.@layer RNNScratch trainable = (Whx, Whh, b_h)

function RNNScratch(num_inputs::Int, num_hiddens::Int; sigma = 0.01)
    Whx = randn(num_hiddens, num_inputs).*sigma 
    Whh = randn(num_hiddens, num_hiddens).*sigma 
    b_h = zeros(num_hiddens)
    RNNScratch(Whx, Whh, b_h, (num_inputs = num_inputs, num_hiddens = num_hiddens, sigma = sigma))
end</code></pre><pre><code class="nohighlight hljs">RNNScratch</code></pre><p>The <code>RNNScratch</code> method below defines how to compute  the output and hidden state at any time step, given the current input and the state of the model at the previous time step. Note that the RNN model loops through  the second dimension of <code>inputs</code>, updating the hidden state  one time step at a time. The model here uses a <span>$\tanh$</span> activation function (:numref:<code>subsec_tanh</code>).</p><pre><code class="language-julia hljs">function (rnn::RNNScratch)(x::AbstractArray, state = nothing)
    batchsize = size(x, 3)
    device = isa(x, CuArray) ? gpu : cpu
    state = if isnothing(state)
        zeros(rnn.args.num_hiddens, size(x, 3))
    else
        state
    end |&gt; device
    outputs = map(eachslice(x, dims = 2)) do x_ 
        state = tanh.(rnn.Whx*x_ + rnn.Whh*state .+ rnn.b_h)
        return state
    end
    outputs_cat = stack(outputs)
    return permutedims(outputs_cat, [1, 3, 2]), state  # num_hiddens x num_steps x batchsize, num_hiddens x batchsize
end</code></pre><p>We can feed a minibatch of input sequences into an RNN model as follows.</p><pre><code class="language-julia hljs">batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100
rnn = RNNScratch(num_inputs, num_hiddens)
X = ones((num_inputs, num_steps, batch_size))
outputs, state = rnn(X)</code></pre><pre><code class="nohighlight hljs">([0.03149590863711263 0.026365868884975295 … 0.026380686362776472 0.026380686362776472; -0.0030060672354854214 -0.003511913636600897 … -0.0035204783968947237 -0.0035204783968947237; … ; 0.05496053938912919 0.05292253724661685 … 0.05305652141664291 0.05305652141664291; 0.006091443127899462 0.004483091327932017 … 0.004324985453887874 0.004324985453887874;;; 0.03149590863711263 0.026365868884975295 … 0.026380686362776472 0.026380686362776472; -0.0030060672354854214 -0.003511913636600897 … -0.0035204783968947237 -0.0035204783968947237; … ; 0.05496053938912919 0.05292253724661685 … 0.05305652141664291 0.05305652141664291; 0.006091443127899462 0.004483091327932017 … 0.004324985453887874 0.004324985453887874], [0.026380686362776472 0.026380686362776472; -0.0035204783968947237 -0.0035204783968947237; … ; 0.05305652141664291 0.05305652141664291; 0.004324985453887874 0.004324985453887874])</code></pre><p>Let’s check whether the RNN model produces results of the correct shapes to ensure that the dimensionality of the hidden state remains unchanged.</p><pre><code class="language-julia hljs">@assert size(outputs, 2) == num_steps 
@assert size(outputs[:, 1, :]) == (num_hiddens, batch_size)
@assert size(outputs) == (num_hiddens, num_steps, batch_size)

</code></pre><h2 id="RNN-Based-Language-Model"><a class="docs-heading-anchor" href="#RNN-Based-Language-Model">RNN-Based Language Model</a><a id="RNN-Based-Language-Model-1"></a><a class="docs-heading-anchor-permalink" href="#RNN-Based-Language-Model" title="Permalink"></a></h2><p>The following <code>RNNLMScratch</code> class defines  an RNN-based language model, where we pass in our RNN  via the <code>rnn</code> argument of the constructor method. When training language models,  the inputs and outputs are  from the same vocabulary.  Hence, they have the same dimension, which is equal to the vocabulary size. Note that we use perplexity to evaluate the model.  As discussed in :numref:<code>subsec_perplexity</code>, this ensures  that sequences of different length are comparable.</p><pre><code class="language-julia hljs">abstract type AbstractRNNClassifier &lt;: AbstractClassifier end

struct RNNLMScratch{R, W, B, A} &lt;: AbstractRNNClassifier
    rnn::R
    Wq::W 
    bq::B
    args::A
end

Flux.@layer RNNLMScratch trainable = (rnn, Wq, bq)

function RNNLMScratch(rnn, vocab_size) 
    Wq = randn(vocab_size, rnn.args.num_hiddens)*rnn.args.sigma 
    bq = zeros(vocab_size)
    RNNLMScratch(rnn, Wq, bq, (vocab_size=vocab_size,))
end

function d2lai.loss(m::AbstractRNNClassifier, y_pred, y)
    Flux.logitcrossentropy(y_pred, Flux.onehotbatch(y, 1:m.args.vocab_size))
end

function d2lai.training_step(m::AbstractRNNClassifier, batch)
    y_pred = d2lai.forward(m, batch[1])
    loss_ = d2lai.loss(m, y_pred, batch[end])
    return loss_
end

function d2lai.validation_step(m::AbstractRNNClassifier, batch)
    y_pred = d2lai.forward(m, batch[1])
    loss_ = d2lai.loss(m, y_pred, batch[end])
    return loss_ , nothing
end
</code></pre><h3 id="Transforming-RNN-Outputs"><a class="docs-heading-anchor" href="#Transforming-RNN-Outputs">Transforming RNN Outputs</a><a id="Transforming-RNN-Outputs-1"></a><a class="docs-heading-anchor-permalink" href="#Transforming-RNN-Outputs" title="Permalink"></a></h3><p>The language model uses a fully connected output layer to transform RNN outputs into token predictions at each time step.</p><pre><code class="language-julia hljs">function output_layer(m::RNNLMScratch, x)
    outs = map(eachslice(x, dims =2)) do x_ 
        m.Wq*x_ .+ m.bq
    end
    outs = stack(outs)
    return permutedims(outs, [1, 3, 2])
end

function (rnnlm::RNNLMScratch)(x, state = nothing)
    output, _ = rnnlm.rnn(x, state)
    output_layer(rnnlm, output)
end</code></pre><p>Let&#39;s check whether the forward computation produces outputs with the correct shape.</p><pre><code class="language-julia hljs">model = RNNLMScratch(rnn, num_inputs)
output = model(ones(num_inputs, num_steps, batch_size))
@assert size(output) == (num_inputs, num_steps, batch_size)</code></pre><h2 id="Gradient-Clipping"><a class="docs-heading-anchor" href="#Gradient-Clipping">Gradient Clipping</a><a id="Gradient-Clipping-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Clipping" title="Permalink"></a></h2><p>While you are already used to thinking of neural networks as &quot;deep&quot; in the sense that many layers separate the input and output  even within a single time step, the length of the sequence introduces a new notion of depth. In addition to the passing through the network in the input-to-output direction, inputs at the first time step must pass through a chain of <span>$T$</span> layers along the time steps in order  to influence the output of the model at the final time step. Taking the backwards view, in each iteration, we backpropagate gradients through time, resulting in a chain of matrix-products  of length  <span>$\mathcal{O}(T)$</span>. As mentioned in :numref:<code>sec_numerical_stability</code>,  this can result in numerical instability,  causing the gradients either to explode or vanish, depending on the properties of the weight matrices. </p><p>Dealing with vanishing and exploding gradients  is a fundamental problem when designing RNNs and has inspired some of the biggest advances in modern neural network architectures. In the next chapter, we will talk about specialized architectures that were designed in hopes of mitigating the vanishing gradient problem. However, even modern RNNs often suffer from exploding gradients. One inelegant but ubiquitous solution is to simply clip the gradients  forcing the resulting &quot;clipped&quot; gradients to take smaller values. </p><p>Generally speaking, when optimizing some objective by gradient descent, we iteratively update the parameter of interest, say a vector <span>$\mathbf{x}$</span>, but pushing it in the direction of the  negative gradient <span>$\mathbf{g}$</span> (in stochastic gradient descent,  we calculate this gradient on a randomly sampled minibatch). For example, with learning rate <span>$\eta &gt; 0$</span>, each update takes the form  <span>$\mathbf{x} \gets \mathbf{x} - \eta \mathbf{g}$</span>. Let&#39;s further assume that the objective function <span>$f$</span> is sufficiently smooth.  Formally, we say that the objective  is <em>Lipschitz continuous</em> with constant <span>$L$</span>, meaning that for any <span>$\mathbf{x}$</span> and <span>$\mathbf{y}$</span>, we have</p><p class="math-container">\[|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|.\]</p><p>As you can see, when we update the parameter vector by subtracting <span>$\eta \mathbf{g}$</span>, the change in the value of the objective depends on the learning rate, the norm of the gradient and <span>$L$</span> as follows:</p><p class="math-container">\[|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta\|\mathbf{g}\|.\]</p><p>In other words, the objective cannot change by more than <span>$L \eta \|\mathbf{g}\|$</span>.  Having a small value for this upper bound  might be viewed as good or bad. On the downside, we are limiting the speed at which we can reduce the value of the objective. On the bright side, this limits by just how much we can go wrong in any one gradient step.</p><p>When we say that gradients explode,  we mean that <span>$\|\mathbf{g}\|$</span>  becomes excessively large. In this worst case, we might do so much damage in a single gradient step that we could undo all of the progress made over the course of thousands of training iterations. When gradients can be so large, neural network training often diverges, failing to reduce the value of the objective. At other times, training eventually converges but is unstable owing to massive spikes in the loss.</p><p>One way to limit the size of <span>$L \eta \|\mathbf{g}\|$</span>  is to shrink the learning rate <span>$\eta$</span> to tiny values. This has the advantage that we do not bias the updates. But what if we only <em>rarely</em> get large gradients? This drastic move slows down our progress at all steps, just to deal with the rare exploding gradient events. A popular alternative is to adopt a <em>gradient clipping</em> heuristic projecting the gradients <span>$\mathbf{g}$</span> onto a ball  of some given radius <span>$\theta$</span> as follows:</p><p>(<strong><span>$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}.$</span></strong>)</p><p>This ensures that the gradient norm never exceeds <span>$\theta$</span>  and that the updated gradient is entirely aligned  with the original direction of <span>$\mathbf{g}$</span>. It also has the desirable side-effect  of limiting the influence any given minibatch  (and within it any given sample)  can exert on the parameter vector.  This bestows a certain degree of robustness to the model.  To be clear, it is a hack.  Gradient clipping means that we are not always following the true gradient and it is hard  to reason analytically about the possible side effects. However, it is a very useful hack, and is widely adopted in RNN implementations in most deep learning frameworks.</p><p>Below we define a method to clip gradients, which is invoked by the <code>fit_epoch</code> method of the <code>d2l.Trainer</code> class (see :numref:<code>sec_linear_scratch</code>). Note that when computing the gradient norm, we are concatenating all model parameters, treating them as a single giant parameter vector.</p><pre><code class="language-julia hljs">function d2lai.clip_gradients!(gs, gradient_clip_val, model)
    sums = fmap(gs, walk=Flux.Functors.IterateWalk()) do g
       !isnothing(g) &amp;&amp; sum(g.^2)
   end
   norm = sqrt(sum(filter(x -&gt; x != false, collect(sums))))
   g_ = fmap(gs) do d
       if !isnothing(d)
           d = d.* (1. / norm)
       end
   end
   g_
end</code></pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>Using <em>The Time Machine</em> dataset (<code>data</code>), we train a character-level language model (<code>model</code>) based on the RNN (<code>rnn</code>) implemented from scratch. Note that we first calculate the gradients, then clip them, and finally  update the model parameters using the clipped gradients.</p><pre><code class="language-julia hljs">data = d2lai.TimeMachine(1024, 32) |&gt; f64
num_hiddens = 32
rnn = RNNScratch(length(data.vocab), num_hiddens)
model = RNNLMScratch(rnn, length(data.vocab)) |&gt; f64

opt = Flux.Optimiser(Descent(1.))
trainer = Trainer(model, data, opt; max_epochs = 100, gpu = true, gradient_clip_val = 1., board_yscale = :identity)
m, _ = d2lai.fit(trainer);</code></pre><pre><code class="nohighlight hljs">┌ Warning: `Flux.Optimiser(...)` has been removed, please call `OptimiserChain(...)`, exported by Flux from Optimisers.jl
└ @ Flux ~/.julia/packages/Flux/3711C/src/deprecations.jl:123</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 2.7970488, Val Loss: 2.7806466
    [ Info: Train Loss: 2.5299044, Val Loss: 2.5298347
    [ Info: Train Loss: 2.4346051, Val Loss: 2.4459527
    [ Info: Train Loss: 2.386652, Val Loss: 2.417434
    [ Info: Train Loss: 2.356393, Val Loss: 2.3768516
    [ Info: Train Loss: 2.3064065, Val Loss: 2.3563817
    [ Info: Train Loss: 2.2758873, Val Loss: 2.3187153
    [ Info: Train Loss: 2.250458, Val Loss: 2.3117213
    [ Info: Train Loss: 2.2068582, Val Loss: 2.285258
    [ Info: Train Loss: 2.1796353, Val Loss: 2.2712932
    [ Info: Train Loss: 2.168831, Val Loss: 2.2566879
    [ Info: Train Loss: 2.139044, Val Loss: 2.2405543
    [ Info: Train Loss: 2.1123457, Val Loss: 2.214762
    [ Info: Train Loss: 2.0906117, Val Loss: 2.2032485
    [ Info: Train Loss: 2.0699565, Val Loss: 2.1767392
    [ Info: Train Loss: 2.0620189, Val Loss: 2.170835
    [ Info: Train Loss: 2.0402772, Val Loss: 2.161874
    [ Info: Train Loss: 2.0210266, Val Loss: 2.155169
    [ Info: Train Loss: 2.0138662, Val Loss: 2.1288521
    [ Info: Train Loss: 1.9950365, Val Loss: 2.1473613
    [ Info: Train Loss: 1.9791988, Val Loss: 2.1329134
    [ Info: Train Loss: 1.9536208, Val Loss: 2.1350038
    [ Info: Train Loss: 1.9502233, Val Loss: 2.1203203
    [ Info: Train Loss: 1.9379421, Val Loss: 2.117951
    [ Info: Train Loss: 1.9315099, Val Loss: 2.1151593
    [ Info: Train Loss: 1.9132171, Val Loss: 2.1154299
    [ Info: Train Loss: 1.9058115, Val Loss: 2.1053178
    [ Info: Train Loss: 1.8888556, Val Loss: 2.0996616
    [ Info: Train Loss: 1.8528732, Val Loss: 2.072391
    [ Info: Train Loss: 1.8539463, Val Loss: 2.106058
    [ Info: Train Loss: 1.850917, Val Loss: 2.080681
    [ Info: Train Loss: 1.8432741, Val Loss: 2.0786562
    [ Info: Train Loss: 1.8390543, Val Loss: 2.0741498
    [ Info: Train Loss: 1.8201522, Val Loss: 2.0682487
    [ Info: Train Loss: 1.8261395, Val Loss: 2.073867
    [ Info: Train Loss: 1.8438075, Val Loss: 2.066239
    [ Info: Train Loss: 1.8288956, Val Loss: 2.0952883
    [ Info: Train Loss: 1.8231257, Val Loss: 2.077878
    [ Info: Train Loss: 1.8426485, Val Loss: 2.0822954
    [ Info: Train Loss: 1.8010774, Val Loss: 2.0722723
    [ Info: Train Loss: 1.8078551, Val Loss: 2.0517821
    [ Info: Train Loss: 1.8137381, Val Loss: 2.1038642
    [ Info: Train Loss: 1.8033065, Val Loss: 2.0715206
    [ Info: Train Loss: 1.7908123, Val Loss: 2.0784037
    [ Info: Train Loss: 1.7791256, Val Loss: 2.058735
    [ Info: Train Loss: 1.7790664, Val Loss: 2.0730052
    [ Info: Train Loss: 1.8065618, Val Loss: 2.081089
    [ Info: Train Loss: 1.806606, Val Loss: 2.0855896
    [ Info: Train Loss: 1.7907915, Val Loss: 2.0717375
    [ Info: Train Loss: 1.7636669, Val Loss: 2.0648215
    [ Info: Train Loss: 1.76411, Val Loss: 2.0652728
    [ Info: Train Loss: 1.7750646, Val Loss: 2.057261
    [ Info: Train Loss: 1.746128, Val Loss: 2.063495
    [ Info: Train Loss: 1.7507931, Val Loss: 2.0503523
    [ Info: Train Loss: 1.7598104, Val Loss: 2.052884
    [ Info: Train Loss: 1.763704, Val Loss: 2.033104
    [ Info: Train Loss: 1.7361344, Val Loss: 2.0438612
    [ Info: Train Loss: 1.7480404, Val Loss: 2.0504959
    [ Info: Train Loss: 1.7418566, Val Loss: 2.0372424
    [ Info: Train Loss: 1.7333186, Val Loss: 2.0370202
    [ Info: Train Loss: 1.7357863, Val Loss: 2.0487626
    [ Info: Train Loss: 1.7316349, Val Loss: 2.068049
    [ Info: Train Loss: 1.722281, Val Loss: 2.0725887
    [ Info: Train Loss: 1.720167, Val Loss: 2.0383306
    [ Info: Train Loss: 1.7335465, Val Loss: 2.0344255
    [ Info: Train Loss: 1.7281626, Val Loss: 2.0483496
    [ Info: Train Loss: 1.7171172, Val Loss: 2.0390682
    [ Info: Train Loss: 1.7396109, Val Loss: 2.0438938
    [ Info: Train Loss: 1.7141985, Val Loss: 2.0546248
    [ Info: Train Loss: 1.7239172, Val Loss: 2.0498676
    [ Info: Train Loss: 1.7217245, Val Loss: 2.0324197
    [ Info: Train Loss: 1.6997478, Val Loss: 2.037931
    [ Info: Train Loss: 1.7096909, Val Loss: 2.046294
    [ Info: Train Loss: 1.7162064, Val Loss: 2.0405195
    [ Info: Train Loss: 1.7022381, Val Loss: 2.069118
    [ Info: Train Loss: 1.7016252, Val Loss: 2.029191
    [ Info: Train Loss: 1.6949564, Val Loss: 2.037418
    [ Info: Train Loss: 1.7046666, Val Loss: 2.0562437
    [ Info: Train Loss: 1.70282, Val Loss: 2.054844
    [ Info: Train Loss: 1.7028077, Val Loss: 2.0440593
    [ Info: Train Loss: 1.6855196, Val Loss: 2.0636458
    [ Info: Train Loss: 1.6911688, Val Loss: 2.022713
    [ Info: Train Loss: 1.7089508, Val Loss: 2.0428846
    [ Info: Train Loss: 1.7001234, Val Loss: 2.0379603
    [ Info: Train Loss: 1.6844562, Val Loss: 2.0555046
    [ Info: Train Loss: 1.6850165, Val Loss: 2.0249376
    [ Info: Train Loss: 1.6774806, Val Loss: 2.024642
    [ Info: Train Loss: 1.6732417, Val Loss: 2.0571303
    [ Info: Train Loss: 1.6991328, Val Loss: 2.0451713
    [ Info: Train Loss: 1.6776098, Val Loss: 2.0474555
    [ Info: Train Loss: 1.6890179, Val Loss: 2.0362225
    [ Info: Train Loss: 1.6613668, Val Loss: 2.0548596
    [ Info: Train Loss: 1.6635011, Val Loss: 2.0376565
    [ Info: Train Loss: 1.662441, Val Loss: 2.0445817
    [ Info: Train Loss: 1.6780094, Val Loss: 2.0266523
    [ Info: Train Loss: 1.6712301, Val Loss: 2.0285335
    [ Info: Train Loss: 1.6786494, Val Loss: 2.0372517
    [ Info: Train Loss: 1.6722633, Val Loss: 2.0376847
    [ Info: Train Loss: 1.6780833, Val Loss: 2.0292208
    [ Info: Train Loss: 1.6620464, Val Loss: 2.0444481</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip460">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip460)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip461">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip460)" d="M185.927 1423.18 L2352.76 1423.18 L2352.76 47.2441 L185.927 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip462">
    <rect x="185" y="47" width="2168" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="226.604,1423.18 226.604,47.2441 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="742.811,1423.18 742.811,47.2441 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1259.02,1423.18 1259.02,47.2441 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1775.22,1423.18 1775.22,47.2441 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2291.43,1423.18 2291.43,47.2441 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="185.927,1301.94 2352.76,1301.94 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="185.927,1038.91 2352.76,1038.91 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="185.927,775.885 2352.76,775.885 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="185.927,512.86 2352.76,512.86 "/>
<polyline clip-path="url(#clip462)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="185.927,249.835 2352.76,249.835 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="226.604,1423.18 226.604,1404.28 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="742.811,1423.18 742.811,1404.28 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1259.02,1423.18 1259.02,1404.28 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1775.22,1423.18 1775.22,1404.28 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2291.43,1423.18 2291.43,1404.28 "/>
<path clip-path="url(#clip460)" d="M226.604 1454.1 Q222.993 1454.1 221.164 1457.66 Q219.359 1461.2 219.359 1468.33 Q219.359 1475.44 221.164 1479.01 Q222.993 1482.55 226.604 1482.55 Q230.238 1482.55 232.044 1479.01 Q233.872 1475.44 233.872 1468.33 Q233.872 1461.2 232.044 1457.66 Q230.238 1454.1 226.604 1454.1 M226.604 1450.39 Q232.414 1450.39 235.47 1455 Q238.548 1459.58 238.548 1468.33 Q238.548 1477.06 235.47 1481.67 Q232.414 1486.25 226.604 1486.25 Q220.794 1486.25 217.715 1481.67 Q214.659 1477.06 214.659 1468.33 Q214.659 1459.58 217.715 1455 Q220.794 1450.39 226.604 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M722.081 1481.64 L738.401 1481.64 L738.401 1485.58 L716.457 1485.58 L716.457 1481.64 Q719.119 1478.89 723.702 1474.26 Q728.308 1469.61 729.489 1468.27 Q731.734 1465.74 732.614 1464.01 Q733.517 1462.25 733.517 1460.56 Q733.517 1457.8 731.572 1456.07 Q729.651 1454.33 726.549 1454.33 Q724.35 1454.33 721.896 1455.09 Q719.466 1455.86 716.688 1457.41 L716.688 1452.69 Q719.512 1451.55 721.966 1450.97 Q724.419 1450.39 726.456 1450.39 Q731.827 1450.39 735.021 1453.08 Q738.216 1455.77 738.216 1460.26 Q738.216 1462.39 737.405 1464.31 Q736.618 1466.2 734.512 1468.8 Q733.933 1469.47 730.831 1472.69 Q727.73 1475.88 722.081 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M748.262 1451.02 L766.618 1451.02 L766.618 1454.96 L752.544 1454.96 L752.544 1463.43 Q753.563 1463.08 754.581 1462.92 Q755.6 1462.73 756.618 1462.73 Q762.405 1462.73 765.785 1465.9 Q769.165 1469.08 769.165 1474.49 Q769.165 1480.07 765.692 1483.17 Q762.22 1486.25 755.901 1486.25 Q753.725 1486.25 751.456 1485.88 Q749.211 1485.51 746.804 1484.77 L746.804 1480.07 Q748.887 1481.2 751.109 1481.76 Q753.331 1482.32 755.808 1482.32 Q759.813 1482.32 762.151 1480.21 Q764.489 1478.1 764.489 1474.49 Q764.489 1470.88 762.151 1468.77 Q759.813 1466.67 755.808 1466.67 Q753.933 1466.67 752.058 1467.08 Q750.206 1467.5 748.262 1468.38 L748.262 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1233.72 1451.02 L1252.07 1451.02 L1252.07 1454.96 L1238 1454.96 L1238 1463.43 Q1239.02 1463.08 1240.04 1462.92 Q1241.05 1462.73 1242.07 1462.73 Q1247.86 1462.73 1251.24 1465.9 Q1254.62 1469.08 1254.62 1474.49 Q1254.62 1480.07 1251.15 1483.17 Q1247.67 1486.25 1241.36 1486.25 Q1239.18 1486.25 1236.91 1485.88 Q1234.67 1485.51 1232.26 1484.77 L1232.26 1480.07 Q1234.34 1481.2 1236.56 1481.76 Q1238.79 1482.32 1241.26 1482.32 Q1245.27 1482.32 1247.61 1480.21 Q1249.94 1478.1 1249.94 1474.49 Q1249.94 1470.88 1247.61 1468.77 Q1245.27 1466.67 1241.26 1466.67 Q1239.39 1466.67 1237.51 1467.08 Q1235.66 1467.5 1233.72 1468.38 L1233.72 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1273.83 1454.1 Q1270.22 1454.1 1268.39 1457.66 Q1266.59 1461.2 1266.59 1468.33 Q1266.59 1475.44 1268.39 1479.01 Q1270.22 1482.55 1273.83 1482.55 Q1277.47 1482.55 1279.27 1479.01 Q1281.1 1475.44 1281.1 1468.33 Q1281.1 1461.2 1279.27 1457.66 Q1277.47 1454.1 1273.83 1454.1 M1273.83 1450.39 Q1279.64 1450.39 1282.7 1455 Q1285.78 1459.58 1285.78 1468.33 Q1285.78 1477.06 1282.7 1481.67 Q1279.64 1486.25 1273.83 1486.25 Q1268.02 1486.25 1264.94 1481.67 Q1261.89 1477.06 1261.89 1468.33 Q1261.89 1459.58 1264.94 1455 Q1268.02 1450.39 1273.83 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1749.08 1451.02 L1771.3 1451.02 L1771.3 1453.01 L1758.75 1485.58 L1753.87 1485.58 L1765.68 1454.96 L1749.08 1454.96 L1749.08 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1780.47 1451.02 L1798.82 1451.02 L1798.82 1454.96 L1784.75 1454.96 L1784.75 1463.43 Q1785.77 1463.08 1786.79 1462.92 Q1787.8 1462.73 1788.82 1462.73 Q1794.61 1462.73 1797.99 1465.9 Q1801.37 1469.08 1801.37 1474.49 Q1801.37 1480.07 1797.9 1483.17 Q1794.43 1486.25 1788.11 1486.25 Q1785.93 1486.25 1783.66 1485.88 Q1781.42 1485.51 1779.01 1484.77 L1779.01 1480.07 Q1781.09 1481.2 1783.31 1481.76 Q1785.54 1482.32 1788.01 1482.32 Q1792.02 1482.32 1794.36 1480.21 Q1796.69 1478.1 1796.69 1474.49 Q1796.69 1470.88 1794.36 1468.77 Q1792.02 1466.67 1788.01 1466.67 Q1786.14 1466.67 1784.26 1467.08 Q1782.41 1467.5 1780.47 1468.38 L1780.47 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2251.04 1481.64 L2258.68 1481.64 L2258.68 1455.28 L2250.37 1456.95 L2250.37 1452.69 L2258.63 1451.02 L2263.31 1451.02 L2263.31 1481.64 L2270.94 1481.64 L2270.94 1485.58 L2251.04 1485.58 L2251.04 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2290.39 1454.1 Q2286.78 1454.1 2284.95 1457.66 Q2283.14 1461.2 2283.14 1468.33 Q2283.14 1475.44 2284.95 1479.01 Q2286.78 1482.55 2290.39 1482.55 Q2294.02 1482.55 2295.83 1479.01 Q2297.66 1475.44 2297.66 1468.33 Q2297.66 1461.2 2295.83 1457.66 Q2294.02 1454.1 2290.39 1454.1 M2290.39 1450.39 Q2296.2 1450.39 2299.25 1455 Q2302.33 1459.58 2302.33 1468.33 Q2302.33 1477.06 2299.25 1481.67 Q2296.2 1486.25 2290.39 1486.25 Q2284.58 1486.25 2281.5 1481.67 Q2278.44 1477.06 2278.44 1468.33 Q2278.44 1459.58 2281.5 1455 Q2284.58 1450.39 2290.39 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2320.55 1454.1 Q2316.94 1454.1 2315.11 1457.66 Q2313.31 1461.2 2313.31 1468.33 Q2313.31 1475.44 2315.11 1479.01 Q2316.94 1482.55 2320.55 1482.55 Q2324.19 1482.55 2325.99 1479.01 Q2327.82 1475.44 2327.82 1468.33 Q2327.82 1461.2 2325.99 1457.66 Q2324.19 1454.1 2320.55 1454.1 M2320.55 1450.39 Q2326.36 1450.39 2329.42 1455 Q2332.5 1459.58 2332.5 1468.33 Q2332.5 1477.06 2329.42 1481.67 Q2326.36 1486.25 2320.55 1486.25 Q2314.74 1486.25 2311.66 1481.67 Q2308.61 1477.06 2308.61 1468.33 Q2308.61 1459.58 2311.66 1455 Q2314.74 1450.39 2320.55 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1189.53 1548.76 L1189.53 1551.62 L1162.6 1551.62 Q1162.99 1557.67 1166.23 1560.85 Q1169.51 1564 1175.34 1564 Q1178.71 1564 1181.86 1563.17 Q1185.04 1562.35 1188.16 1560.69 L1188.16 1566.23 Q1185.01 1567.57 1181.7 1568.27 Q1178.39 1568.97 1174.99 1568.97 Q1166.46 1568.97 1161.46 1564 Q1156.49 1559.04 1156.49 1550.57 Q1156.49 1541.82 1161.2 1536.69 Q1165.95 1531.54 1173.97 1531.54 Q1181.16 1531.54 1185.33 1536.18 Q1189.53 1540.8 1189.53 1548.76 M1183.67 1547.04 Q1183.61 1542.23 1180.97 1539.37 Q1178.36 1536.5 1174.03 1536.5 Q1169.13 1536.5 1166.17 1539.27 Q1163.24 1542.04 1162.8 1547.07 L1183.67 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1204.81 1562.7 L1204.81 1581.6 L1198.92 1581.6 L1198.92 1532.4 L1204.81 1532.4 L1204.81 1537.81 Q1206.66 1534.62 1209.46 1533.1 Q1212.29 1531.54 1216.2 1531.54 Q1222.7 1531.54 1226.74 1536.69 Q1230.81 1541.85 1230.81 1550.25 Q1230.81 1558.65 1226.74 1563.81 Q1222.7 1568.97 1216.2 1568.97 Q1212.29 1568.97 1209.46 1567.44 Q1206.66 1565.88 1204.81 1562.7 M1224.73 1550.25 Q1224.73 1543.79 1222.06 1540.13 Q1219.42 1536.44 1214.77 1536.44 Q1210.12 1536.44 1207.45 1540.13 Q1204.81 1543.79 1204.81 1550.25 Q1204.81 1556.71 1207.45 1560.4 Q1210.12 1564.07 1214.77 1564.07 Q1219.42 1564.07 1222.06 1560.4 Q1224.73 1556.71 1224.73 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1254.33 1536.5 Q1249.62 1536.5 1246.89 1540.19 Q1244.15 1543.85 1244.15 1550.25 Q1244.15 1556.65 1246.85 1560.34 Q1249.59 1564 1254.33 1564 Q1259.01 1564 1261.75 1560.31 Q1264.49 1556.62 1264.49 1550.25 Q1264.49 1543.92 1261.75 1540.23 Q1259.01 1536.5 1254.33 1536.5 M1254.33 1531.54 Q1261.97 1531.54 1266.33 1536.5 Q1270.69 1541.47 1270.69 1550.25 Q1270.69 1559 1266.33 1564 Q1261.97 1568.97 1254.33 1568.97 Q1246.66 1568.97 1242.3 1564 Q1237.97 1559 1237.97 1550.25 Q1237.97 1541.47 1242.3 1536.5 Q1246.66 1531.54 1254.33 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1306.06 1533.76 L1306.06 1539.24 Q1303.57 1537.87 1301.06 1537.2 Q1298.58 1536.5 1296.03 1536.5 Q1290.33 1536.5 1287.18 1540.13 Q1284.03 1543.73 1284.03 1550.25 Q1284.03 1556.78 1287.18 1560.4 Q1290.33 1564 1296.03 1564 Q1298.58 1564 1301.06 1563.33 Q1303.57 1562.63 1306.06 1561.26 L1306.06 1566.68 Q1303.6 1567.82 1300.96 1568.39 Q1298.35 1568.97 1295.39 1568.97 Q1287.34 1568.97 1282.6 1563.91 Q1277.86 1558.85 1277.86 1550.25 Q1277.86 1541.53 1282.63 1536.53 Q1287.44 1531.54 1295.77 1531.54 Q1298.48 1531.54 1301.06 1532.11 Q1303.64 1532.65 1306.06 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1345.87 1546.53 L1345.87 1568.04 L1340.02 1568.04 L1340.02 1546.72 Q1340.02 1541.66 1338.04 1539.14 Q1336.07 1536.63 1332.12 1536.63 Q1327.38 1536.63 1324.64 1539.65 Q1321.91 1542.68 1321.91 1547.9 L1321.91 1568.04 L1316.02 1568.04 L1316.02 1518.52 L1321.91 1518.52 L1321.91 1537.93 Q1324.01 1534.72 1326.84 1533.13 Q1329.7 1531.54 1333.43 1531.54 Q1339.57 1531.54 1342.72 1535.36 Q1345.87 1539.14 1345.87 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M1380.28 1533.45 L1380.28 1538.98 Q1377.8 1537.71 1375.12 1537.07 Q1372.45 1536.44 1369.59 1536.44 Q1365.22 1536.44 1363.03 1537.77 Q1360.86 1539.11 1360.86 1541.79 Q1360.86 1543.82 1362.42 1545 Q1363.98 1546.15 1368.69 1547.2 L1370.7 1547.64 Q1376.94 1548.98 1379.55 1551.43 Q1382.19 1553.85 1382.19 1558.21 Q1382.19 1563.17 1378.24 1566.07 Q1374.33 1568.97 1367.45 1568.97 Q1364.59 1568.97 1361.47 1568.39 Q1358.38 1567.85 1354.94 1566.74 L1354.94 1560.69 Q1358.19 1562.38 1361.34 1563.24 Q1364.49 1564.07 1367.58 1564.07 Q1371.72 1564.07 1373.95 1562.66 Q1376.17 1561.23 1376.17 1558.65 Q1376.17 1556.27 1374.55 1554.99 Q1372.96 1553.72 1367.52 1552.54 L1365.48 1552.07 Q1360.04 1550.92 1357.62 1548.56 Q1355.2 1546.18 1355.2 1542.04 Q1355.2 1537.01 1358.76 1534.27 Q1362.33 1531.54 1368.89 1531.54 Q1372.13 1531.54 1375 1532.01 Q1377.86 1532.49 1380.28 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,1423.18 185.927,47.2441 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,1301.94 204.824,1301.94 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,1038.91 204.824,1038.91 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,775.885 204.824,775.885 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,512.86 204.824,512.86 "/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="185.927,249.835 204.824,249.835 "/>
<path clip-path="url(#clip460)" d="M54.3949 1315.28 L62.0337 1315.28 L62.0337 1288.91 L53.7236 1290.58 L53.7236 1286.32 L61.9874 1284.66 L66.6633 1284.66 L66.6633 1315.28 L74.3022 1315.28 L74.3022 1319.22 L54.3949 1319.22 L54.3949 1315.28 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M83.7466 1313.34 L88.6308 1313.34 L88.6308 1319.22 L83.7466 1319.22 L83.7466 1313.34 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M97.6354 1284.66 L119.857 1284.66 L119.857 1286.65 L107.311 1319.22 L102.427 1319.22 L114.233 1288.59 L97.6354 1288.59 L97.6354 1284.66 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M129.024 1284.66 L147.38 1284.66 L147.38 1288.59 L133.306 1288.59 L133.306 1297.06 Q134.325 1296.72 135.344 1296.55 Q136.362 1296.37 137.381 1296.37 Q143.168 1296.37 146.547 1299.54 Q149.927 1302.71 149.927 1308.13 Q149.927 1313.71 146.455 1316.81 Q142.982 1319.89 136.663 1319.89 Q134.487 1319.89 132.219 1319.52 Q129.973 1319.15 127.566 1318.41 L127.566 1313.71 Q129.649 1314.84 131.871 1315.4 Q134.094 1315.95 136.57 1315.95 Q140.575 1315.95 142.913 1313.85 Q145.251 1311.74 145.251 1308.13 Q145.251 1304.52 142.913 1302.41 Q140.575 1300.3 136.57 1300.3 Q134.695 1300.3 132.82 1300.72 Q130.969 1301.14 129.024 1302.02 L129.024 1284.66 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M56.6171 1052.26 L72.9365 1052.26 L72.9365 1056.19 L50.9921 1056.19 L50.9921 1052.26 Q53.6541 1049.5 58.2375 1044.87 Q62.8439 1040.22 64.0245 1038.88 Q66.2698 1036.35 67.1494 1034.62 Q68.0522 1032.86 68.0522 1031.17 Q68.0522 1028.41 66.1078 1026.68 Q64.1865 1024.94 61.0847 1024.94 Q58.8856 1024.94 56.4319 1025.7 Q54.0014 1026.47 51.2236 1028.02 L51.2236 1023.3 Q54.0477 1022.16 56.5014 1021.58 Q58.955 1021.01 60.9921 1021.01 Q66.3624 1021.01 69.5568 1023.69 Q72.7513 1026.38 72.7513 1030.87 Q72.7513 1033 71.9411 1034.92 Q71.1541 1036.82 69.0476 1039.41 Q68.4689 1040.08 65.367 1043.3 Q62.2652 1046.49 56.6171 1052.26 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M82.7512 1050.31 L87.6354 1050.31 L87.6354 1056.19 L82.7512 1056.19 L82.7512 1050.31 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M107.821 1024.71 Q104.209 1024.71 102.381 1028.27 Q100.575 1031.82 100.575 1038.95 Q100.575 1046.05 102.381 1049.62 Q104.209 1053.16 107.821 1053.16 Q111.455 1053.16 113.26 1049.62 Q115.089 1046.05 115.089 1038.95 Q115.089 1031.82 113.26 1028.27 Q111.455 1024.71 107.821 1024.71 M107.821 1021.01 Q113.631 1021.01 116.686 1025.61 Q119.765 1030.2 119.765 1038.95 Q119.765 1047.67 116.686 1052.28 Q113.631 1056.86 107.821 1056.86 Q102.01 1056.86 98.9317 1052.28 Q95.8761 1047.67 95.8761 1038.95 Q95.8761 1030.2 98.9317 1025.61 Q102.01 1021.01 107.821 1021.01 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M137.982 1024.71 Q134.371 1024.71 132.543 1028.27 Q130.737 1031.82 130.737 1038.95 Q130.737 1046.05 132.543 1049.62 Q134.371 1053.16 137.982 1053.16 Q141.617 1053.16 143.422 1049.62 Q145.251 1046.05 145.251 1038.95 Q145.251 1031.82 143.422 1028.27 Q141.617 1024.71 137.982 1024.71 M137.982 1021.01 Q143.793 1021.01 146.848 1025.61 Q149.927 1030.2 149.927 1038.95 Q149.927 1047.67 146.848 1052.28 Q143.793 1056.86 137.982 1056.86 Q132.172 1056.86 129.094 1052.28 Q126.038 1047.67 126.038 1038.95 Q126.038 1030.2 129.094 1025.61 Q132.172 1021.01 137.982 1021.01 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M57.6125 789.23 L73.9318 789.23 L73.9318 793.165 L51.9875 793.165 L51.9875 789.23 Q54.6495 786.475 59.2328 781.846 Q63.8393 777.193 65.0198 775.85 Q67.2652 773.327 68.1448 771.591 Q69.0476 769.832 69.0476 768.142 Q69.0476 765.387 67.1032 763.651 Q65.1819 761.915 62.08 761.915 Q59.881 761.915 57.4273 762.679 Q54.9967 763.443 52.219 764.994 L52.219 760.272 Q55.043 759.138 57.4967 758.559 Q59.9504 757.98 61.9874 757.98 Q67.3578 757.98 70.5522 760.665 Q73.7466 763.35 73.7466 767.841 Q73.7466 769.971 72.9365 771.892 Q72.1494 773.79 70.0429 776.383 Q69.4642 777.054 66.3624 780.272 Q63.2606 783.466 57.6125 789.23 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M83.7466 787.286 L88.6308 787.286 L88.6308 793.165 L83.7466 793.165 L83.7466 787.286 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M102.844 789.23 L119.163 789.23 L119.163 793.165 L97.2187 793.165 L97.2187 789.23 Q99.8808 786.475 104.464 781.846 Q109.071 777.193 110.251 775.85 Q112.496 773.327 113.376 771.591 Q114.279 769.832 114.279 768.142 Q114.279 765.387 112.334 763.651 Q110.413 761.915 107.311 761.915 Q105.112 761.915 102.659 762.679 Q100.228 763.443 97.4502 764.994 L97.4502 760.272 Q100.274 759.138 102.728 758.559 Q105.182 757.98 107.219 757.98 Q112.589 757.98 115.783 760.665 Q118.978 763.35 118.978 767.841 Q118.978 769.971 118.168 771.892 Q117.381 773.79 115.274 776.383 Q114.695 777.054 111.594 780.272 Q108.492 783.466 102.844 789.23 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M129.024 758.605 L147.38 758.605 L147.38 762.54 L133.306 762.54 L133.306 771.012 Q134.325 770.665 135.344 770.503 Q136.362 770.318 137.381 770.318 Q143.168 770.318 146.547 773.489 Q149.927 776.661 149.927 782.077 Q149.927 787.656 146.455 790.758 Q142.982 793.836 136.663 793.836 Q134.487 793.836 132.219 793.466 Q129.973 793.096 127.566 792.355 L127.566 787.656 Q129.649 788.79 131.871 789.346 Q134.094 789.901 136.57 789.901 Q140.575 789.901 142.913 787.795 Q145.251 785.688 145.251 782.077 Q145.251 778.466 142.913 776.36 Q140.575 774.253 136.57 774.253 Q134.695 774.253 132.82 774.67 Q130.969 775.087 129.024 775.966 L129.024 758.605 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M56.6171 526.205 L72.9365 526.205 L72.9365 530.14 L50.9921 530.14 L50.9921 526.205 Q53.6541 523.45 58.2375 518.82 Q62.8439 514.168 64.0245 512.825 Q66.2698 510.302 67.1494 508.566 Q68.0522 506.807 68.0522 505.117 Q68.0522 502.362 66.1078 500.626 Q64.1865 498.89 61.0847 498.89 Q58.8856 498.89 56.4319 499.654 Q54.0014 500.418 51.2236 501.969 L51.2236 497.246 Q54.0477 496.112 56.5014 495.534 Q58.955 494.955 60.9921 494.955 Q66.3624 494.955 69.5568 497.64 Q72.7513 500.325 72.7513 504.816 Q72.7513 506.945 71.9411 508.867 Q71.1541 510.765 69.0476 513.357 Q68.4689 514.029 65.367 517.246 Q62.2652 520.441 56.6171 526.205 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M82.7512 524.26 L87.6354 524.26 L87.6354 530.14 L82.7512 530.14 L82.7512 524.26 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M97.8669 495.58 L116.223 495.58 L116.223 499.515 L102.149 499.515 L102.149 507.987 Q103.168 507.64 104.186 507.478 Q105.205 507.293 106.223 507.293 Q112.01 507.293 115.39 510.464 Q118.77 513.635 118.77 519.052 Q118.77 524.631 115.297 527.732 Q111.825 530.811 105.506 530.811 Q103.33 530.811 101.061 530.441 Q98.8159 530.07 96.4085 529.33 L96.4085 524.631 Q98.4919 525.765 100.714 526.32 Q102.936 526.876 105.413 526.876 Q109.418 526.876 111.756 524.769 Q114.094 522.663 114.094 519.052 Q114.094 515.441 111.756 513.334 Q109.418 511.228 105.413 511.228 Q103.538 511.228 101.663 511.645 Q99.8113 512.061 97.8669 512.941 L97.8669 495.58 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M137.982 498.659 Q134.371 498.659 132.543 502.223 Q130.737 505.765 130.737 512.895 Q130.737 520.001 132.543 523.566 Q134.371 527.107 137.982 527.107 Q141.617 527.107 143.422 523.566 Q145.251 520.001 145.251 512.895 Q145.251 505.765 143.422 502.223 Q141.617 498.659 137.982 498.659 M137.982 494.955 Q143.793 494.955 146.848 499.561 Q149.927 504.145 149.927 512.895 Q149.927 521.621 146.848 526.228 Q143.793 530.811 137.982 530.811 Q132.172 530.811 129.094 526.228 Q126.038 521.621 126.038 512.895 Q126.038 504.145 129.094 499.561 Q132.172 494.955 137.982 494.955 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M57.6125 263.179 L73.9318 263.179 L73.9318 267.115 L51.9875 267.115 L51.9875 263.179 Q54.6495 260.425 59.2328 255.795 Q63.8393 251.142 65.0198 249.8 Q67.2652 247.277 68.1448 245.541 Q69.0476 243.781 69.0476 242.091 Q69.0476 239.337 67.1032 237.601 Q65.1819 235.865 62.08 235.865 Q59.881 235.865 57.4273 236.629 Q54.9967 237.392 52.219 238.943 L52.219 234.221 Q55.043 233.087 57.4967 232.508 Q59.9504 231.93 61.9874 231.93 Q67.3578 231.93 70.5522 234.615 Q73.7466 237.3 73.7466 241.791 Q73.7466 243.92 72.9365 245.841 Q72.1494 247.74 70.0429 250.332 Q69.4642 251.003 66.3624 254.221 Q63.2606 257.415 57.6125 263.179 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M83.7466 261.235 L88.6308 261.235 L88.6308 267.115 L83.7466 267.115 L83.7466 261.235 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M97.6354 232.555 L119.857 232.555 L119.857 234.545 L107.311 267.115 L102.427 267.115 L114.233 236.49 L97.6354 236.49 L97.6354 232.555 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M129.024 232.555 L147.38 232.555 L147.38 236.49 L133.306 236.49 L133.306 244.962 Q134.325 244.615 135.344 244.453 Q136.362 244.267 137.381 244.267 Q143.168 244.267 146.547 247.439 Q149.927 250.61 149.927 256.027 Q149.927 261.605 146.455 264.707 Q142.982 267.786 136.663 267.786 Q134.487 267.786 132.219 267.415 Q129.973 267.045 127.566 266.304 L127.566 261.605 Q129.649 262.74 131.871 263.295 Q134.094 263.851 136.57 263.851 Q140.575 263.851 142.913 261.744 Q145.251 259.638 145.251 256.027 Q145.251 252.416 142.913 250.309 Q140.575 248.203 136.57 248.203 Q134.695 248.203 132.82 248.619 Q130.969 249.036 129.024 249.916 L129.024 232.555 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip462)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="247.252,86.1857 267.9,364.658 288.549,548.108 309.197,617.227 329.845,659.628 350.493,695.044 371.142,735.378 391.79,765.796 412.438,799.048 433.087,828.998 453.735,853.404 474.383,880.172 495.031,904.432 515.68,930.189 536.328,951.914 556.976,972.771 577.624,991.909 598.273,1012.01 618.921,1026.48 639.569,1043.4 660.217,1057.57 680.866,1071.21 701.514,1086.69 722.162,1097.79 742.811,1109.03 763.459,1122.62 784.107,1131.87 804.755,1142.45 825.404,1152.95 846.052,1164.47 866.7,1169.92 887.348,1179.79 907.997,1187.68 928.645,1200.19 949.293,1205.26 969.941,1211.3 990.59,1218.08 1011.24,1221.32 1031.89,1229.14 1052.53,1234 1073.18,1237.33 1093.83,1244.05 1114.48,1249.32 1135.13,1254.56 1155.78,1257.85 1176.42,1262.63 1197.07,1265.56 1217.72,1266.86 1238.37,1273.84 1259.02,1276.52 1279.67,1281.25 1300.31,1283.03 1320.96,1290.62 1341.61,1291.79 1362.26,1294.65 1382.91,1296.57 1403.56,1301.41 1424.2,1299.18 1444.85,1307.69 1465.5,1307.21 1486.15,1310.65 1506.8,1312.38 1527.44,1316.67 1548.09,1318.55 1568.74,1321.62 1589.39,1325.3 1610.04,1327.25 1630.69,1328.14 1651.33,1332.44 1671.98,1329.8 1692.63,1334.43 1713.28,1340.74 1733.93,1340.29 1754.58,1340.88 1775.22,1339.93 1795.87,1345.78 1816.52,1351.19 1837.17,1349.3 1857.82,1348.1 1878.47,1351.22 1899.11,1356.54 1919.76,1355.42 1940.41,1357.14 1961.06,1359.03 1981.71,1362.02 2002.35,1363.51 2023,1364.22 2043.65,1366.33 2064.3,1367.12 2084.95,1372.98 2105.6,1368.74 2126.24,1372.65 2146.89,1376.2 2167.54,1376.6 2188.19,1376.27 2208.84,1375.9 2229.49,1378.7 2250.13,1381.25 2270.78,1382.19 2291.43,1384.24 "/>
<polyline clip-path="url(#clip462)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="247.252,236.556 267.9,515.07 288.549,614.537 309.197,655.919 329.845,693.097 350.493,698.729 371.142,745.798 391.79,769.757 412.438,790.486 433.087,807.92 453.735,822.841 474.383,848.135 495.031,859.722 515.68,873.504 536.328,886.173 556.976,910.023 577.624,903.669 598.273,925.71 618.921,933.487 639.569,929.137 660.217,930.611 680.866,934.807 701.514,942.474 722.162,965.597 742.811,957.367 763.459,956.946 784.107,972.087 804.755,990.071 825.404,1002.67 846.052,980.954 866.7,1003.1 887.348,1016.46 907.997,1011.13 928.645,1011.58 949.293,1005.53 969.941,1013.16 990.59,1006.2 1011.24,1005.6 1031.89,1010.09 1052.53,1027.29 1073.18,1034.73 1093.83,1005.35 1114.48,1023.11 1135.13,1014.95 1155.78,1020.97 1176.42,1011.6 1197.07,1016.34 1217.72,999.359 1238.37,1023.84 1259.02,1023.33 1279.67,1017.4 1300.31,1034.47 1320.96,1034.94 1341.61,1034.39 1362.26,1031.9 1382.91,1041.2 1403.56,1036.41 1424.2,1045.25 1444.85,1052.64 1465.5,1053.92 1486.15,1035.68 1506.8,1022.31 1527.44,1032.69 1548.09,1046.5 1568.74,1046.95 1589.39,1040.55 1610.04,1038.99 1630.69,1044.36 1651.33,1033.82 1671.98,1047.46 1692.63,1045.17 1713.28,1045.04 1733.93,1043.91 1754.58,1029.02 1775.22,1024.21 1795.87,1049.78 1816.52,1039.51 1837.17,1030.65 1857.82,1022.84 1878.47,1033.12 1899.11,1026.53 1919.76,1035.11 1940.41,1028.09 1961.06,1039.76 1981.71,1043.35 2002.35,1053.86 2023,1044.21 2043.65,1027.71 2064.3,1022.98 2084.95,1027.13 2105.6,1028.51 2126.24,1033.08 2146.89,1034.3 2167.54,1037.91 2188.19,1027.9 2208.84,1034.56 2229.49,1022 2250.13,1058.9 2270.78,1044.16 2291.43,1046.65 "/>
<path clip-path="url(#clip460)" d="M1841.81 248.629 L2280.53 248.629 L2280.53 93.1086 L1841.81 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip460)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1841.81,248.629 2280.53,248.629 2280.53,93.1086 1841.81,93.1086 1841.81,248.629 "/>
<polyline clip-path="url(#clip460)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1865.89,144.949 2010.34,144.949 "/>
<path clip-path="url(#clip460)" d="M2041.82 128.942 L2041.82 136.303 L2050.6 136.303 L2050.6 139.613 L2041.82 139.613 L2041.82 153.687 Q2041.82 156.858 2042.68 157.761 Q2043.56 158.664 2046.22 158.664 L2050.6 158.664 L2050.6 162.229 L2046.22 162.229 Q2041.29 162.229 2039.42 160.4 Q2037.54 158.548 2037.54 153.687 L2037.54 139.613 L2034.42 139.613 L2034.42 136.303 L2037.54 136.303 L2037.54 128.942 L2041.82 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2071.22 140.284 Q2070.5 139.868 2069.65 139.682 Q2068.81 139.474 2067.8 139.474 Q2064.18 139.474 2062.24 141.835 Q2060.32 144.173 2060.32 148.571 L2060.32 162.229 L2056.04 162.229 L2056.04 136.303 L2060.32 136.303 L2060.32 140.331 Q2061.66 137.969 2063.81 136.835 Q2065.97 135.678 2069.05 135.678 Q2069.49 135.678 2070.02 135.747 Q2070.55 135.794 2071.2 135.909 L2071.22 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2087.47 149.196 Q2082.31 149.196 2080.32 150.377 Q2078.33 151.557 2078.33 154.405 Q2078.33 156.673 2079.81 158.016 Q2081.31 159.335 2083.88 159.335 Q2087.43 159.335 2089.56 156.835 Q2091.71 154.312 2091.71 150.145 L2091.71 149.196 L2087.47 149.196 M2095.97 147.437 L2095.97 162.229 L2091.71 162.229 L2091.71 158.293 Q2090.25 160.655 2088.07 161.789 Q2085.9 162.9 2082.75 162.9 Q2078.77 162.9 2076.41 160.678 Q2074.07 158.432 2074.07 154.682 Q2074.07 150.307 2076.99 148.085 Q2079.93 145.863 2085.74 145.863 L2091.71 145.863 L2091.71 145.446 Q2091.71 142.507 2089.76 140.909 Q2087.84 139.289 2084.35 139.289 Q2082.12 139.289 2080.02 139.821 Q2077.91 140.354 2075.97 141.419 L2075.97 137.483 Q2078.31 136.581 2080.5 136.141 Q2082.7 135.678 2084.79 135.678 Q2090.41 135.678 2093.19 138.594 Q2095.97 141.511 2095.97 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2104.74 136.303 L2109 136.303 L2109 162.229 L2104.74 162.229 L2104.74 136.303 M2104.74 126.21 L2109 126.21 L2109 131.604 L2104.74 131.604 L2104.74 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2139.46 146.581 L2139.46 162.229 L2135.2 162.229 L2135.2 146.719 Q2135.2 143.039 2133.77 141.21 Q2132.33 139.382 2129.46 139.382 Q2126.01 139.382 2124.02 141.581 Q2122.03 143.78 2122.03 147.576 L2122.03 162.229 L2117.75 162.229 L2117.75 136.303 L2122.03 136.303 L2122.03 140.331 Q2123.56 137.993 2125.62 136.835 Q2127.7 135.678 2130.41 135.678 Q2134.88 135.678 2137.17 138.456 Q2139.46 141.21 2139.46 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2167.66 170.099 L2167.66 173.409 L2143.03 173.409 L2143.03 170.099 L2167.66 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2171.66 126.21 L2175.92 126.21 L2175.92 162.229 L2171.66 162.229 L2171.66 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2194.88 139.289 Q2191.45 139.289 2189.46 141.974 Q2187.47 144.636 2187.47 149.289 Q2187.47 153.942 2189.44 156.627 Q2191.43 159.289 2194.88 159.289 Q2198.28 159.289 2200.27 156.604 Q2202.26 153.918 2202.26 149.289 Q2202.26 144.682 2200.27 141.997 Q2198.28 139.289 2194.88 139.289 M2194.88 135.678 Q2200.43 135.678 2203.61 139.289 Q2206.78 142.9 2206.78 149.289 Q2206.78 155.655 2203.61 159.289 Q2200.43 162.9 2194.88 162.9 Q2189.3 162.9 2186.13 159.289 Q2182.98 155.655 2182.98 149.289 Q2182.98 142.9 2186.13 139.289 Q2189.3 135.678 2194.88 135.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2230.36 137.067 L2230.36 141.094 Q2228.56 140.169 2226.61 139.706 Q2224.67 139.243 2222.59 139.243 Q2219.42 139.243 2217.82 140.215 Q2216.24 141.187 2216.24 143.131 Q2216.24 144.613 2217.38 145.469 Q2218.51 146.303 2221.94 147.067 L2223.4 147.391 Q2227.93 148.363 2229.83 150.145 Q2231.75 151.905 2231.75 155.076 Q2231.75 158.687 2228.88 160.793 Q2226.04 162.9 2221.04 162.9 Q2218.95 162.9 2216.68 162.483 Q2214.44 162.09 2211.94 161.28 L2211.94 156.881 Q2214.3 158.108 2216.59 158.733 Q2218.88 159.335 2221.13 159.335 Q2224.14 159.335 2225.76 158.317 Q2227.38 157.275 2227.38 155.4 Q2227.38 153.664 2226.2 152.738 Q2225.04 151.812 2221.08 150.956 L2219.6 150.608 Q2215.64 149.775 2213.88 148.062 Q2212.12 146.326 2212.12 143.317 Q2212.12 139.659 2214.72 137.669 Q2217.31 135.678 2222.08 135.678 Q2224.44 135.678 2226.52 136.025 Q2228.61 136.372 2230.36 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2255.06 137.067 L2255.06 141.094 Q2253.26 140.169 2251.31 139.706 Q2249.37 139.243 2247.29 139.243 Q2244.11 139.243 2242.52 140.215 Q2240.94 141.187 2240.94 143.131 Q2240.94 144.613 2242.08 145.469 Q2243.21 146.303 2246.64 147.067 L2248.1 147.391 Q2252.63 148.363 2254.53 150.145 Q2256.45 151.905 2256.45 155.076 Q2256.45 158.687 2253.58 160.793 Q2250.73 162.9 2245.73 162.9 Q2243.65 162.9 2241.38 162.483 Q2239.14 162.09 2236.64 161.28 L2236.64 156.881 Q2239 158.108 2241.29 158.733 Q2243.58 159.335 2245.83 159.335 Q2248.84 159.335 2250.46 158.317 Q2252.08 157.275 2252.08 155.4 Q2252.08 153.664 2250.9 152.738 Q2249.74 151.812 2245.78 150.956 L2244.3 150.608 Q2240.34 149.775 2238.58 148.062 Q2236.82 146.326 2236.82 143.317 Q2236.82 139.659 2239.42 137.669 Q2242.01 135.678 2246.78 135.678 Q2249.14 135.678 2251.22 136.025 Q2253.3 136.372 2255.06 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip460)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1865.89,196.789 2010.34,196.789 "/>
<path clip-path="url(#clip460)" d="M2034.42 188.143 L2038.93 188.143 L2047.03 209.902 L2055.13 188.143 L2059.65 188.143 L2049.93 214.069 L2044.14 214.069 L2034.42 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2077.31 201.036 Q2072.15 201.036 2070.16 202.217 Q2068.17 203.397 2068.17 206.245 Q2068.17 208.513 2069.65 209.856 Q2071.15 211.175 2073.72 211.175 Q2077.26 211.175 2079.39 208.675 Q2081.55 206.152 2081.55 201.985 L2081.55 201.036 L2077.31 201.036 M2085.81 199.277 L2085.81 214.069 L2081.55 214.069 L2081.55 210.133 Q2080.09 212.495 2077.91 213.629 Q2075.74 214.74 2072.59 214.74 Q2068.61 214.74 2066.25 212.518 Q2063.91 210.272 2063.91 206.522 Q2063.91 202.147 2066.82 199.925 Q2069.76 197.703 2075.57 197.703 L2081.55 197.703 L2081.55 197.286 Q2081.55 194.347 2079.6 192.749 Q2077.68 191.129 2074.18 191.129 Q2071.96 191.129 2069.86 191.661 Q2067.75 192.194 2065.81 193.259 L2065.81 189.323 Q2068.14 188.421 2070.34 187.981 Q2072.54 187.518 2074.62 187.518 Q2080.25 187.518 2083.03 190.434 Q2085.81 193.351 2085.81 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2094.58 178.05 L2098.84 178.05 L2098.84 214.069 L2094.58 214.069 L2094.58 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2127.45 221.939 L2127.45 225.249 L2102.82 225.249 L2102.82 221.939 L2127.45 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2131.45 178.05 L2135.71 178.05 L2135.71 214.069 L2131.45 214.069 L2131.45 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2154.67 191.129 Q2151.24 191.129 2149.25 193.814 Q2147.26 196.476 2147.26 201.129 Q2147.26 205.782 2149.23 208.467 Q2151.22 211.129 2154.67 211.129 Q2158.07 211.129 2160.06 208.444 Q2162.05 205.758 2162.05 201.129 Q2162.05 196.522 2160.06 193.837 Q2158.07 191.129 2154.67 191.129 M2154.67 187.518 Q2160.23 187.518 2163.4 191.129 Q2166.57 194.74 2166.57 201.129 Q2166.57 207.495 2163.4 211.129 Q2160.23 214.74 2154.67 214.74 Q2149.09 214.74 2145.92 211.129 Q2142.77 207.495 2142.77 201.129 Q2142.77 194.74 2145.92 191.129 Q2149.09 187.518 2154.67 187.518 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2190.16 188.907 L2190.16 192.934 Q2188.35 192.009 2186.41 191.546 Q2184.46 191.083 2182.38 191.083 Q2179.21 191.083 2177.61 192.055 Q2176.04 193.027 2176.04 194.971 Q2176.04 196.453 2177.17 197.309 Q2178.3 198.143 2181.73 198.907 L2183.19 199.231 Q2187.73 200.203 2189.62 201.985 Q2191.55 203.745 2191.55 206.916 Q2191.55 210.527 2188.67 212.633 Q2185.83 214.74 2180.83 214.74 Q2178.74 214.74 2176.48 214.323 Q2174.23 213.93 2171.73 213.12 L2171.73 208.721 Q2174.09 209.948 2176.38 210.573 Q2178.68 211.175 2180.92 211.175 Q2183.93 211.175 2185.55 210.157 Q2187.17 209.115 2187.17 207.24 Q2187.17 205.504 2185.99 204.578 Q2184.83 203.652 2180.87 202.796 L2179.39 202.448 Q2175.43 201.615 2173.68 199.902 Q2171.92 198.166 2171.92 195.157 Q2171.92 191.499 2174.51 189.509 Q2177.1 187.518 2181.87 187.518 Q2184.23 187.518 2186.31 187.865 Q2188.4 188.212 2190.16 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip460)" d="M2214.86 188.907 L2214.86 192.934 Q2213.05 192.009 2211.11 191.546 Q2209.16 191.083 2207.08 191.083 Q2203.91 191.083 2202.31 192.055 Q2200.74 193.027 2200.74 194.971 Q2200.74 196.453 2201.87 197.309 Q2203 198.143 2206.43 198.907 L2207.89 199.231 Q2212.42 200.203 2214.32 201.985 Q2216.24 203.745 2216.24 206.916 Q2216.24 210.527 2213.37 212.633 Q2210.53 214.74 2205.53 214.74 Q2203.44 214.74 2201.17 214.323 Q2198.93 213.93 2196.43 213.12 L2196.43 208.721 Q2198.79 209.948 2201.08 210.573 Q2203.37 211.175 2205.62 211.175 Q2208.63 211.175 2210.25 210.157 Q2211.87 209.115 2211.87 207.24 Q2211.87 205.504 2210.69 204.578 Q2209.53 203.652 2205.57 202.796 L2204.09 202.448 Q2200.13 201.615 2198.37 199.902 Q2196.61 198.166 2196.61 195.157 Q2196.61 191.499 2199.21 189.509 Q2201.8 187.518 2206.57 187.518 Q2208.93 187.518 2211.01 187.865 Q2213.1 188.212 2214.86 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><h2 id="Decoding"><a class="docs-heading-anchor" href="#Decoding">Decoding</a><a id="Decoding-1"></a><a class="docs-heading-anchor-permalink" href="#Decoding" title="Permalink"></a></h2><p>Once a language model has been learned, we can use it not only to predict the next token but to continue predicting each subsequent one, treating the previously predicted token as though it were the next in the input.  Sometimes we will just want to generate text as though we were starting at the beginning  of a document.  However, it is often useful to condition the language model on a user-supplied prefix. For example, if we were developing an autocomplete feature for a search engine or to assist users in writing emails, we would want to feed in what they  had written so far (the prefix),  and then generate a likely continuation.</p><p>The following <code>predict</code> method generates a continuation, one character at a time, after ingesting a user-provided <code>prefix</code>. When looping through the characters in <code>prefix</code>, we keep passing the hidden state to the next time step  but do not generate any output. This is called the <em>warm-up</em> period. After ingesting the prefix, we are now ready to begin emitting the subsequent characters, each of which will be fed back into the model  as the input at the next time step.</p><pre><code class="language-julia hljs">function prediction(prefix, model, vocab, num_preds)
    outputs = [vocab.token_to_idx[string(prefix[1])]]
    state = zeros(32)
    for i in 2:length(prefix)
        x = outputs[end]
        x = reshape(Flux.onehotbatch(x, 1:length(vocab)), :, 1, 1)
        _, state = model.rnn(x, state)
        push!(outputs, vocab.token_to_idx[string(prefix[i])])
    end
    for i in 1:num_preds 
        x = outputs[end]
        x = reshape(Flux.onehotbatch(x, 1:length(vocab)), :, 1, 1)
        out, state = model.rnn(x, state)
        out = output_layer(model, out)
        idx = argmax(softmax(out), dims = 1)[1][1]
        push!(outputs, idx)
    end
    out_chars = map(outputs) do o 
        vocab.idx_to_token[o]
    end
    join(out_chars)
end
</code></pre><pre><code class="nohighlight hljs">prediction (generic function with 1 method)</code></pre><p>In the following, we specify the prefix  and have it generate 20 additional characters.</p><pre><code class="language-julia hljs">prefix = &quot;it has&quot;
prediction(prefix, m, data.vocab, 20)</code></pre><pre><code class="nohighlight hljs">&quot;it has the time traveller &quot;</code></pre><p>While implementing the above RNN model from scratch is instructive, it is not convenient. In the next section, we will see how to leverage deep learning frameworks to whip up RNNs using standard architectures, and to reap performance gains  by relying on highly optimized library functions.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>We can train RNN-based language models to generate text following the user-provided text prefix.  A simple RNN language model consists of input encoding, RNN modeling, and output generation. During training, gradient clipping can mitigate the problem of exploding gradients but does not address the problem of vanishing gradients. In the experiment, we implemented a simple RNN language model and trained it with gradient clipping on sequences of text, tokenized at the character level. By conditioning on a prefix, we can use a language model to generate likely continuations, which proves useful in many applications, e.g., autocomplete features.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Does the implemented language model predict the next token based on all the past tokens up to the very first token in <em>The Time Machine</em>? </li><li>Which hyperparameter controls the length of history used for prediction?</li><li>Show that one-hot encoding is equivalent to picking a different embedding for each object.</li><li>Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of time steps in a minibatch, and learning rate) to improve the perplexity. How low can you go while sticking with this simple architecture?</li><li>Replace one-hot encoding with learnable embeddings. Does this lead to better performance?</li><li>Conduct an experiment to determine how well this language model  trained on <em>The Time Machine</em> works on other books by H. G. Wells, e.g., <em>The War of the Worlds</em>.</li><li>Conduct another experiment to evaluate the perplexity of this model on books written by other authors. </li><li>Modify the prediction method so as to use sampling  rather than picking the most likely next character.<ul><li>What happens?</li><li>Bias the model towards more likely outputs, e.g., </li></ul>by sampling from <span>$q(x_t \mid x_{t-1}, \ldots, x_1) \propto P(x_t \mid x_{t-1}, \ldots, x_1)^\alpha$</span> for <span>$\alpha &gt; 1$</span>.</li><li>Run the code in this section without clipping the gradient. What happens?</li><li>Replace the activation function used in this section with ReLU  and repeat the experiments in this section. Do we still need gradient clipping? Why?</li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../RNN_4/">« Recurrent Neural Networks</a><a class="docs-footer-nextpage" href="../RNN_6/">Concise Implementation of Recurrent Neural Networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
