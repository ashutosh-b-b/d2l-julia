<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear Regression · d2l Julia</title><meta name="title" content="Linear Regression · d2l Julia"/><meta property="og:title" content="Linear Regression · d2l Julia"/><meta property="twitter:title" content="Linear Regression · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li class="is-active"><a class="tocitem" href>Linear Regression</a><ul class="internal"><li><a class="tocitem" href="#Basics"><span>Basics</span></a></li><li><a class="tocitem" href="#Vectorization-for-Speed"><span>Vectorization for Speed</span></a></li><li><a class="tocitem" href="#The-Normal-Distribution-and-Squared-Loss"><span>The Normal Distribution and Squared Loss</span></a></li><li><a class="tocitem" href="#Linear-Regression-as-a-Neural-Network"><span>Linear Regression as a Neural Network</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../LNN_6/">Generalization</a></li><li><a class="tocitem" href="../LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Linear Neural Networks for Regression</a></li><li class="is-active"><a href>Linear Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear Regression</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-Regression"><a class="docs-heading-anchor" href="#Linear-Regression">Linear Regression</a><a id="Linear-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Regression" title="Permalink"></a></h1><p>:label:<code>sec_linear_regression</code></p><p><em>Regression</em> problems pop up whenever we want to predict a numerical value. Common examples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for patients in the hospital), forecasting demand (for retail sales), among numerous others. Not every prediction problem is one of classical regression. Later on, we will introduce classification problems, where the goal is to predict membership among a set of categories.</p><p>As a running example, suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To develop a model for predicting house prices, we need to get our hands on data, including the sales price, area, and age for each home. In the terminology of machine learning, the dataset is called a <em>training dataset</em> or <em>training set</em>, and each row (containing the data corresponding to one sale) is called an <em>example</em> (or <em>data point</em>, <em>instance</em>, <em>sample</em>). The thing we are trying to predict (price) is called a <em>label</em> (or <em>target</em>). The variables (age and area) upon which the predictions are based are called <em>features</em> (or <em>covariates</em>).</p><pre><code class="language-julia hljs">using Pkg;
Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Plots</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/workspace/d2l-julia/d2lai`</code></pre><h2 id="Basics"><a class="docs-heading-anchor" href="#Basics">Basics</a><a id="Basics-1"></a><a class="docs-heading-anchor-permalink" href="#Basics" title="Permalink"></a></h2><p><em>Linear regression</em> is both the simplest and most popular among the standard tools for tackling regression problems. Dating back to the dawn of the 19th century :cite:<code>Legendre.1805,Gauss.1809</code>, linear regression flows from a few simple assumptions. First, we assume that the relationship between features <span>$\mathbf{x}$</span> and target <span>$y$</span> is approximately linear, i.e., that the conditional mean <span>$E[Y \mid X=\mathbf{x}]$</span> can be expressed as a weighted sum of the features <span>$\mathbf{x}$</span>. This setup allows that the target value may still deviate from its expected value on account of observation noise. Next, we can impose the assumption that any such noise is well behaved, following a Gaussian distribution. Typically, we will use <span>$n$</span> to denote the number of examples in our dataset. We use superscripts to enumerate samples and targets, and subscripts to index coordinates. More concretely, <span>$\mathbf{x}^{(i)}$</span> denotes the <span>$i^{\textrm{th}}$</span> sample and <span>$x_j^{(i)}$</span> denotes its <span>$j^{\textrm{th}}$</span> coordinate.</p><h3 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h3><p>:label:<code>subsec_linear_model</code></p><p>At the heart of every solution is a model that describes how features can be transformed into an estimate of the target. The assumption of linearity means that the expected value of the target (price) can be expressed as a weighted sum of the features (area and age):</p><p class="math-container">\[\textrm{price} = w_{\textrm{area}} \cdot \textrm{area} + w_{\textrm{age}} \cdot \textrm{age} + b.\]</p><p>:eqlabel:<code>eq_price-area</code></p><p>Here <span>$w_{\textrm{area}}$</span> and <span>$w_{\textrm{age}}$</span> are called <em>weights</em>, and <span>$b$</span> is called a <em>bias</em> (or <em>offset</em> or <em>intercept</em>). The weights determine the influence of each feature on our prediction. The bias determines the value of the estimate when all features are zero. Even though we will never see any newly-built homes with precisely zero area, we still need the bias because it allows us to express all linear functions of our features (rather than restricting us to lines that pass through the origin). Strictly speaking, :eqref:<code>eq_price-area</code> is an <em>affine transformation</em> of input features, which is characterized by a <em>linear transformation</em> of features via a weighted sum, combined with a <em>translation</em> via the added bias. Given a dataset, our goal is to choose the weights <span>$\mathbf{w}$</span> and the bias <span>$b$</span> that, on average, make our model&#39;s predictions fit the true prices observed in the data as closely as possible.</p><p>In disciplines where it is common to focus on datasets with just a few features, explicitly expressing models long-form, as in :eqref:<code>eq_price-area</code>, is common. In machine learning, we usually work with high-dimensional datasets, where it is more convenient to employ compact linear algebra notation. When our inputs consist of <span>$d$</span> features, we can assign each an index (between <span>$1$</span> and <span>$d$</span>) and express our prediction <span>$\hat{y}$</span> (in general the &quot;hat&quot; symbol denotes an estimate) as</p><p class="math-container">\[\hat{y} = w_1  x_1 + \cdots + w_d  x_d + b.\]</p><p>Collecting all features into a vector <span>$\mathbf{x} \in \mathbb{R}^d$</span> and all weights into a vector <span>$\mathbf{w} \in \mathbb{R}^d$</span>, we can express our model compactly via the dot product between <span>$\mathbf{w}$</span> and <span>$\mathbf{x}$</span>:</p><p class="math-container">\[\hat{y} = \mathbf{w}^\top \mathbf{x} + b.\]</p><p>:eqlabel:<code>eq_linreg-y</code></p><p>In :eqref:<code>eq_linreg-y</code>, the vector <span>$\mathbf{x}$</span> corresponds to the features of a single example. We will often find it convenient to refer to features of our entire dataset of <span>$n$</span> examples via the <em>design matrix</em> <span>$\mathbf{X} \in \mathbb{R}^{n \times d}$</span>. Here, <span>$\mathbf{X}$</span> contains one row for every example and one column for every feature. For a collection of features <span>$\mathbf{X}$</span>, the predictions <span>$\hat{\mathbf{y}} \in \mathbb{R}^n$</span> can be expressed via the matrix–vector product:</p><p class="math-container">\[{\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,\]</p><p>:eqlabel:<code>eq_linreg-y-vec</code></p><p>where broadcasting (:numref:<code>subsec_broadcasting</code>) is applied during the summation. Given features of a training dataset <span>$\mathbf{X}$</span> and corresponding (known) labels <span>$\mathbf{y}$</span>, the goal of linear regression is to find the weight vector <span>$\mathbf{w}$</span> and the bias term <span>$b$</span> such that, given features of a new data example sampled from the same distribution as <span>$\mathbf{X}$</span>, the new example&#39;s label will (in expectation) be predicted with the smallest error.</p><p>Even if we believe that the best model for predicting <span>$y$</span> given <span>$\mathbf{x}$</span> is linear, we would not expect to find a real-world dataset of <span>$n$</span> examples where <span>$y^{(i)}$</span> exactly equals <span>$\mathbf{w}^\top \mathbf{x}^{(i)}+b$</span> for all <span>$1 \leq i \leq n$</span>. For example, whatever instruments we use to observe the features <span>$\mathbf{X}$</span> and labels <span>$\mathbf{y}$</span>, there might be a small amount of measurement error. Thus, even when we are confident that the underlying relationship is linear, we will incorporate a noise term to account for such errors.</p><p>Before we can go about searching for the best <em>parameters</em> (or <em>model parameters</em>) <span>$\mathbf{w}$</span> and <span>$b$</span>, we will need two more things: (i) a measure of the quality of some given model; and (ii) a procedure for updating the model to improve its quality.</p><h3 id="Loss-Function"><a class="docs-heading-anchor" href="#Loss-Function">Loss Function</a><a id="Loss-Function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-Function" title="Permalink"></a></h3><p>:label:<code>subsec_linear-regression-loss-function</code></p><p>Naturally, fitting our model to the data requires that we agree on some measure of <em>fitness</em> (or, equivalently, of <em>unfitness</em>). <em>Loss functions</em> quantify the distance between the <em>real</em> and <em>predicted</em> values of the target. The loss will usually be a nonnegative number where smaller values are better and perfect predictions incur a loss of 0. For regression problems, the most common loss function is the squared error. When our prediction for an example <span>$i$</span> is <span>$\hat{y}^{(i)}$</span> and the corresponding true label is <span>$y^{(i)}$</span>, the <em>squared error</em> is given by:</p><p class="math-container">\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.\]</p><p>:eqlabel:<code>eq_mse</code></p><p>The constant <span>$\frac{1}{2}$</span> makes no real difference but proves to be notationally convenient, since it cancels out when we take the derivative of the loss. Because the training dataset is given to us, and thus is out of our control, the empirical error is only a function of the model parameters. In :numref:<code>fig_fit_linreg</code>, we visualize the fit of a linear regression model in a problem with one-dimensional inputs.</p><p><img src="../../img/fit-linreg.svg" alt="Fitting a linear regression model to one-dimensional data."/> :label:<code>fig_fit_linreg</code></p><p>Note that large differences between estimates <span>$\hat{y}^{(i)}$</span> and targets <span>$y^{(i)}$</span> lead to even larger contributions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword; while it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data). To measure the quality of a model on the entire dataset of <span>$n$</span> examples, we simply average (or equivalently, sum) the losses on the training set:</p><p class="math-container">\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]</p><p>When training the model, we seek parameters (<span>$\mathbf{w}^*, b^*$</span>) that minimize the total loss across all training examples:</p><p class="math-container">\[\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).\]</p><h3 id="Analytic-Solution"><a class="docs-heading-anchor" href="#Analytic-Solution">Analytic Solution</a><a id="Analytic-Solution-1"></a><a class="docs-heading-anchor-permalink" href="#Analytic-Solution" title="Permalink"></a></h3><p>Unlike most of the models that we will cover, linear regression presents us with a surprisingly easy optimization problem. In particular, we can find the optimal parameters (as assessed on the training data) analytically by applying a simple formula as follows. First, we can subsume the bias <span>$b$</span> into the parameter <span>$\mathbf{w}$</span> by appending a column to the design matrix consisting of all 1s. Then our prediction problem is to minimize <span>$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$</span>. As long as the design matrix <span>$\mathbf{X}$</span> has full rank (no feature is linearly dependent on the others), then there will be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain. Taking the derivative of the loss with respect to <span>$\mathbf{w}$</span> and setting it equal to zero yields:</p><p class="math-container">\[\begin{aligned}
    \partial_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =
    2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) = 0
    \textrm{ and hence }
    \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}.
\end{aligned}\]</p><p>Solving for <span>$\mathbf{w}$</span> provides us with the optimal solution for the optimization problem. Note that this solution </p><p class="math-container">\[\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}\]</p><p>will only be unique when the matrix <span>$\mathbf X^\top \mathbf X$</span> is invertible, i.e., when the columns of the design matrix are linearly independent :cite:<code>Golub.Van-Loan.1996</code>.</p><p>While simple problems like linear regression may admit analytic solutions, you should not get used to such good fortune. Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution is so restrictive that it would exclude almost all exciting aspects of deep learning.</p><h3 id="Minibatch-Stochastic-Gradient-Descent"><a class="docs-heading-anchor" href="#Minibatch-Stochastic-Gradient-Descent">Minibatch Stochastic Gradient Descent</a><a id="Minibatch-Stochastic-Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Minibatch-Stochastic-Gradient-Descent" title="Permalink"></a></h3><p>Fortunately, even in cases where we cannot solve the models analytically, we can still often train models effectively in practice. Moreover, for many tasks, those hard-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.</p><p>The key technique for optimizing nearly every deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called <em>gradient descent</em>.</p><p>The most naive application of gradient descent consists of taking the derivative of the loss function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update, even if the update steps might be very powerful :cite:<code>Liu.Nocedal.1989</code>. Even worse, if there is a lot of redundancy in the training data, the benefit of a full update is limited.</p><p>The other extreme is to consider only a single example at a time and to take update steps based on one observation at a time. The resulting algorithm, <em>stochastic gradient descent</em> (SGD) can be an effective strategy :cite:<code>Bottou.2010</code>, even for large datasets. Unfortunately, SGD has drawbacks, both computational and statistical. One problem arises from the fact that processors are a lot faster multiplying and adding numbers than they are at moving data from main memory to processor cache. It is up to an order of magnitude more efficient to perform a matrix–vector multiplication than a corresponding number of vector–vector operations. This means that it can take a lot longer to process one sample at a time compared to a full batch. A second problem is that some of the layers, such as batch normalization (to be described in :numref:<code>sec_batch_norm</code>), only work well when we have access to more than one observation at a time.</p><p>The solution to both problems is to pick an intermediate strategy: rather than taking a full batch or only a single sample at a time, we take a <em>minibatch</em> of observations :cite:<code>Li.Zhang.Chen.ea.2014</code>. The specific choice of the size of the said minibatch depends on many factors, such as the amount of memory, the number of accelerators, the choice of layers, and the total dataset size. Despite all that, a number between 32 and 256, preferably a multiple of a large power of <span>$2$</span>, is a good start. This leads us to <em>minibatch stochastic gradient descent</em>.</p><p>In its most basic form, in each iteration <span>$t$</span>, we first randomly sample a minibatch <span>$\mathcal{B}_t$</span> consisting of a fixed number <span>$|\mathcal{B}|$</span> of training examples. We then compute the derivative (gradient) of the average loss on the minibatch with respect to the model parameters. Finally, we multiply the gradient by a predetermined small positive value <span>$\eta$</span>, called the <em>learning rate</em>, and subtract the resulting term from the current parameter values. We can express the update as follows:</p><p class="math-container">\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]</p><p>In summary, minibatch SGD proceeds as follows: (i) initialize the values of the model parameters, typically at random; (ii) iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient. For quadratic losses and affine transformations, this has a closed-form expansion:</p><p class="math-container">\[\begin{aligned} \mathbf{w} &amp; \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) &amp;&amp; = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)\\ b &amp;\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_b l^{(i)}(\mathbf{w}, b) &amp;&amp;  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\]</p><p>:eqlabel:<code>eq_linreg_batch_update</code></p><p>Since we pick a minibatch <span>$\mathcal{B}$</span> we need to normalize by its size <span>$|\mathcal{B}|$</span>. Frequently minibatch size and learning rate are user-defined. Such tunable parameters that are not updated in the training loop are called <em>hyperparameters</em>. They can be tuned automatically by a number of techniques, such as Bayesian optimization :cite:<code>Frazier.2018</code>. In the end, the quality of the solution is typically assessed on a separate <em>validation dataset</em> (or <em>validation set</em>).</p><p>After training for some predetermined number of iterations (or until some other stopping criterion is met), we record the estimated model parameters, denoted <span>$\hat{\mathbf{w}}, \hat{b}$</span>. Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss, nor even deterministic. Although the algorithm converges slowly towards the minimizers it typically will not find them exactly in a finite number of steps. Moreover, the minibatches <span>$\mathcal{B}$</span> used for updating the parameters are chosen at random. This breaks determinism.</p><p>Linear regression happens to be a learning problem with a global minimum (whenever <span>$\mathbf{X}$</span> is full rank, or equivalently, whenever <span>$\mathbf{X}^\top \mathbf{X}$</span> is invertible). However, the loss surfaces for deep networks contain many saddle points and minima. Fortunately, we typically do not care about finding an exact set of parameters but merely any set of parameters that leads to accurate predictions (and thus low loss). In practice, deep learning practitioners seldom struggle to find parameters that minimize the loss <em>on training sets</em> :cite:<code>Izmailov.Podoprikhin.Garipov.ea.2018,Frankle.Carbin.2018</code>. The more formidable task is to find parameters that lead to accurate predictions on previously unseen data, a challenge called <em>generalization</em>. We return to these topics throughout the book.</p><h3 id="Predictions"><a class="docs-heading-anchor" href="#Predictions">Predictions</a><a id="Predictions-1"></a><a class="docs-heading-anchor-permalink" href="#Predictions" title="Permalink"></a></h3><p>Given the model <span>$\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}$</span>, we can now make <em>predictions</em> for a new example, e.g., predicting the sales price of a previously unseen house given its area <span>$x_1$</span> and age <span>$x_2$</span>. Deep learning practitioners have taken to calling the prediction phase <em>inference</em> but this is a bit of a misnomer–-<em>inference</em> refers broadly to any conclusion reached on the basis of evidence, including both the values of the parameters and the likely label for an unseen instance. If anything, in the statistics literature <em>inference</em> more often denotes parameter inference and this overloading of terminology creates unnecessary confusion when deep learning practitioners talk to statisticians. In the following we will stick to <em>prediction</em> whenever possible.</p><h2 id="Vectorization-for-Speed"><a class="docs-heading-anchor" href="#Vectorization-for-Speed">Vectorization for Speed</a><a id="Vectorization-for-Speed-1"></a><a class="docs-heading-anchor-permalink" href="#Vectorization-for-Speed" title="Permalink"></a></h2><p>When training our models, we typically want to process whole minibatches of examples simultaneously. Doing this efficiently requires that (<strong>we</strong>) (~~should~~) (<strong>vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python.</strong>)</p><p>To see why this matters so much, let&#39;s (<strong>consider two methods for adding vectors.</strong>) To start, we instantiate two 10,000-dimensional vectors containing all 1s. In the first method, we loop over the vectors with a Python for-loop. In the second, we rely on a single call to <code>+</code>.</p><pre><code class="language-julia hljs">n = 100000
a = ones(n)
b = ones(n);</code></pre><p>Now we can benchmark the workloads. First, we add them, one coordinate at a time, using a for-loop.</p><pre><code class="language-julia hljs">c = zeros(n)
@time begin
    for i in 1:n
        c[i] = a[i] + b[i]
    end
end</code></pre><pre><code class="nohighlight hljs">  0.026434 seconds (598.98 k allocations: 10.666 MiB, 39.13% gc time)</code></pre><p>Alternatively, we rely on the reloaded <code>+</code> operator to compute the elementwise sum.</p><pre><code class="language-julia hljs">@time c .= a .+ b;</code></pre><pre><code class="nohighlight hljs">  0.000103 seconds (1 allocation: 32 bytes)</code></pre><p>The second method is dramatically faster than the first. Vectorizing code often yields order-of-magnitude speedups. Moreover, we push more of the mathematics to the library so we do not have to write as many calculations ourselves, reducing the potential for errors and increasing portability of the code.</p><h2 id="The-Normal-Distribution-and-Squared-Loss"><a class="docs-heading-anchor" href="#The-Normal-Distribution-and-Squared-Loss">The Normal Distribution and Squared Loss</a><a id="The-Normal-Distribution-and-Squared-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#The-Normal-Distribution-and-Squared-Loss" title="Permalink"></a></h2><p>:label:<code>subsec_normal_distribution_and_squared_loss</code></p><p>So far we have given a fairly functional motivation of the squared loss objective: the optimal parameters return the conditional expectation <span>$E[Y\mid X]$</span> whenever the underlying pattern is truly linear, and the loss assigns large penalties for outliers. We can also provide a more formal motivation for the squared loss objective by making probabilistic assumptions about the distribution of noise.</p><p>Linear regression was invented at the turn of the 19th century. While it has long been debated whether Gauss or Legendre first thought up the idea, it was Gauss who also discovered the normal distribution (also called the <em>Gaussian</em>). It turns out that the normal distribution and linear regression with squared loss share a deeper connection than common parentage.</p><p>To begin, recall that a normal distribution with mean <span>$\mu$</span> and variance <span>$\sigma^2$</span> (standard deviation <span>$\sigma$</span>) is given as</p><p class="math-container">\[p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).\]</p><p>Below we define a function to compute the normal distribution.</p><pre><code class="language-julia hljs">function normal(x, mu, sigma)
    p = 1 / sqrt(2*pi*sigma^2)
    exp(-1*((1 / (2*sigma^2))*(x - mu)^2))*p
end</code></pre><pre><code class="nohighlight hljs">normal (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">x = -7:0.01:7
params = [(0, 1), (0, 2), (3, 1)]
plt = plot()
map(params) do (mu, sigma)
    plot!(x, normal.(x, Ref(mu), Ref(sigma)), label = &quot;mean $mu sigma $sigma&quot;)
end
plt</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip680">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip680)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip681">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip680)" d="M156.598 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.598 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip682">
    <rect x="156" y="47" width="2197" height="1440"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="514.732,1486.45 514.732,47.2441 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="884.704,1486.45 884.704,47.2441 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1254.68,1486.45 1254.68,47.2441 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1624.65,1486.45 1624.65,47.2441 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1994.62,1486.45 1994.62,47.2441 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,1445.72 2352.76,1445.72 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,1105.38 2352.76,1105.38 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,765.046 2352.76,765.046 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,424.711 2352.76,424.711 "/>
<polyline clip-path="url(#clip682)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,84.3765 2352.76,84.3765 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1486.45 2352.76,1486.45 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="514.732,1486.45 514.732,1467.55 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="884.704,1486.45 884.704,1467.55 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1254.68,1486.45 1254.68,1467.55 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1624.65,1486.45 1624.65,1467.55 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1994.62,1486.45 1994.62,1467.55 "/>
<path clip-path="url(#clip680)" d="M461.26 1532.02 L490.936 1532.02 L490.936 1535.95 L461.26 1535.95 L461.26 1532.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M501.074 1514.29 L519.431 1514.29 L519.431 1518.22 L505.357 1518.22 L505.357 1526.7 Q506.375 1526.35 507.394 1526.19 Q508.412 1526 509.431 1526 Q515.218 1526 518.597 1529.17 Q521.977 1532.34 521.977 1537.76 Q521.977 1543.34 518.505 1546.44 Q515.033 1549.52 508.713 1549.52 Q506.537 1549.52 504.269 1549.15 Q502.023 1548.78 499.616 1548.04 L499.616 1543.34 Q501.699 1544.47 503.922 1545.03 Q506.144 1545.58 508.621 1545.58 Q512.625 1545.58 514.963 1543.48 Q517.301 1541.37 517.301 1537.76 Q517.301 1534.15 514.963 1532.04 Q512.625 1529.94 508.621 1529.94 Q506.746 1529.94 504.871 1530.35 Q503.019 1530.77 501.074 1531.65 L501.074 1514.29 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M531.19 1542.97 L536.074 1542.97 L536.074 1548.85 L531.19 1548.85 L531.19 1542.97 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M556.259 1517.37 Q552.648 1517.37 550.82 1520.93 Q549.014 1524.47 549.014 1531.6 Q549.014 1538.71 550.82 1542.27 Q552.648 1545.82 556.259 1545.82 Q559.894 1545.82 561.699 1542.27 Q563.528 1538.71 563.528 1531.6 Q563.528 1524.47 561.699 1520.93 Q559.894 1517.37 556.259 1517.37 M556.259 1513.66 Q562.069 1513.66 565.125 1518.27 Q568.204 1522.85 568.204 1531.6 Q568.204 1540.33 565.125 1544.94 Q562.069 1549.52 556.259 1549.52 Q550.449 1549.52 547.37 1544.94 Q544.315 1540.33 544.315 1531.6 Q544.315 1522.85 547.37 1518.27 Q550.449 1513.66 556.259 1513.66 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M831.73 1532.02 L861.406 1532.02 L861.406 1535.95 L831.73 1535.95 L831.73 1532.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M875.526 1544.91 L891.846 1544.91 L891.846 1548.85 L869.901 1548.85 L869.901 1544.91 Q872.563 1542.16 877.147 1537.53 Q881.753 1532.88 882.934 1531.53 Q885.179 1529.01 886.059 1527.27 Q886.961 1525.51 886.961 1523.82 Q886.961 1521.07 885.017 1519.33 Q883.096 1517.6 879.994 1517.6 Q877.795 1517.6 875.341 1518.36 Q872.91 1519.13 870.133 1520.68 L870.133 1515.95 Q872.957 1514.82 875.41 1514.24 Q877.864 1513.66 879.901 1513.66 Q885.272 1513.66 888.466 1516.35 Q891.66 1519.03 891.66 1523.52 Q891.66 1525.65 890.85 1527.57 Q890.063 1529.47 887.957 1532.07 Q887.378 1532.74 884.276 1535.95 Q881.174 1539.15 875.526 1544.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M901.66 1542.97 L906.545 1542.97 L906.545 1548.85 L901.66 1548.85 L901.66 1542.97 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M916.776 1514.29 L935.132 1514.29 L935.132 1518.22 L921.058 1518.22 L921.058 1526.7 Q922.077 1526.35 923.095 1526.19 Q924.114 1526 925.132 1526 Q930.919 1526 934.299 1529.17 Q937.679 1532.34 937.679 1537.76 Q937.679 1543.34 934.206 1546.44 Q930.734 1549.52 924.415 1549.52 Q922.239 1549.52 919.97 1549.15 Q917.725 1548.78 915.318 1548.04 L915.318 1543.34 Q917.401 1544.47 919.623 1545.03 Q921.845 1545.58 924.322 1545.58 Q928.327 1545.58 930.665 1543.48 Q933.003 1541.37 933.003 1537.76 Q933.003 1534.15 930.665 1532.04 Q928.327 1529.94 924.322 1529.94 Q922.447 1529.94 920.572 1530.35 Q918.72 1530.77 916.776 1531.65 L916.776 1514.29 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1232.06 1517.37 Q1228.45 1517.37 1226.62 1520.93 Q1224.82 1524.47 1224.82 1531.6 Q1224.82 1538.71 1226.62 1542.27 Q1228.45 1545.82 1232.06 1545.82 Q1235.7 1545.82 1237.5 1542.27 Q1239.33 1538.71 1239.33 1531.6 Q1239.33 1524.47 1237.5 1520.93 Q1235.7 1517.37 1232.06 1517.37 M1232.06 1513.66 Q1237.87 1513.66 1240.93 1518.27 Q1244.01 1522.85 1244.01 1531.6 Q1244.01 1540.33 1240.93 1544.94 Q1237.87 1549.52 1232.06 1549.52 Q1226.25 1549.52 1223.17 1544.94 Q1220.12 1540.33 1220.12 1531.6 Q1220.12 1522.85 1223.17 1518.27 Q1226.25 1513.66 1232.06 1513.66 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1252.22 1542.97 L1257.11 1542.97 L1257.11 1548.85 L1252.22 1548.85 L1252.22 1542.97 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1277.29 1517.37 Q1273.68 1517.37 1271.85 1520.93 Q1270.05 1524.47 1270.05 1531.6 Q1270.05 1538.71 1271.85 1542.27 Q1273.68 1545.82 1277.29 1545.82 Q1280.93 1545.82 1282.73 1542.27 Q1284.56 1538.71 1284.56 1531.6 Q1284.56 1524.47 1282.73 1520.93 Q1280.93 1517.37 1277.29 1517.37 M1277.29 1513.66 Q1283.1 1513.66 1286.16 1518.27 Q1289.24 1522.85 1289.24 1531.6 Q1289.24 1540.33 1286.16 1544.94 Q1283.1 1549.52 1277.29 1549.52 Q1271.48 1549.52 1268.4 1544.94 Q1265.35 1540.33 1265.35 1531.6 Q1265.35 1522.85 1268.4 1518.27 Q1271.48 1513.66 1277.29 1513.66 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1596.39 1544.91 L1612.71 1544.91 L1612.71 1548.85 L1590.76 1548.85 L1590.76 1544.91 Q1593.42 1542.16 1598.01 1537.53 Q1602.61 1532.88 1603.79 1531.53 Q1606.04 1529.01 1606.92 1527.27 Q1607.82 1525.51 1607.82 1523.82 Q1607.82 1521.07 1605.88 1519.33 Q1603.96 1517.6 1600.85 1517.6 Q1598.65 1517.6 1596.2 1518.36 Q1593.77 1519.13 1590.99 1520.68 L1590.99 1515.95 Q1593.82 1514.82 1596.27 1514.24 Q1598.72 1513.66 1600.76 1513.66 Q1606.13 1513.66 1609.33 1516.35 Q1612.52 1519.03 1612.52 1523.52 Q1612.52 1525.65 1611.71 1527.57 Q1610.92 1529.47 1608.82 1532.07 Q1608.24 1532.74 1605.14 1535.95 Q1602.03 1539.15 1596.39 1544.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1622.52 1542.97 L1627.4 1542.97 L1627.4 1548.85 L1622.52 1548.85 L1622.52 1542.97 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1637.64 1514.29 L1655.99 1514.29 L1655.99 1518.22 L1641.92 1518.22 L1641.92 1526.7 Q1642.94 1526.35 1643.96 1526.19 Q1644.97 1526 1645.99 1526 Q1651.78 1526 1655.16 1529.17 Q1658.54 1532.34 1658.54 1537.76 Q1658.54 1543.34 1655.07 1546.44 Q1651.59 1549.52 1645.27 1549.52 Q1643.1 1549.52 1640.83 1549.15 Q1638.58 1548.78 1636.18 1548.04 L1636.18 1543.34 Q1638.26 1544.47 1640.48 1545.03 Q1642.71 1545.58 1645.18 1545.58 Q1649.19 1545.58 1651.52 1543.48 Q1653.86 1541.37 1653.86 1537.76 Q1653.86 1534.15 1651.52 1532.04 Q1649.19 1529.94 1645.18 1529.94 Q1643.31 1529.94 1641.43 1530.35 Q1639.58 1530.77 1637.64 1531.65 L1637.64 1514.29 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1961.79 1514.29 L1980.14 1514.29 L1980.14 1518.22 L1966.07 1518.22 L1966.07 1526.7 Q1967.09 1526.35 1968.11 1526.19 Q1969.12 1526 1970.14 1526 Q1975.93 1526 1979.31 1529.17 Q1982.69 1532.34 1982.69 1537.76 Q1982.69 1543.34 1979.22 1546.44 Q1975.75 1549.52 1969.43 1549.52 Q1967.25 1549.52 1964.98 1549.15 Q1962.74 1548.78 1960.33 1548.04 L1960.33 1543.34 Q1962.41 1544.47 1964.63 1545.03 Q1966.86 1545.58 1969.33 1545.58 Q1973.34 1545.58 1975.68 1543.48 Q1978.01 1541.37 1978.01 1537.76 Q1978.01 1534.15 1975.68 1532.04 Q1973.34 1529.94 1969.33 1529.94 Q1967.46 1529.94 1965.58 1530.35 Q1963.73 1530.77 1961.79 1531.65 L1961.79 1514.29 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M1991.9 1542.97 L1996.79 1542.97 L1996.79 1548.85 L1991.9 1548.85 L1991.9 1542.97 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M2016.97 1517.37 Q2013.36 1517.37 2011.53 1520.93 Q2009.73 1524.47 2009.73 1531.6 Q2009.73 1538.71 2011.53 1542.27 Q2013.36 1545.82 2016.97 1545.82 Q2020.61 1545.82 2022.41 1542.27 Q2024.24 1538.71 2024.24 1531.6 Q2024.24 1524.47 2022.41 1520.93 Q2020.61 1517.37 2016.97 1517.37 M2016.97 1513.66 Q2022.78 1513.66 2025.84 1518.27 Q2028.92 1522.85 2028.92 1531.6 Q2028.92 1540.33 2025.84 1544.94 Q2022.78 1549.52 2016.97 1549.52 Q2011.16 1549.52 2008.08 1544.94 Q2005.03 1540.33 2005.03 1531.6 Q2005.03 1522.85 2008.08 1518.27 Q2011.16 1513.66 2016.97 1513.66 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1486.45 156.598,47.2441 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1445.72 175.496,1445.72 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1105.38 175.496,1105.38 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,765.046 175.496,765.046 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,424.711 175.496,424.711 "/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,84.3765 175.496,84.3765 "/>
<path clip-path="url(#clip680)" d="M63.4226 1431.51 Q59.8115 1431.51 57.9828 1435.08 Q56.1773 1438.62 56.1773 1445.75 Q56.1773 1452.86 57.9828 1456.42 Q59.8115 1459.96 63.4226 1459.96 Q67.0569 1459.96 68.8624 1456.42 Q70.6911 1452.86 70.6911 1445.75 Q70.6911 1438.62 68.8624 1435.08 Q67.0569 1431.51 63.4226 1431.51 M63.4226 1427.81 Q69.2328 1427.81 72.2883 1432.42 Q75.367 1437 75.367 1445.75 Q75.367 1454.48 72.2883 1459.08 Q69.2328 1463.67 63.4226 1463.67 Q57.6125 1463.67 54.5338 1459.08 Q51.4782 1454.48 51.4782 1445.75 Q51.4782 1437 54.5338 1432.42 Q57.6125 1427.81 63.4226 1427.81 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M83.5845 1457.12 L88.4688 1457.12 L88.4688 1463 L83.5845 1463 L83.5845 1457.12 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M108.654 1431.51 Q105.043 1431.51 103.214 1435.08 Q101.409 1438.62 101.409 1445.75 Q101.409 1452.86 103.214 1456.42 Q105.043 1459.96 108.654 1459.96 Q112.288 1459.96 114.094 1456.42 Q115.922 1452.86 115.922 1445.75 Q115.922 1438.62 114.094 1435.08 Q112.288 1431.51 108.654 1431.51 M108.654 1427.81 Q114.464 1427.81 117.52 1432.42 Q120.598 1437 120.598 1445.75 Q120.598 1454.48 117.52 1459.08 Q114.464 1463.67 108.654 1463.67 Q102.844 1463.67 99.765 1459.08 Q96.7095 1454.48 96.7095 1445.75 Q96.7095 1437 99.765 1432.42 Q102.844 1427.81 108.654 1427.81 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M64.6495 1091.18 Q61.0384 1091.18 59.2097 1094.74 Q57.4041 1098.29 57.4041 1105.42 Q57.4041 1112.52 59.2097 1116.09 Q61.0384 1119.63 64.6495 1119.63 Q68.2837 1119.63 70.0892 1116.09 Q71.9179 1112.52 71.9179 1105.42 Q71.9179 1098.29 70.0892 1094.74 Q68.2837 1091.18 64.6495 1091.18 M64.6495 1087.48 Q70.4596 1087.48 73.5152 1092.08 Q76.5938 1096.67 76.5938 1105.42 Q76.5938 1114.14 73.5152 1118.75 Q70.4596 1123.33 64.6495 1123.33 Q58.8393 1123.33 55.7606 1118.75 Q52.7051 1114.14 52.7051 1105.42 Q52.7051 1096.67 55.7606 1092.08 Q58.8393 1087.48 64.6495 1087.48 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M84.8114 1116.78 L89.6956 1116.78 L89.6956 1122.66 L84.8114 1122.66 L84.8114 1116.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M100.691 1118.73 L108.33 1118.73 L108.33 1092.36 L100.02 1094.03 L100.02 1089.77 L108.283 1088.1 L112.959 1088.1 L112.959 1118.73 L120.598 1118.73 L120.598 1122.66 L100.691 1122.66 L100.691 1118.73 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M65.0198 750.845 Q61.4087 750.845 59.58 754.41 Q57.7745 757.951 57.7745 765.081 Q57.7745 772.187 59.58 775.752 Q61.4087 779.294 65.0198 779.294 Q68.6541 779.294 70.4596 775.752 Q72.2883 772.187 72.2883 765.081 Q72.2883 757.951 70.4596 754.41 Q68.6541 750.845 65.0198 750.845 M65.0198 747.141 Q70.83 747.141 73.8855 751.748 Q76.9642 756.331 76.9642 765.081 Q76.9642 773.808 73.8855 778.414 Q70.83 782.997 65.0198 782.997 Q59.2097 782.997 56.131 778.414 Q53.0754 773.808 53.0754 765.081 Q53.0754 756.331 56.131 751.748 Q59.2097 747.141 65.0198 747.141 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M85.1818 776.446 L90.066 776.446 L90.066 782.326 L85.1818 782.326 L85.1818 776.446 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M104.279 778.391 L120.598 778.391 L120.598 782.326 L98.6539 782.326 L98.6539 778.391 Q101.316 775.636 105.899 771.007 Q110.506 766.354 111.686 765.011 Q113.932 762.488 114.811 760.752 Q115.714 758.993 115.714 757.303 Q115.714 754.548 113.77 752.812 Q111.848 751.076 108.746 751.076 Q106.547 751.076 104.094 751.84 Q101.663 752.604 98.8854 754.155 L98.8854 749.433 Q101.709 748.298 104.163 747.72 Q106.617 747.141 108.654 747.141 Q114.024 747.141 117.219 749.826 Q120.413 752.511 120.413 757.002 Q120.413 759.132 119.603 761.053 Q118.816 762.951 116.709 765.544 Q116.131 766.215 113.029 769.433 Q109.927 772.627 104.279 778.391 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M64.0708 410.51 Q60.4597 410.51 58.631 414.075 Q56.8254 417.616 56.8254 424.746 Q56.8254 431.852 58.631 435.417 Q60.4597 438.959 64.0708 438.959 Q67.705 438.959 69.5105 435.417 Q71.3392 431.852 71.3392 424.746 Q71.3392 417.616 69.5105 414.075 Q67.705 410.51 64.0708 410.51 M64.0708 406.806 Q69.8809 406.806 72.9365 411.413 Q76.0151 415.996 76.0151 424.746 Q76.0151 433.473 72.9365 438.079 Q69.8809 442.663 64.0708 442.663 Q58.2606 442.663 55.1819 438.079 Q52.1264 433.473 52.1264 424.746 Q52.1264 415.996 55.1819 411.413 Q58.2606 406.806 64.0708 406.806 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M84.2327 436.112 L89.1169 436.112 L89.1169 441.991 L84.2327 441.991 L84.2327 436.112 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M113.469 423.357 Q116.825 424.075 118.7 426.343 Q120.598 428.612 120.598 431.945 Q120.598 437.061 117.08 439.862 Q113.561 442.663 107.08 442.663 Q104.904 442.663 102.589 442.223 Q100.297 441.806 97.8437 440.95 L97.8437 436.436 Q99.7882 437.57 102.103 438.149 Q104.418 438.727 106.941 438.727 Q111.339 438.727 113.631 436.991 Q115.945 435.255 115.945 431.945 Q115.945 428.89 113.793 427.177 Q111.663 425.44 107.844 425.44 L103.816 425.44 L103.816 421.598 L108.029 421.598 Q111.478 421.598 113.307 420.232 Q115.135 418.843 115.135 416.251 Q115.135 413.589 113.237 412.177 Q111.362 410.741 107.844 410.741 Q105.922 410.741 103.723 411.158 Q101.524 411.575 98.8854 412.454 L98.8854 408.288 Q101.547 407.547 103.862 407.177 Q106.2 406.806 108.26 406.806 Q113.584 406.806 116.686 409.237 Q119.788 411.644 119.788 415.765 Q119.788 418.635 118.145 420.626 Q116.501 422.593 113.469 423.357 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M62.9365 70.1752 Q59.3254 70.1752 57.4967 73.74 Q55.6912 77.2816 55.6912 84.4112 Q55.6912 91.5177 57.4967 95.0825 Q59.3254 98.6241 62.9365 98.6241 Q66.5707 98.6241 68.3763 95.0825 Q70.205 91.5177 70.205 84.4112 Q70.205 77.2816 68.3763 73.74 Q66.5707 70.1752 62.9365 70.1752 M62.9365 66.4715 Q68.7467 66.4715 71.8022 71.0779 Q74.8809 75.6613 74.8809 84.4112 Q74.8809 93.138 71.8022 97.7445 Q68.7467 102.328 62.9365 102.328 Q57.1264 102.328 54.0477 97.7445 Q50.9921 93.138 50.9921 84.4112 Q50.9921 75.6613 54.0477 71.0779 Q57.1264 66.4715 62.9365 66.4715 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M83.0984 95.7769 L87.9827 95.7769 L87.9827 101.656 L83.0984 101.656 L83.0984 95.7769 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M111.015 71.1705 L99.2095 89.6195 L111.015 89.6195 L111.015 71.1705 M109.788 67.0965 L115.668 67.0965 L115.668 89.6195 L120.598 89.6195 L120.598 93.5084 L115.668 93.5084 L115.668 101.656 L111.015 101.656 L111.015 93.5084 L95.4132 93.5084 L95.4132 88.9945 L109.788 67.0965 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip682)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,1445.72 220.234,1445.72 221.713,1445.72 223.193,1445.72 224.673,1445.72 226.153,1445.72 227.633,1445.72 229.113,1445.72 230.593,1445.72 232.073,1445.72 233.553,1445.72 235.032,1445.72 236.512,1445.72 237.992,1445.72 239.472,1445.72 240.952,1445.72 242.432,1445.72 243.912,1445.72 245.392,1445.72 246.872,1445.72 248.351,1445.72 249.831,1445.72 251.311,1445.72 252.791,1445.72 254.271,1445.72 255.751,1445.72 257.231,1445.72 258.711,1445.72 260.191,1445.72 261.67,1445.72 263.15,1445.72 264.63,1445.72 266.11,1445.72 267.59,1445.72 269.07,1445.72 270.55,1445.72 272.03,1445.72 273.51,1445.72 274.989,1445.72 276.469,1445.72 277.949,1445.72 279.429,1445.72 280.909,1445.72 282.389,1445.72 283.869,1445.72 285.349,1445.72 286.829,1445.72 288.308,1445.72 289.788,1445.72 291.268,1445.72 292.748,1445.72 294.228,1445.72 295.708,1445.72 297.188,1445.72 298.668,1445.72 300.148,1445.72 301.628,1445.72 303.107,1445.72 304.587,1445.72 306.067,1445.72 307.547,1445.72 309.027,1445.72 310.507,1445.72 311.987,1445.72 313.467,1445.72 314.947,1445.72 316.426,1445.72 317.906,1445.72 319.386,1445.72 320.866,1445.72 322.346,1445.72 323.826,1445.72 325.306,1445.72 326.786,1445.72 328.266,1445.72 329.745,1445.72 331.225,1445.72 332.705,1445.72 334.185,1445.72 335.665,1445.72 337.145,1445.72 338.625,1445.72 340.105,1445.72 341.585,1445.72 343.064,1445.72 344.544,1445.72 346.024,1445.72 347.504,1445.72 348.984,1445.72 350.464,1445.72 351.944,1445.72 353.424,1445.72 354.904,1445.72 356.383,1445.72 357.863,1445.72 359.343,1445.72 360.823,1445.72 362.303,1445.72 363.783,1445.72 365.263,1445.72 366.743,1445.72 368.223,1445.72 369.702,1445.72 371.182,1445.72 372.662,1445.72 374.142,1445.72 375.622,1445.72 377.102,1445.72 378.582,1445.72 380.062,1445.72 381.542,1445.72 383.021,1445.72 384.501,1445.72 385.981,1445.72 387.461,1445.72 388.941,1445.72 390.421,1445.72 391.901,1445.72 393.381,1445.72 394.861,1445.72 396.341,1445.72 397.82,1445.72 399.3,1445.72 400.78,1445.72 402.26,1445.72 403.74,1445.72 405.22,1445.72 406.7,1445.72 408.18,1445.72 409.66,1445.72 411.139,1445.72 412.619,1445.72 414.099,1445.72 415.579,1445.72 417.059,1445.72 418.539,1445.72 420.019,1445.72 421.499,1445.72 422.979,1445.72 424.458,1445.72 425.938,1445.72 427.418,1445.72 428.898,1445.72 430.378,1445.72 431.858,1445.72 433.338,1445.72 434.818,1445.72 436.298,1445.72 437.777,1445.72 439.257,1445.72 440.737,1445.72 442.217,1445.72 443.697,1445.72 445.177,1445.72 446.657,1445.72 448.137,1445.72 449.617,1445.72 451.096,1445.72 452.576,1445.72 454.056,1445.72 455.536,1445.72 457.016,1445.72 458.496,1445.71 459.976,1445.71 461.456,1445.71 462.936,1445.71 464.415,1445.71 465.895,1445.71 467.375,1445.71 468.855,1445.71 470.335,1445.71 471.815,1445.71 473.295,1445.71 474.775,1445.71 476.255,1445.71 477.734,1445.71 479.214,1445.71 480.694,1445.71 482.174,1445.71 483.654,1445.71 485.134,1445.71 486.614,1445.71 488.094,1445.71 489.574,1445.71 491.054,1445.71 492.533,1445.71 494.013,1445.71 495.493,1445.71 496.973,1445.71 498.453,1445.71 499.933,1445.71 501.413,1445.71 502.893,1445.71 504.373,1445.71 505.852,1445.71 507.332,1445.71 508.812,1445.71 510.292,1445.71 511.772,1445.71 513.252,1445.71 514.732,1445.71 516.212,1445.71 517.692,1445.71 519.171,1445.71 520.651,1445.71 522.131,1445.71 523.611,1445.71 525.091,1445.71 526.571,1445.71 528.051,1445.71 529.531,1445.71 531.011,1445.71 532.49,1445.71 533.97,1445.71 535.45,1445.71 536.93,1445.71 538.41,1445.7 539.89,1445.7 541.37,1445.7 542.85,1445.7 544.33,1445.7 545.809,1445.7 547.289,1445.7 548.769,1445.7 550.249,1445.7 551.729,1445.7 553.209,1445.7 554.689,1445.7 556.169,1445.7 557.649,1445.7 559.128,1445.69 560.608,1445.69 562.088,1445.69 563.568,1445.69 565.048,1445.69 566.528,1445.69 568.008,1445.69 569.488,1445.69 570.968,1445.68 572.447,1445.68 573.927,1445.68 575.407,1445.68 576.887,1445.68 578.367,1445.68 579.847,1445.67 581.327,1445.67 582.807,1445.67 584.287,1445.67 585.767,1445.67 587.246,1445.66 588.726,1445.66 590.206,1445.66 591.686,1445.66 593.166,1445.65 594.646,1445.65 596.126,1445.65 597.606,1445.64 599.086,1445.64 600.565,1445.64 602.045,1445.63 603.525,1445.63 605.005,1445.63 606.485,1445.62 607.965,1445.62 609.445,1445.61 610.925,1445.61 612.405,1445.61 613.884,1445.6 615.364,1445.6 616.844,1445.59 618.324,1445.58 619.804,1445.58 621.284,1445.57 622.764,1445.57 624.244,1445.56 625.724,1445.55 627.203,1445.55 628.683,1445.54 630.163,1445.53 631.643,1445.52 633.123,1445.52 634.603,1445.51 636.083,1445.5 637.563,1445.49 639.043,1445.48 640.522,1445.47 642.002,1445.46 643.482,1445.45 644.962,1445.44 646.442,1445.42 647.922,1445.41 649.402,1445.4 650.882,1445.39 652.362,1445.37 653.841,1445.36 655.321,1445.34 656.801,1445.33 658.281,1445.31 659.761,1445.3 661.241,1445.28 662.721,1445.26 664.201,1445.24 665.681,1445.22 667.16,1445.2 668.64,1445.18 670.12,1445.16 671.6,1445.14 673.08,1445.11 674.56,1445.09 676.04,1445.07 677.52,1445.04 679,1445.01 680.48,1444.98 681.959,1444.96 683.439,1444.93 684.919,1444.9 686.399,1444.86 687.879,1444.83 689.359,1444.79 690.839,1444.76 692.319,1444.72 693.799,1444.68 695.278,1444.64 696.758,1444.6 698.238,1444.56 699.718,1444.52 701.198,1444.47 702.678,1444.42 704.158,1444.37 705.638,1444.32 707.118,1444.27 708.597,1444.22 710.077,1444.16 711.557,1444.1 713.037,1444.04 714.517,1443.98 715.997,1443.91 717.477,1443.85 718.957,1443.78 720.437,1443.71 721.916,1443.63 723.396,1443.56 724.876,1443.48 726.356,1443.4 727.836,1443.31 729.316,1443.23 730.796,1443.14 732.276,1443.04 733.756,1442.95 735.235,1442.85 736.715,1442.75 738.195,1442.64 739.675,1442.53 741.155,1442.42 742.635,1442.3 744.115,1442.18 745.595,1442.06 747.075,1441.93 748.554,1441.8 750.034,1441.66 751.514,1441.52 752.994,1441.38 754.474,1441.23 755.954,1441.07 757.434,1440.91 758.914,1440.75 760.394,1440.58 761.873,1440.41 763.353,1440.23 764.833,1440.04 766.313,1439.85 767.793,1439.66 769.273,1439.45 770.753,1439.25 772.233,1439.03 773.713,1438.81 775.193,1438.58 776.672,1438.35 778.152,1438.11 779.632,1437.86 781.112,1437.6 782.592,1437.34 784.072,1437.07 785.552,1436.79 787.032,1436.5 788.512,1436.21 789.991,1435.9 791.471,1435.59 792.951,1435.27 794.431,1434.94 795.911,1434.6 797.391,1434.25 798.871,1433.89 800.351,1433.52 801.831,1433.14 803.31,1432.75 804.79,1432.35 806.27,1431.94 807.75,1431.51 809.23,1431.08 810.71,1430.63 812.19,1430.17 813.67,1429.7 815.15,1429.22 816.629,1428.72 818.109,1428.21 819.589,1427.69 821.069,1427.15 822.549,1426.6 824.029,1426.04 825.509,1425.46 826.989,1424.86 828.469,1424.25 829.948,1423.63 831.428,1422.98 832.908,1422.33 834.388,1421.65 835.868,1420.96 837.348,1420.25 838.828,1419.52 840.308,1418.78 841.788,1418.01 843.267,1417.23 844.747,1416.43 846.227,1415.61 847.707,1414.77 849.187,1413.91 850.667,1413.02 852.147,1412.12 853.627,1411.2 855.107,1410.25 856.586,1409.28 858.066,1408.29 859.546,1407.27 861.026,1406.24 862.506,1405.17 863.986,1404.09 865.466,1402.98 866.946,1401.84 868.426,1400.68 869.906,1399.49 871.385,1398.27 872.865,1397.03 874.345,1395.76 875.825,1394.46 877.305,1393.14 878.785,1391.78 880.265,1390.4 881.745,1388.98 883.225,1387.54 884.704,1386.06 886.184,1384.55 887.664,1383.01 889.144,1381.44 890.624,1379.84 892.104,1378.2 893.584,1376.53 895.064,1374.83 896.544,1373.09 898.023,1371.31 899.503,1369.5 900.983,1367.65 902.463,1365.77 903.943,1363.85 905.423,1361.89 906.903,1359.89 908.383,1357.85 909.863,1355.78 911.342,1353.66 912.822,1351.51 914.302,1349.31 915.782,1347.07 917.262,1344.79 918.742,1342.47 920.222,1340.1 921.702,1337.69 923.182,1335.24 924.661,1332.75 926.141,1330.2 927.621,1327.62 929.101,1324.98 930.581,1322.3 932.061,1319.58 933.541,1316.8 935.021,1313.98 936.501,1311.11 937.98,1308.19 939.46,1305.23 940.94,1302.21 942.42,1299.14 943.9,1296.02 945.38,1292.86 946.86,1289.63 948.34,1286.36 949.82,1283.04 951.299,1279.66 952.779,1276.23 954.259,1272.74 955.739,1269.21 957.219,1265.61 958.699,1261.97 960.179,1258.26 961.659,1254.5 963.139,1250.69 964.619,1246.82 966.098,1242.89 967.578,1238.91 969.058,1234.87 970.538,1230.77 972.018,1226.62 973.498,1222.4 974.978,1218.13 976.458,1213.8 977.938,1209.41 979.417,1204.96 980.897,1200.45 982.377,1195.89 983.857,1191.26 985.337,1186.57 986.817,1181.83 988.297,1177.02 989.777,1172.15 991.257,1167.23 992.736,1162.24 994.216,1157.19 995.696,1152.08 997.176,1146.92 998.656,1141.69 1000.14,1136.4 1001.62,1131.05 1003.1,1125.63 1004.58,1120.16 1006.06,1114.63 1007.54,1109.04 1009.02,1103.39 1010.5,1097.67 1011.98,1091.9 1013.45,1086.07 1014.93,1080.18 1016.41,1074.22 1017.89,1068.21 1019.37,1062.14 1020.85,1056.02 1022.33,1049.83 1023.81,1043.59 1025.29,1037.28 1026.77,1030.92 1028.25,1024.51 1029.73,1018.04 1031.21,1011.51 1032.69,1004.92 1034.17,998.283 1035.65,991.589 1037.13,984.841 1038.61,978.039 1040.09,971.185 1041.57,964.278 1043.05,957.32 1044.53,950.31 1046.01,943.25 1047.49,936.141 1048.97,928.983 1050.45,921.776 1051.93,914.522 1053.41,907.222 1054.89,899.875 1056.37,892.484 1057.85,885.049 1059.33,877.571 1060.81,870.05 1062.29,862.489 1063.77,854.887 1065.25,847.246 1066.73,839.566 1068.21,831.85 1069.69,824.097 1071.17,816.31 1072.65,808.488 1074.13,800.634 1075.61,792.749 1077.09,784.833 1078.57,776.888 1080.05,768.915 1081.53,760.916 1083.01,752.891 1084.49,744.843 1085.97,736.772 1087.45,728.679 1088.93,720.567 1090.41,712.437 1091.89,704.289 1093.37,696.126 1094.85,687.948 1096.33,679.758 1097.81,671.557 1099.29,663.347 1100.77,655.128 1102.25,646.903 1103.73,638.673 1105.21,630.44 1106.69,622.205 1108.17,613.97 1109.65,605.737 1111.13,597.507 1112.61,589.282 1114.09,581.064 1115.57,572.855 1117.05,564.655 1118.53,556.467 1120.01,548.294 1121.49,540.135 1122.97,531.994 1124.45,523.871 1125.93,515.77 1127.41,507.691 1128.89,499.636 1130.37,491.608 1131.85,483.608 1133.33,475.638 1134.81,467.699 1136.29,459.795 1137.77,451.925 1139.25,444.093 1140.73,436.301 1142.21,428.549 1143.69,420.84 1145.17,413.176 1146.65,405.559 1148.12,397.991 1149.6,390.473 1151.08,383.007 1152.56,375.595 1154.04,368.24 1155.52,360.942 1157,353.705 1158.48,346.528 1159.96,339.416 1161.44,332.368 1162.92,325.388 1164.4,318.477 1165.88,311.636 1167.36,304.869 1168.84,298.175 1170.32,291.558 1171.8,285.018 1173.28,278.558 1174.76,272.18 1176.24,265.885 1177.72,259.674 1179.2,253.551 1180.68,247.515 1182.16,241.569 1183.64,235.715 1185.12,229.954 1186.6,224.287 1188.08,218.717 1189.56,213.245 1191.04,207.872 1192.52,202.6 1194,197.43 1195.48,192.364 1196.96,187.404 1198.44,182.55 1199.92,177.804 1201.4,173.168 1202.88,168.642 1204.36,164.229 1205.84,159.929 1207.32,155.743 1208.8,151.673 1210.28,147.72 1211.76,143.885 1213.24,140.17 1214.72,136.575 1216.2,133.101 1217.68,129.75 1219.16,126.521 1220.64,123.418 1222.12,120.439 1223.6,117.587 1225.08,114.861 1226.56,112.264 1228.04,109.794 1229.52,107.455 1231,105.245 1232.48,103.165 1233.96,101.217 1235.44,99.4008 1236.92,97.7169 1238.4,96.1658 1239.88,94.748 1241.36,93.464 1242.84,92.3141 1244.32,91.2987 1245.8,90.418 1247.28,89.6724 1248.76,89.062 1250.24,88.5871 1251.72,88.2478 1253.2,88.0442 1254.68,87.9763 1256.16,88.0442 1257.64,88.2478 1259.12,88.5871 1260.6,89.062 1262.08,89.6724 1263.56,90.418 1265.04,91.2987 1266.52,92.3141 1268,93.464 1269.48,94.748 1270.96,96.1658 1272.44,97.7169 1273.92,99.4008 1275.4,101.217 1276.88,103.165 1278.36,105.245 1279.84,107.455 1281.32,109.794 1282.79,112.264 1284.27,114.861 1285.75,117.587 1287.23,120.439 1288.71,123.418 1290.19,126.521 1291.67,129.75 1293.15,133.101 1294.63,136.575 1296.11,140.17 1297.59,143.885 1299.07,147.72 1300.55,151.673 1302.03,155.743 1303.51,159.929 1304.99,164.229 1306.47,168.642 1307.95,173.168 1309.43,177.804 1310.91,182.55 1312.39,187.404 1313.87,192.364 1315.35,197.43 1316.83,202.6 1318.31,207.872 1319.79,213.245 1321.27,218.717 1322.75,224.287 1324.23,229.954 1325.71,235.715 1327.19,241.569 1328.67,247.515 1330.15,253.551 1331.63,259.674 1333.11,265.885 1334.59,272.18 1336.07,278.558 1337.55,285.018 1339.03,291.558 1340.51,298.175 1341.99,304.869 1343.47,311.636 1344.95,318.477 1346.43,325.388 1347.91,332.368 1349.39,339.416 1350.87,346.528 1352.35,353.705 1353.83,360.942 1355.31,368.24 1356.79,375.595 1358.27,383.007 1359.75,390.473 1361.23,397.991 1362.71,405.559 1364.19,413.176 1365.67,420.84 1367.15,428.549 1368.63,436.301 1370.11,444.093 1371.59,451.925 1373.07,459.795 1374.55,467.699 1376.03,475.638 1377.51,483.608 1378.99,491.608 1380.47,499.636 1381.95,507.691 1383.43,515.77 1384.91,523.871 1386.39,531.994 1387.87,540.135 1389.35,548.294 1390.83,556.467 1392.31,564.655 1393.79,572.855 1395.27,581.064 1396.75,589.282 1398.23,597.507 1399.71,605.737 1401.19,613.97 1402.67,622.205 1404.15,630.44 1405.63,638.673 1407.11,646.903 1408.59,655.128 1410.07,663.347 1411.55,671.557 1413.03,679.758 1414.51,687.948 1415.99,696.126 1417.47,704.289 1418.94,712.437 1420.42,720.567 1421.9,728.679 1423.38,736.772 1424.86,744.843 1426.34,752.891 1427.82,760.916 1429.3,768.915 1430.78,776.888 1432.26,784.833 1433.74,792.749 1435.22,800.634 1436.7,808.488 1438.18,816.31 1439.66,824.097 1441.14,831.85 1442.62,839.566 1444.1,847.246 1445.58,854.887 1447.06,862.489 1448.54,870.05 1450.02,877.571 1451.5,885.049 1452.98,892.484 1454.46,899.875 1455.94,907.222 1457.42,914.522 1458.9,921.776 1460.38,928.983 1461.86,936.141 1463.34,943.25 1464.82,950.31 1466.3,957.32 1467.78,964.278 1469.26,971.185 1470.74,978.039 1472.22,984.841 1473.7,991.589 1475.18,998.283 1476.66,1004.92 1478.14,1011.51 1479.62,1018.04 1481.1,1024.51 1482.58,1030.92 1484.06,1037.28 1485.54,1043.59 1487.02,1049.83 1488.5,1056.02 1489.98,1062.14 1491.46,1068.21 1492.94,1074.22 1494.42,1080.18 1495.9,1086.07 1497.38,1091.9 1498.86,1097.67 1500.34,1103.39 1501.82,1109.04 1503.3,1114.63 1504.78,1120.16 1506.26,1125.63 1507.74,1131.05 1509.22,1136.4 1510.7,1141.69 1512.18,1146.92 1513.66,1152.08 1515.14,1157.19 1516.62,1162.24 1518.1,1167.23 1519.58,1172.15 1521.06,1177.02 1522.54,1181.83 1524.02,1186.57 1525.5,1191.26 1526.98,1195.89 1528.46,1200.45 1529.94,1204.96 1531.42,1209.41 1532.9,1213.8 1534.38,1218.13 1535.86,1222.4 1537.34,1226.62 1538.82,1230.77 1540.3,1234.87 1541.78,1238.91 1543.26,1242.89 1544.74,1246.82 1546.22,1250.69 1547.7,1254.5 1549.18,1258.26 1550.66,1261.97 1552.14,1265.61 1553.61,1269.21 1555.09,1272.74 1556.57,1276.23 1558.05,1279.66 1559.53,1283.04 1561.01,1286.36 1562.49,1289.63 1563.97,1292.86 1565.45,1296.02 1566.93,1299.14 1568.41,1302.21 1569.89,1305.23 1571.37,1308.19 1572.85,1311.11 1574.33,1313.98 1575.81,1316.8 1577.29,1319.58 1578.77,1322.3 1580.25,1324.98 1581.73,1327.62 1583.21,1330.2 1584.69,1332.75 1586.17,1335.24 1587.65,1337.69 1589.13,1340.1 1590.61,1342.47 1592.09,1344.79 1593.57,1347.07 1595.05,1349.31 1596.53,1351.51 1598.01,1353.66 1599.49,1355.78 1600.97,1357.85 1602.45,1359.89 1603.93,1361.89 1605.41,1363.85 1606.89,1365.77 1608.37,1367.65 1609.85,1369.5 1611.33,1371.31 1612.81,1373.09 1614.29,1374.83 1615.77,1376.53 1617.25,1378.2 1618.73,1379.84 1620.21,1381.44 1621.69,1383.01 1623.17,1384.55 1624.65,1386.06 1626.13,1387.54 1627.61,1388.98 1629.09,1390.4 1630.57,1391.78 1632.05,1393.14 1633.53,1394.46 1635.01,1395.76 1636.49,1397.03 1637.97,1398.27 1639.45,1399.49 1640.93,1400.68 1642.41,1401.84 1643.89,1402.98 1645.37,1404.09 1646.85,1405.17 1648.33,1406.24 1649.81,1407.27 1651.29,1408.29 1652.77,1409.28 1654.25,1410.25 1655.73,1411.2 1657.21,1412.12 1658.69,1413.02 1660.17,1413.91 1661.65,1414.77 1663.13,1415.61 1664.61,1416.43 1666.09,1417.23 1667.57,1418.01 1669.05,1418.78 1670.53,1419.52 1672.01,1420.25 1673.49,1420.96 1674.97,1421.65 1676.45,1422.33 1677.93,1422.98 1679.41,1423.63 1680.89,1424.25 1682.37,1424.86 1683.85,1425.46 1685.33,1426.04 1686.81,1426.6 1688.29,1427.15 1689.76,1427.69 1691.24,1428.21 1692.72,1428.72 1694.2,1429.22 1695.68,1429.7 1697.16,1430.17 1698.64,1430.63 1700.12,1431.08 1701.6,1431.51 1703.08,1431.94 1704.56,1432.35 1706.04,1432.75 1707.52,1433.14 1709,1433.52 1710.48,1433.89 1711.96,1434.25 1713.44,1434.6 1714.92,1434.94 1716.4,1435.27 1717.88,1435.59 1719.36,1435.9 1720.84,1436.21 1722.32,1436.5 1723.8,1436.79 1725.28,1437.07 1726.76,1437.34 1728.24,1437.6 1729.72,1437.86 1731.2,1438.11 1732.68,1438.35 1734.16,1438.58 1735.64,1438.81 1737.12,1439.03 1738.6,1439.25 1740.08,1439.45 1741.56,1439.66 1743.04,1439.85 1744.52,1440.04 1746,1440.23 1747.48,1440.41 1748.96,1440.58 1750.44,1440.75 1751.92,1440.91 1753.4,1441.07 1754.88,1441.23 1756.36,1441.38 1757.84,1441.52 1759.32,1441.66 1760.8,1441.8 1762.28,1441.93 1763.76,1442.06 1765.24,1442.18 1766.72,1442.3 1768.2,1442.42 1769.68,1442.53 1771.16,1442.64 1772.64,1442.75 1774.12,1442.85 1775.6,1442.95 1777.08,1443.04 1778.56,1443.14 1780.04,1443.23 1781.52,1443.31 1783,1443.4 1784.48,1443.48 1785.96,1443.56 1787.44,1443.63 1788.92,1443.71 1790.4,1443.78 1791.88,1443.85 1793.36,1443.91 1794.84,1443.98 1796.32,1444.04 1797.8,1444.1 1799.28,1444.16 1800.76,1444.22 1802.24,1444.27 1803.72,1444.32 1805.2,1444.37 1806.68,1444.42 1808.16,1444.47 1809.64,1444.52 1811.12,1444.56 1812.6,1444.6 1814.08,1444.64 1815.56,1444.68 1817.04,1444.72 1818.52,1444.76 1820,1444.79 1821.48,1444.83 1822.96,1444.86 1824.43,1444.9 1825.91,1444.93 1827.39,1444.96 1828.87,1444.98 1830.35,1445.01 1831.83,1445.04 1833.31,1445.07 1834.79,1445.09 1836.27,1445.11 1837.75,1445.14 1839.23,1445.16 1840.71,1445.18 1842.19,1445.2 1843.67,1445.22 1845.15,1445.24 1846.63,1445.26 1848.11,1445.28 1849.59,1445.3 1851.07,1445.31 1852.55,1445.33 1854.03,1445.34 1855.51,1445.36 1856.99,1445.37 1858.47,1445.39 1859.95,1445.4 1861.43,1445.41 1862.91,1445.42 1864.39,1445.44 1865.87,1445.45 1867.35,1445.46 1868.83,1445.47 1870.31,1445.48 1871.79,1445.49 1873.27,1445.5 1874.75,1445.51 1876.23,1445.52 1877.71,1445.52 1879.19,1445.53 1880.67,1445.54 1882.15,1445.55 1883.63,1445.55 1885.11,1445.56 1886.59,1445.57 1888.07,1445.57 1889.55,1445.58 1891.03,1445.58 1892.51,1445.59 1893.99,1445.6 1895.47,1445.6 1896.95,1445.61 1898.43,1445.61 1899.91,1445.61 1901.39,1445.62 1902.87,1445.62 1904.35,1445.63 1905.83,1445.63 1907.31,1445.63 1908.79,1445.64 1910.27,1445.64 1911.75,1445.64 1913.23,1445.65 1914.71,1445.65 1916.19,1445.65 1917.67,1445.66 1919.15,1445.66 1920.63,1445.66 1922.11,1445.66 1923.59,1445.67 1925.07,1445.67 1926.55,1445.67 1928.03,1445.67 1929.51,1445.67 1930.99,1445.68 1932.47,1445.68 1933.95,1445.68 1935.43,1445.68 1936.91,1445.68 1938.39,1445.68 1939.87,1445.69 1941.35,1445.69 1942.83,1445.69 1944.31,1445.69 1945.79,1445.69 1947.27,1445.69 1948.75,1445.69 1950.23,1445.69 1951.71,1445.7 1953.19,1445.7 1954.67,1445.7 1956.15,1445.7 1957.63,1445.7 1959.11,1445.7 1960.58,1445.7 1962.06,1445.7 1963.54,1445.7 1965.02,1445.7 1966.5,1445.7 1967.98,1445.7 1969.46,1445.7 1970.94,1445.7 1972.42,1445.71 1973.9,1445.71 1975.38,1445.71 1976.86,1445.71 1978.34,1445.71 1979.82,1445.71 1981.3,1445.71 1982.78,1445.71 1984.26,1445.71 1985.74,1445.71 1987.22,1445.71 1988.7,1445.71 1990.18,1445.71 1991.66,1445.71 1993.14,1445.71 1994.62,1445.71 1996.1,1445.71 1997.58,1445.71 1999.06,1445.71 2000.54,1445.71 2002.02,1445.71 2003.5,1445.71 2004.98,1445.71 2006.46,1445.71 2007.94,1445.71 2009.42,1445.71 2010.9,1445.71 2012.38,1445.71 2013.86,1445.71 2015.34,1445.71 2016.82,1445.71 2018.3,1445.71 2019.78,1445.71 2021.26,1445.71 2022.74,1445.71 2024.22,1445.71 2025.7,1445.71 2027.18,1445.71 2028.66,1445.71 2030.14,1445.71 2031.62,1445.71 2033.1,1445.71 2034.58,1445.71 2036.06,1445.71 2037.54,1445.71 2039.02,1445.71 2040.5,1445.71 2041.98,1445.71 2043.46,1445.71 2044.94,1445.71 2046.42,1445.71 2047.9,1445.71 2049.38,1445.71 2050.86,1445.71 2052.34,1445.72 2053.82,1445.72 2055.3,1445.72 2056.78,1445.72 2058.26,1445.72 2059.74,1445.72 2061.22,1445.72 2062.7,1445.72 2064.18,1445.72 2065.66,1445.72 2067.14,1445.72 2068.62,1445.72 2070.1,1445.72 2071.58,1445.72 2073.06,1445.72 2074.54,1445.72 2076.02,1445.72 2077.5,1445.72 2078.98,1445.72 2080.46,1445.72 2081.94,1445.72 2083.42,1445.72 2084.9,1445.72 2086.38,1445.72 2087.86,1445.72 2089.34,1445.72 2090.82,1445.72 2092.3,1445.72 2093.78,1445.72 2095.25,1445.72 2096.73,1445.72 2098.21,1445.72 2099.69,1445.72 2101.17,1445.72 2102.65,1445.72 2104.13,1445.72 2105.61,1445.72 2107.09,1445.72 2108.57,1445.72 2110.05,1445.72 2111.53,1445.72 2113.01,1445.72 2114.49,1445.72 2115.97,1445.72 2117.45,1445.72 2118.93,1445.72 2120.41,1445.72 2121.89,1445.72 2123.37,1445.72 2124.85,1445.72 2126.33,1445.72 2127.81,1445.72 2129.29,1445.72 2130.77,1445.72 2132.25,1445.72 2133.73,1445.72 2135.21,1445.72 2136.69,1445.72 2138.17,1445.72 2139.65,1445.72 2141.13,1445.72 2142.61,1445.72 2144.09,1445.72 2145.57,1445.72 2147.05,1445.72 2148.53,1445.72 2150.01,1445.72 2151.49,1445.72 2152.97,1445.72 2154.45,1445.72 2155.93,1445.72 2157.41,1445.72 2158.89,1445.72 2160.37,1445.72 2161.85,1445.72 2163.33,1445.72 2164.81,1445.72 2166.29,1445.72 2167.77,1445.72 2169.25,1445.72 2170.73,1445.72 2172.21,1445.72 2173.69,1445.72 2175.17,1445.72 2176.65,1445.72 2178.13,1445.72 2179.61,1445.72 2181.09,1445.72 2182.57,1445.72 2184.05,1445.72 2185.53,1445.72 2187.01,1445.72 2188.49,1445.72 2189.97,1445.72 2191.45,1445.72 2192.93,1445.72 2194.41,1445.72 2195.89,1445.72 2197.37,1445.72 2198.85,1445.72 2200.33,1445.72 2201.81,1445.72 2203.29,1445.72 2204.77,1445.72 2206.25,1445.72 2207.73,1445.72 2209.21,1445.72 2210.69,1445.72 2212.17,1445.72 2213.65,1445.72 2215.13,1445.72 2216.61,1445.72 2218.09,1445.72 2219.57,1445.72 2221.05,1445.72 2222.53,1445.72 2224.01,1445.72 2225.49,1445.72 2226.97,1445.72 2228.45,1445.72 2229.92,1445.72 2231.4,1445.72 2232.88,1445.72 2234.36,1445.72 2235.84,1445.72 2237.32,1445.72 2238.8,1445.72 2240.28,1445.72 2241.76,1445.72 2243.24,1445.72 2244.72,1445.72 2246.2,1445.72 2247.68,1445.72 2249.16,1445.72 2250.64,1445.72 2252.12,1445.72 2253.6,1445.72 2255.08,1445.72 2256.56,1445.72 2258.04,1445.72 2259.52,1445.72 2261,1445.72 2262.48,1445.72 2263.96,1445.72 2265.44,1445.72 2266.92,1445.72 2268.4,1445.72 2269.88,1445.72 2271.36,1445.72 2272.84,1445.72 2274.32,1445.72 2275.8,1445.72 2277.28,1445.72 2278.76,1445.72 2280.24,1445.72 2281.72,1445.72 2283.2,1445.72 2284.68,1445.72 2286.16,1445.72 2287.64,1445.72 2289.12,1445.72 2290.6,1445.72 "/>
<polyline clip-path="url(#clip682)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,1444.23 220.234,1444.2 221.713,1444.18 223.193,1444.15 224.673,1444.12 226.153,1444.1 227.633,1444.07 229.113,1444.04 230.593,1444.01 232.073,1443.98 233.553,1443.95 235.032,1443.92 236.512,1443.89 237.992,1443.86 239.472,1443.82 240.952,1443.79 242.432,1443.76 243.912,1443.72 245.392,1443.69 246.872,1443.65 248.351,1443.62 249.831,1443.58 251.311,1443.55 252.791,1443.51 254.271,1443.47 255.751,1443.43 257.231,1443.39 258.711,1443.36 260.191,1443.32 261.67,1443.27 263.15,1443.23 264.63,1443.19 266.11,1443.15 267.59,1443.11 269.07,1443.06 270.55,1443.02 272.03,1442.97 273.51,1442.93 274.989,1442.88 276.469,1442.83 277.949,1442.78 279.429,1442.74 280.909,1442.69 282.389,1442.64 283.869,1442.59 285.349,1442.53 286.829,1442.48 288.308,1442.43 289.788,1442.37 291.268,1442.32 292.748,1442.26 294.228,1442.21 295.708,1442.15 297.188,1442.09 298.668,1442.03 300.148,1441.97 301.628,1441.91 303.107,1441.85 304.587,1441.79 306.067,1441.72 307.547,1441.66 309.027,1441.59 310.507,1441.53 311.987,1441.46 313.467,1441.39 314.947,1441.32 316.426,1441.25 317.906,1441.18 319.386,1441.11 320.866,1441.04 322.346,1440.96 323.826,1440.89 325.306,1440.81 326.786,1440.73 328.266,1440.65 329.745,1440.57 331.225,1440.49 332.705,1440.41 334.185,1440.33 335.665,1440.24 337.145,1440.16 338.625,1440.07 340.105,1439.98 341.585,1439.89 343.064,1439.8 344.544,1439.71 346.024,1439.62 347.504,1439.52 348.984,1439.43 350.464,1439.33 351.944,1439.23 353.424,1439.13 354.904,1439.03 356.383,1438.93 357.863,1438.83 359.343,1438.72 360.823,1438.61 362.303,1438.51 363.783,1438.4 365.263,1438.29 366.743,1438.17 368.223,1438.06 369.702,1437.94 371.182,1437.83 372.662,1437.71 374.142,1437.59 375.622,1437.47 377.102,1437.34 378.582,1437.22 380.062,1437.09 381.542,1436.96 383.021,1436.83 384.501,1436.7 385.981,1436.57 387.461,1436.43 388.941,1436.3 390.421,1436.16 391.901,1436.02 393.381,1435.88 394.861,1435.73 396.341,1435.59 397.82,1435.44 399.3,1435.29 400.78,1435.14 402.26,1434.98 403.74,1434.83 405.22,1434.67 406.7,1434.51 408.18,1434.35 409.66,1434.19 411.139,1434.02 412.619,1433.85 414.099,1433.68 415.579,1433.51 417.059,1433.34 418.539,1433.16 420.019,1432.98 421.499,1432.8 422.979,1432.62 424.458,1432.43 425.938,1432.25 427.418,1432.06 428.898,1431.86 430.378,1431.67 431.858,1431.47 433.338,1431.27 434.818,1431.07 436.298,1430.87 437.777,1430.66 439.257,1430.45 440.737,1430.24 442.217,1430.03 443.697,1429.81 445.177,1429.59 446.657,1429.37 448.137,1429.15 449.617,1428.92 451.096,1428.69 452.576,1428.46 454.056,1428.22 455.536,1427.98 457.016,1427.74 458.496,1427.5 459.976,1427.25 461.456,1427 462.936,1426.75 464.415,1426.5 465.895,1426.24 467.375,1425.98 468.855,1425.71 470.335,1425.44 471.815,1425.17 473.295,1424.9 474.775,1424.63 476.255,1424.35 477.734,1424.06 479.214,1423.78 480.694,1423.49 482.174,1423.2 483.654,1422.9 485.134,1422.6 486.614,1422.3 488.094,1421.99 489.574,1421.69 491.054,1421.37 492.533,1421.06 494.013,1420.74 495.493,1420.42 496.973,1420.09 498.453,1419.76 499.933,1419.43 501.413,1419.09 502.893,1418.75 504.373,1418.4 505.852,1418.06 507.332,1417.7 508.812,1417.35 510.292,1416.99 511.772,1416.63 513.252,1416.26 514.732,1415.89 516.212,1415.51 517.692,1415.13 519.171,1414.75 520.651,1414.37 522.131,1413.97 523.611,1413.58 525.091,1413.18 526.571,1412.78 528.051,1412.37 529.531,1411.96 531.011,1411.54 532.49,1411.12 533.97,1410.7 535.45,1410.27 536.93,1409.84 538.41,1409.4 539.89,1408.96 541.37,1408.51 542.85,1408.06 544.33,1407.61 545.809,1407.15 547.289,1406.68 548.769,1406.22 550.249,1405.74 551.729,1405.26 553.209,1404.78 554.689,1404.29 556.169,1403.8 557.649,1403.3 559.128,1402.8 560.608,1402.3 562.088,1401.78 563.568,1401.27 565.048,1400.75 566.528,1400.22 568.008,1399.69 569.488,1399.15 570.968,1398.61 572.447,1398.06 573.927,1397.51 575.407,1396.96 576.887,1396.39 578.367,1395.83 579.847,1395.25 581.327,1394.68 582.807,1394.09 584.287,1393.5 585.767,1392.91 587.246,1392.31 588.726,1391.71 590.206,1391.09 591.686,1390.48 593.166,1389.86 594.646,1389.23 596.126,1388.6 597.606,1387.96 599.086,1387.32 600.565,1386.67 602.045,1386.01 603.525,1385.35 605.005,1384.68 606.485,1384.01 607.965,1383.33 609.445,1382.65 610.925,1381.96 612.405,1381.26 613.884,1380.56 615.364,1379.85 616.844,1379.13 618.324,1378.41 619.804,1377.69 621.284,1376.96 622.764,1376.22 624.244,1375.47 625.724,1374.72 627.203,1373.96 628.683,1373.2 630.163,1372.43 631.643,1371.65 633.123,1370.87 634.603,1370.08 636.083,1369.29 637.563,1368.48 639.043,1367.68 640.522,1366.86 642.002,1366.04 643.482,1365.21 644.962,1364.38 646.442,1363.54 647.922,1362.69 649.402,1361.83 650.882,1360.97 652.362,1360.1 653.841,1359.23 655.321,1358.35 656.801,1357.46 658.281,1356.57 659.761,1355.66 661.241,1354.76 662.721,1353.84 664.201,1352.92 665.681,1351.99 667.16,1351.05 668.64,1350.11 670.12,1349.16 671.6,1348.2 673.08,1347.24 674.56,1346.27 676.04,1345.29 677.52,1344.3 679,1343.31 680.48,1342.31 681.959,1341.31 683.439,1340.29 684.919,1339.27 686.399,1338.24 687.879,1337.21 689.359,1336.17 690.839,1335.12 692.319,1334.06 693.799,1332.99 695.278,1331.92 696.758,1330.84 698.238,1329.76 699.718,1328.66 701.198,1327.56 702.678,1326.45 704.158,1325.34 705.638,1324.22 707.118,1323.08 708.597,1321.95 710.077,1320.8 711.557,1319.65 713.037,1318.49 714.517,1317.32 715.997,1316.14 717.477,1314.96 718.957,1313.77 720.437,1312.57 721.916,1311.37 723.396,1310.16 724.876,1308.93 726.356,1307.71 727.836,1306.47 729.316,1305.23 730.796,1303.98 732.276,1302.72 733.756,1301.45 735.235,1300.18 736.715,1298.9 738.195,1297.61 739.675,1296.32 741.155,1295.01 742.635,1293.7 744.115,1292.38 745.595,1291.06 747.075,1289.72 748.554,1288.38 750.034,1287.03 751.514,1285.67 752.994,1284.31 754.474,1282.94 755.954,1281.56 757.434,1280.17 758.914,1278.78 760.394,1277.38 761.873,1275.97 763.353,1274.55 764.833,1273.13 766.313,1271.69 767.793,1270.25 769.273,1268.81 770.753,1267.35 772.233,1265.89 773.713,1264.42 775.193,1262.95 776.672,1261.46 778.152,1259.97 779.632,1258.47 781.112,1256.96 782.592,1255.45 784.072,1253.93 785.552,1252.4 787.032,1250.87 788.512,1249.32 789.991,1247.77 791.471,1246.22 792.951,1244.65 794.431,1243.08 795.911,1241.5 797.391,1239.91 798.871,1238.32 800.351,1236.72 801.831,1235.11 803.31,1233.5 804.79,1231.88 806.27,1230.25 807.75,1228.61 809.23,1226.97 810.71,1225.32 812.19,1223.66 813.67,1222 815.15,1220.33 816.629,1218.65 818.109,1216.97 819.589,1215.28 821.069,1213.58 822.549,1211.88 824.029,1210.17 825.509,1208.45 826.989,1206.73 828.469,1205 829.948,1203.26 831.428,1201.52 832.908,1199.77 834.388,1198.01 835.868,1196.25 837.348,1194.48 838.828,1192.71 840.308,1190.93 841.788,1189.14 843.267,1187.35 844.747,1185.55 846.227,1183.75 847.707,1181.94 849.187,1180.12 850.667,1178.3 852.147,1176.47 853.627,1174.63 855.107,1172.8 856.586,1170.95 858.066,1169.1 859.546,1167.24 861.026,1165.38 862.506,1163.52 863.986,1161.64 865.466,1159.77 866.946,1157.88 868.426,1156 869.906,1154.1 871.385,1152.2 872.865,1150.3 874.345,1148.39 875.825,1146.48 877.305,1144.56 878.785,1142.64 880.265,1140.71 881.745,1138.78 883.225,1136.85 884.704,1134.91 886.184,1132.96 887.664,1131.01 889.144,1129.06 890.624,1127.1 892.104,1125.14 893.584,1123.18 895.064,1121.21 896.544,1119.23 898.023,1117.26 899.503,1115.27 900.983,1113.29 902.463,1111.3 903.943,1109.31 905.423,1107.32 906.903,1105.32 908.383,1103.32 909.863,1101.31 911.342,1099.3 912.822,1097.29 914.302,1095.28 915.782,1093.26 917.262,1091.24 918.742,1089.22 920.222,1087.2 921.702,1085.17 923.182,1083.14 924.661,1081.11 926.141,1079.08 927.621,1077.04 929.101,1075 930.581,1072.96 932.061,1070.92 933.541,1068.88 935.021,1066.83 936.501,1064.79 937.98,1062.74 939.46,1060.69 940.94,1058.64 942.42,1056.58 943.9,1054.53 945.38,1052.48 946.86,1050.42 948.34,1048.37 949.82,1046.31 951.299,1044.25 952.779,1042.19 954.259,1040.14 955.739,1038.08 957.219,1036.02 958.699,1033.96 960.179,1031.9 961.659,1029.84 963.139,1027.78 964.619,1025.73 966.098,1023.67 967.578,1021.61 969.058,1019.55 970.538,1017.5 972.018,1015.44 973.498,1013.39 974.978,1011.34 976.458,1009.29 977.938,1007.23 979.417,1005.19 980.897,1003.14 982.377,1001.09 983.857,999.047 985.337,997.005 986.817,994.964 988.297,992.925 989.777,990.889 991.257,988.855 992.736,986.823 994.216,984.794 995.696,982.767 997.176,980.743 998.656,978.722 1000.14,976.703 1001.62,974.688 1003.1,972.676 1004.58,970.667 1006.06,968.662 1007.54,966.66 1009.02,964.662 1010.5,962.667 1011.98,960.677 1013.45,958.69 1014.93,956.708 1016.41,954.729 1017.89,952.755 1019.37,950.786 1020.85,948.82 1022.33,946.86 1023.81,944.904 1025.29,942.954 1026.77,941.008 1028.25,939.068 1029.73,937.132 1031.21,935.202 1032.69,933.278 1034.17,931.359 1035.65,929.446 1037.13,927.539 1038.61,925.637 1040.09,923.742 1041.57,921.853 1043.05,919.97 1044.53,918.094 1046.01,916.224 1047.49,914.361 1048.97,912.505 1050.45,910.656 1051.93,908.813 1053.41,906.978 1054.89,905.15 1056.37,903.329 1057.85,901.516 1059.33,899.71 1060.81,897.912 1062.29,896.122 1063.77,894.34 1065.25,892.566 1066.73,890.8 1068.21,889.042 1069.69,887.293 1071.17,885.552 1072.65,883.82 1074.13,882.096 1075.61,880.382 1077.09,878.676 1078.57,876.979 1080.05,875.292 1081.53,873.614 1083.01,871.945 1084.49,870.286 1085.97,868.637 1087.45,866.997 1088.93,865.367 1090.41,863.747 1091.89,862.137 1093.37,860.537 1094.85,858.948 1096.33,857.369 1097.81,855.8 1099.29,854.242 1100.77,852.695 1102.25,851.159 1103.73,849.633 1105.21,848.119 1106.69,846.615 1108.17,845.123 1109.65,843.642 1111.13,842.173 1112.61,840.715 1114.09,839.269 1115.57,837.835 1117.05,836.412 1118.53,835.001 1120.01,833.603 1121.49,832.216 1122.97,830.842 1124.45,829.48 1125.93,828.131 1127.41,826.794 1128.89,825.469 1130.37,824.158 1131.85,822.859 1133.33,821.573 1134.81,820.3 1136.29,819.04 1137.77,817.793 1139.25,816.56 1140.73,815.34 1142.21,814.133 1143.69,812.94 1145.17,811.76 1146.65,810.594 1148.12,809.442 1149.6,808.303 1151.08,807.179 1152.56,806.069 1154.04,804.972 1155.52,803.89 1157,802.822 1158.48,801.769 1159.96,800.729 1161.44,799.705 1162.92,798.694 1164.4,797.699 1165.88,796.718 1167.36,795.752 1168.84,794.801 1170.32,793.864 1171.8,792.943 1173.28,792.036 1174.76,791.145 1176.24,790.269 1177.72,789.408 1179.2,788.563 1180.68,787.733 1182.16,786.918 1183.64,786.119 1185.12,785.335 1186.6,784.567 1188.08,783.814 1189.56,783.077 1191.04,782.356 1192.52,781.651 1194,780.962 1195.48,780.289 1196.96,779.631 1198.44,778.99 1199.92,778.364 1201.4,777.755 1202.88,777.162 1204.36,776.585 1205.84,776.024 1207.32,775.48 1208.8,774.952 1210.28,774.44 1211.76,773.945 1213.24,773.466 1214.72,773.004 1216.2,772.558 1217.68,772.129 1219.16,771.716 1220.64,771.32 1222.12,770.941 1223.6,770.578 1225.08,770.232 1226.56,769.902 1228.04,769.59 1229.52,769.294 1231,769.015 1232.48,768.753 1233.96,768.507 1235.44,768.279 1236.92,768.067 1238.4,767.872 1239.88,767.694 1241.36,767.533 1242.84,767.389 1244.32,767.262 1245.8,767.151 1247.28,767.058 1248.76,766.982 1250.24,766.922 1251.72,766.88 1253.2,766.854 1254.68,766.846 1256.16,766.854 1257.64,766.88 1259.12,766.922 1260.6,766.982 1262.08,767.058 1263.56,767.151 1265.04,767.262 1266.52,767.389 1268,767.533 1269.48,767.694 1270.96,767.872 1272.44,768.067 1273.92,768.279 1275.4,768.507 1276.88,768.753 1278.36,769.015 1279.84,769.294 1281.32,769.59 1282.79,769.902 1284.27,770.232 1285.75,770.578 1287.23,770.941 1288.71,771.32 1290.19,771.716 1291.67,772.129 1293.15,772.558 1294.63,773.004 1296.11,773.466 1297.59,773.945 1299.07,774.44 1300.55,774.952 1302.03,775.48 1303.51,776.024 1304.99,776.585 1306.47,777.162 1307.95,777.755 1309.43,778.364 1310.91,778.99 1312.39,779.631 1313.87,780.289 1315.35,780.962 1316.83,781.651 1318.31,782.356 1319.79,783.077 1321.27,783.814 1322.75,784.567 1324.23,785.335 1325.71,786.119 1327.19,786.918 1328.67,787.733 1330.15,788.563 1331.63,789.408 1333.11,790.269 1334.59,791.145 1336.07,792.036 1337.55,792.943 1339.03,793.864 1340.51,794.801 1341.99,795.752 1343.47,796.718 1344.95,797.699 1346.43,798.694 1347.91,799.705 1349.39,800.729 1350.87,801.769 1352.35,802.822 1353.83,803.89 1355.31,804.972 1356.79,806.069 1358.27,807.179 1359.75,808.303 1361.23,809.442 1362.71,810.594 1364.19,811.76 1365.67,812.94 1367.15,814.133 1368.63,815.34 1370.11,816.56 1371.59,817.793 1373.07,819.04 1374.55,820.3 1376.03,821.573 1377.51,822.859 1378.99,824.158 1380.47,825.469 1381.95,826.794 1383.43,828.131 1384.91,829.48 1386.39,830.842 1387.87,832.216 1389.35,833.603 1390.83,835.001 1392.31,836.412 1393.79,837.835 1395.27,839.269 1396.75,840.715 1398.23,842.173 1399.71,843.642 1401.19,845.123 1402.67,846.615 1404.15,848.119 1405.63,849.633 1407.11,851.159 1408.59,852.695 1410.07,854.242 1411.55,855.8 1413.03,857.369 1414.51,858.948 1415.99,860.537 1417.47,862.137 1418.94,863.747 1420.42,865.367 1421.9,866.997 1423.38,868.637 1424.86,870.286 1426.34,871.945 1427.82,873.614 1429.3,875.292 1430.78,876.979 1432.26,878.676 1433.74,880.382 1435.22,882.096 1436.7,883.82 1438.18,885.552 1439.66,887.293 1441.14,889.042 1442.62,890.8 1444.1,892.566 1445.58,894.34 1447.06,896.122 1448.54,897.912 1450.02,899.71 1451.5,901.516 1452.98,903.329 1454.46,905.15 1455.94,906.978 1457.42,908.813 1458.9,910.656 1460.38,912.505 1461.86,914.361 1463.34,916.224 1464.82,918.094 1466.3,919.97 1467.78,921.853 1469.26,923.742 1470.74,925.637 1472.22,927.539 1473.7,929.446 1475.18,931.359 1476.66,933.278 1478.14,935.202 1479.62,937.132 1481.1,939.068 1482.58,941.008 1484.06,942.954 1485.54,944.904 1487.02,946.86 1488.5,948.82 1489.98,950.786 1491.46,952.755 1492.94,954.729 1494.42,956.708 1495.9,958.69 1497.38,960.677 1498.86,962.667 1500.34,964.662 1501.82,966.66 1503.3,968.662 1504.78,970.667 1506.26,972.676 1507.74,974.688 1509.22,976.703 1510.7,978.722 1512.18,980.743 1513.66,982.767 1515.14,984.794 1516.62,986.823 1518.1,988.855 1519.58,990.889 1521.06,992.925 1522.54,994.964 1524.02,997.005 1525.5,999.047 1526.98,1001.09 1528.46,1003.14 1529.94,1005.19 1531.42,1007.23 1532.9,1009.29 1534.38,1011.34 1535.86,1013.39 1537.34,1015.44 1538.82,1017.5 1540.3,1019.55 1541.78,1021.61 1543.26,1023.67 1544.74,1025.73 1546.22,1027.78 1547.7,1029.84 1549.18,1031.9 1550.66,1033.96 1552.14,1036.02 1553.61,1038.08 1555.09,1040.14 1556.57,1042.19 1558.05,1044.25 1559.53,1046.31 1561.01,1048.37 1562.49,1050.42 1563.97,1052.48 1565.45,1054.53 1566.93,1056.58 1568.41,1058.64 1569.89,1060.69 1571.37,1062.74 1572.85,1064.79 1574.33,1066.83 1575.81,1068.88 1577.29,1070.92 1578.77,1072.96 1580.25,1075 1581.73,1077.04 1583.21,1079.08 1584.69,1081.11 1586.17,1083.14 1587.65,1085.17 1589.13,1087.2 1590.61,1089.22 1592.09,1091.24 1593.57,1093.26 1595.05,1095.28 1596.53,1097.29 1598.01,1099.3 1599.49,1101.31 1600.97,1103.32 1602.45,1105.32 1603.93,1107.32 1605.41,1109.31 1606.89,1111.3 1608.37,1113.29 1609.85,1115.27 1611.33,1117.26 1612.81,1119.23 1614.29,1121.21 1615.77,1123.18 1617.25,1125.14 1618.73,1127.1 1620.21,1129.06 1621.69,1131.01 1623.17,1132.96 1624.65,1134.91 1626.13,1136.85 1627.61,1138.78 1629.09,1140.71 1630.57,1142.64 1632.05,1144.56 1633.53,1146.48 1635.01,1148.39 1636.49,1150.3 1637.97,1152.2 1639.45,1154.1 1640.93,1156 1642.41,1157.88 1643.89,1159.77 1645.37,1161.64 1646.85,1163.52 1648.33,1165.38 1649.81,1167.24 1651.29,1169.1 1652.77,1170.95 1654.25,1172.8 1655.73,1174.63 1657.21,1176.47 1658.69,1178.3 1660.17,1180.12 1661.65,1181.94 1663.13,1183.75 1664.61,1185.55 1666.09,1187.35 1667.57,1189.14 1669.05,1190.93 1670.53,1192.71 1672.01,1194.48 1673.49,1196.25 1674.97,1198.01 1676.45,1199.77 1677.93,1201.52 1679.41,1203.26 1680.89,1205 1682.37,1206.73 1683.85,1208.45 1685.33,1210.17 1686.81,1211.88 1688.29,1213.58 1689.76,1215.28 1691.24,1216.97 1692.72,1218.65 1694.2,1220.33 1695.68,1222 1697.16,1223.66 1698.64,1225.32 1700.12,1226.97 1701.6,1228.61 1703.08,1230.25 1704.56,1231.88 1706.04,1233.5 1707.52,1235.11 1709,1236.72 1710.48,1238.32 1711.96,1239.91 1713.44,1241.5 1714.92,1243.08 1716.4,1244.65 1717.88,1246.22 1719.36,1247.77 1720.84,1249.32 1722.32,1250.87 1723.8,1252.4 1725.28,1253.93 1726.76,1255.45 1728.24,1256.96 1729.72,1258.47 1731.2,1259.97 1732.68,1261.46 1734.16,1262.95 1735.64,1264.42 1737.12,1265.89 1738.6,1267.35 1740.08,1268.81 1741.56,1270.25 1743.04,1271.69 1744.52,1273.13 1746,1274.55 1747.48,1275.97 1748.96,1277.38 1750.44,1278.78 1751.92,1280.17 1753.4,1281.56 1754.88,1282.94 1756.36,1284.31 1757.84,1285.67 1759.32,1287.03 1760.8,1288.38 1762.28,1289.72 1763.76,1291.06 1765.24,1292.38 1766.72,1293.7 1768.2,1295.01 1769.68,1296.32 1771.16,1297.61 1772.64,1298.9 1774.12,1300.18 1775.6,1301.45 1777.08,1302.72 1778.56,1303.98 1780.04,1305.23 1781.52,1306.47 1783,1307.71 1784.48,1308.93 1785.96,1310.16 1787.44,1311.37 1788.92,1312.57 1790.4,1313.77 1791.88,1314.96 1793.36,1316.14 1794.84,1317.32 1796.32,1318.49 1797.8,1319.65 1799.28,1320.8 1800.76,1321.95 1802.24,1323.08 1803.72,1324.22 1805.2,1325.34 1806.68,1326.45 1808.16,1327.56 1809.64,1328.66 1811.12,1329.76 1812.6,1330.84 1814.08,1331.92 1815.56,1332.99 1817.04,1334.06 1818.52,1335.12 1820,1336.17 1821.48,1337.21 1822.96,1338.24 1824.43,1339.27 1825.91,1340.29 1827.39,1341.31 1828.87,1342.31 1830.35,1343.31 1831.83,1344.3 1833.31,1345.29 1834.79,1346.27 1836.27,1347.24 1837.75,1348.2 1839.23,1349.16 1840.71,1350.11 1842.19,1351.05 1843.67,1351.99 1845.15,1352.92 1846.63,1353.84 1848.11,1354.76 1849.59,1355.66 1851.07,1356.57 1852.55,1357.46 1854.03,1358.35 1855.51,1359.23 1856.99,1360.1 1858.47,1360.97 1859.95,1361.83 1861.43,1362.69 1862.91,1363.54 1864.39,1364.38 1865.87,1365.21 1867.35,1366.04 1868.83,1366.86 1870.31,1367.68 1871.79,1368.48 1873.27,1369.29 1874.75,1370.08 1876.23,1370.87 1877.71,1371.65 1879.19,1372.43 1880.67,1373.2 1882.15,1373.96 1883.63,1374.72 1885.11,1375.47 1886.59,1376.22 1888.07,1376.96 1889.55,1377.69 1891.03,1378.41 1892.51,1379.13 1893.99,1379.85 1895.47,1380.56 1896.95,1381.26 1898.43,1381.96 1899.91,1382.65 1901.39,1383.33 1902.87,1384.01 1904.35,1384.68 1905.83,1385.35 1907.31,1386.01 1908.79,1386.67 1910.27,1387.32 1911.75,1387.96 1913.23,1388.6 1914.71,1389.23 1916.19,1389.86 1917.67,1390.48 1919.15,1391.09 1920.63,1391.71 1922.11,1392.31 1923.59,1392.91 1925.07,1393.5 1926.55,1394.09 1928.03,1394.68 1929.51,1395.25 1930.99,1395.83 1932.47,1396.39 1933.95,1396.96 1935.43,1397.51 1936.91,1398.06 1938.39,1398.61 1939.87,1399.15 1941.35,1399.69 1942.83,1400.22 1944.31,1400.75 1945.79,1401.27 1947.27,1401.78 1948.75,1402.3 1950.23,1402.8 1951.71,1403.3 1953.19,1403.8 1954.67,1404.29 1956.15,1404.78 1957.63,1405.26 1959.11,1405.74 1960.58,1406.22 1962.06,1406.68 1963.54,1407.15 1965.02,1407.61 1966.5,1408.06 1967.98,1408.51 1969.46,1408.96 1970.94,1409.4 1972.42,1409.84 1973.9,1410.27 1975.38,1410.7 1976.86,1411.12 1978.34,1411.54 1979.82,1411.96 1981.3,1412.37 1982.78,1412.78 1984.26,1413.18 1985.74,1413.58 1987.22,1413.97 1988.7,1414.37 1990.18,1414.75 1991.66,1415.13 1993.14,1415.51 1994.62,1415.89 1996.1,1416.26 1997.58,1416.63 1999.06,1416.99 2000.54,1417.35 2002.02,1417.7 2003.5,1418.06 2004.98,1418.4 2006.46,1418.75 2007.94,1419.09 2009.42,1419.43 2010.9,1419.76 2012.38,1420.09 2013.86,1420.42 2015.34,1420.74 2016.82,1421.06 2018.3,1421.37 2019.78,1421.69 2021.26,1421.99 2022.74,1422.3 2024.22,1422.6 2025.7,1422.9 2027.18,1423.2 2028.66,1423.49 2030.14,1423.78 2031.62,1424.06 2033.1,1424.35 2034.58,1424.63 2036.06,1424.9 2037.54,1425.17 2039.02,1425.44 2040.5,1425.71 2041.98,1425.98 2043.46,1426.24 2044.94,1426.5 2046.42,1426.75 2047.9,1427 2049.38,1427.25 2050.86,1427.5 2052.34,1427.74 2053.82,1427.98 2055.3,1428.22 2056.78,1428.46 2058.26,1428.69 2059.74,1428.92 2061.22,1429.15 2062.7,1429.37 2064.18,1429.59 2065.66,1429.81 2067.14,1430.03 2068.62,1430.24 2070.1,1430.45 2071.58,1430.66 2073.06,1430.87 2074.54,1431.07 2076.02,1431.27 2077.5,1431.47 2078.98,1431.67 2080.46,1431.86 2081.94,1432.06 2083.42,1432.25 2084.9,1432.43 2086.38,1432.62 2087.86,1432.8 2089.34,1432.98 2090.82,1433.16 2092.3,1433.34 2093.78,1433.51 2095.25,1433.68 2096.73,1433.85 2098.21,1434.02 2099.69,1434.19 2101.17,1434.35 2102.65,1434.51 2104.13,1434.67 2105.61,1434.83 2107.09,1434.98 2108.57,1435.14 2110.05,1435.29 2111.53,1435.44 2113.01,1435.59 2114.49,1435.73 2115.97,1435.88 2117.45,1436.02 2118.93,1436.16 2120.41,1436.3 2121.89,1436.43 2123.37,1436.57 2124.85,1436.7 2126.33,1436.83 2127.81,1436.96 2129.29,1437.09 2130.77,1437.22 2132.25,1437.34 2133.73,1437.47 2135.21,1437.59 2136.69,1437.71 2138.17,1437.83 2139.65,1437.94 2141.13,1438.06 2142.61,1438.17 2144.09,1438.29 2145.57,1438.4 2147.05,1438.51 2148.53,1438.61 2150.01,1438.72 2151.49,1438.83 2152.97,1438.93 2154.45,1439.03 2155.93,1439.13 2157.41,1439.23 2158.89,1439.33 2160.37,1439.43 2161.85,1439.52 2163.33,1439.62 2164.81,1439.71 2166.29,1439.8 2167.77,1439.89 2169.25,1439.98 2170.73,1440.07 2172.21,1440.16 2173.69,1440.24 2175.17,1440.33 2176.65,1440.41 2178.13,1440.49 2179.61,1440.57 2181.09,1440.65 2182.57,1440.73 2184.05,1440.81 2185.53,1440.89 2187.01,1440.96 2188.49,1441.04 2189.97,1441.11 2191.45,1441.18 2192.93,1441.25 2194.41,1441.32 2195.89,1441.39 2197.37,1441.46 2198.85,1441.53 2200.33,1441.59 2201.81,1441.66 2203.29,1441.72 2204.77,1441.79 2206.25,1441.85 2207.73,1441.91 2209.21,1441.97 2210.69,1442.03 2212.17,1442.09 2213.65,1442.15 2215.13,1442.21 2216.61,1442.26 2218.09,1442.32 2219.57,1442.37 2221.05,1442.43 2222.53,1442.48 2224.01,1442.53 2225.49,1442.59 2226.97,1442.64 2228.45,1442.69 2229.92,1442.74 2231.4,1442.78 2232.88,1442.83 2234.36,1442.88 2235.84,1442.93 2237.32,1442.97 2238.8,1443.02 2240.28,1443.06 2241.76,1443.11 2243.24,1443.15 2244.72,1443.19 2246.2,1443.23 2247.68,1443.27 2249.16,1443.32 2250.64,1443.36 2252.12,1443.39 2253.6,1443.43 2255.08,1443.47 2256.56,1443.51 2258.04,1443.55 2259.52,1443.58 2261,1443.62 2262.48,1443.65 2263.96,1443.69 2265.44,1443.72 2266.92,1443.76 2268.4,1443.79 2269.88,1443.82 2271.36,1443.86 2272.84,1443.89 2274.32,1443.92 2275.8,1443.95 2277.28,1443.98 2278.76,1444.01 2280.24,1444.04 2281.72,1444.07 2283.2,1444.1 2284.68,1444.12 2286.16,1444.15 2287.64,1444.18 2289.12,1444.2 2290.6,1444.23 "/>
<polyline clip-path="url(#clip682)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,1445.72 220.234,1445.72 221.713,1445.72 223.193,1445.72 224.673,1445.72 226.153,1445.72 227.633,1445.72 229.113,1445.72 230.593,1445.72 232.073,1445.72 233.553,1445.72 235.032,1445.72 236.512,1445.72 237.992,1445.72 239.472,1445.72 240.952,1445.72 242.432,1445.72 243.912,1445.72 245.392,1445.72 246.872,1445.72 248.351,1445.72 249.831,1445.72 251.311,1445.72 252.791,1445.72 254.271,1445.72 255.751,1445.72 257.231,1445.72 258.711,1445.72 260.191,1445.72 261.67,1445.72 263.15,1445.72 264.63,1445.72 266.11,1445.72 267.59,1445.72 269.07,1445.72 270.55,1445.72 272.03,1445.72 273.51,1445.72 274.989,1445.72 276.469,1445.72 277.949,1445.72 279.429,1445.72 280.909,1445.72 282.389,1445.72 283.869,1445.72 285.349,1445.72 286.829,1445.72 288.308,1445.72 289.788,1445.72 291.268,1445.72 292.748,1445.72 294.228,1445.72 295.708,1445.72 297.188,1445.72 298.668,1445.72 300.148,1445.72 301.628,1445.72 303.107,1445.72 304.587,1445.72 306.067,1445.72 307.547,1445.72 309.027,1445.72 310.507,1445.72 311.987,1445.72 313.467,1445.72 314.947,1445.72 316.426,1445.72 317.906,1445.72 319.386,1445.72 320.866,1445.72 322.346,1445.72 323.826,1445.72 325.306,1445.72 326.786,1445.72 328.266,1445.72 329.745,1445.72 331.225,1445.72 332.705,1445.72 334.185,1445.72 335.665,1445.72 337.145,1445.72 338.625,1445.72 340.105,1445.72 341.585,1445.72 343.064,1445.72 344.544,1445.72 346.024,1445.72 347.504,1445.72 348.984,1445.72 350.464,1445.72 351.944,1445.72 353.424,1445.72 354.904,1445.72 356.383,1445.72 357.863,1445.72 359.343,1445.72 360.823,1445.72 362.303,1445.72 363.783,1445.72 365.263,1445.72 366.743,1445.72 368.223,1445.72 369.702,1445.72 371.182,1445.72 372.662,1445.72 374.142,1445.72 375.622,1445.72 377.102,1445.72 378.582,1445.72 380.062,1445.72 381.542,1445.72 383.021,1445.72 384.501,1445.72 385.981,1445.72 387.461,1445.72 388.941,1445.72 390.421,1445.72 391.901,1445.72 393.381,1445.72 394.861,1445.72 396.341,1445.72 397.82,1445.72 399.3,1445.72 400.78,1445.72 402.26,1445.72 403.74,1445.72 405.22,1445.72 406.7,1445.72 408.18,1445.72 409.66,1445.72 411.139,1445.72 412.619,1445.72 414.099,1445.72 415.579,1445.72 417.059,1445.72 418.539,1445.72 420.019,1445.72 421.499,1445.72 422.979,1445.72 424.458,1445.72 425.938,1445.72 427.418,1445.72 428.898,1445.72 430.378,1445.72 431.858,1445.72 433.338,1445.72 434.818,1445.72 436.298,1445.72 437.777,1445.72 439.257,1445.72 440.737,1445.72 442.217,1445.72 443.697,1445.72 445.177,1445.72 446.657,1445.72 448.137,1445.72 449.617,1445.72 451.096,1445.72 452.576,1445.72 454.056,1445.72 455.536,1445.72 457.016,1445.72 458.496,1445.72 459.976,1445.72 461.456,1445.72 462.936,1445.72 464.415,1445.72 465.895,1445.72 467.375,1445.72 468.855,1445.72 470.335,1445.72 471.815,1445.72 473.295,1445.72 474.775,1445.72 476.255,1445.72 477.734,1445.72 479.214,1445.72 480.694,1445.72 482.174,1445.72 483.654,1445.72 485.134,1445.72 486.614,1445.72 488.094,1445.72 489.574,1445.72 491.054,1445.72 492.533,1445.72 494.013,1445.72 495.493,1445.72 496.973,1445.72 498.453,1445.72 499.933,1445.72 501.413,1445.72 502.893,1445.72 504.373,1445.72 505.852,1445.72 507.332,1445.72 508.812,1445.72 510.292,1445.72 511.772,1445.72 513.252,1445.72 514.732,1445.72 516.212,1445.72 517.692,1445.72 519.171,1445.72 520.651,1445.72 522.131,1445.72 523.611,1445.72 525.091,1445.72 526.571,1445.72 528.051,1445.72 529.531,1445.72 531.011,1445.72 532.49,1445.72 533.97,1445.72 535.45,1445.72 536.93,1445.72 538.41,1445.72 539.89,1445.72 541.37,1445.72 542.85,1445.72 544.33,1445.72 545.809,1445.72 547.289,1445.72 548.769,1445.72 550.249,1445.72 551.729,1445.72 553.209,1445.72 554.689,1445.72 556.169,1445.72 557.649,1445.72 559.128,1445.72 560.608,1445.72 562.088,1445.72 563.568,1445.72 565.048,1445.72 566.528,1445.72 568.008,1445.72 569.488,1445.72 570.968,1445.72 572.447,1445.72 573.927,1445.72 575.407,1445.72 576.887,1445.72 578.367,1445.72 579.847,1445.72 581.327,1445.72 582.807,1445.72 584.287,1445.72 585.767,1445.72 587.246,1445.72 588.726,1445.72 590.206,1445.72 591.686,1445.72 593.166,1445.72 594.646,1445.72 596.126,1445.72 597.606,1445.72 599.086,1445.72 600.565,1445.72 602.045,1445.72 603.525,1445.72 605.005,1445.72 606.485,1445.72 607.965,1445.72 609.445,1445.72 610.925,1445.72 612.405,1445.72 613.884,1445.72 615.364,1445.72 616.844,1445.72 618.324,1445.72 619.804,1445.72 621.284,1445.72 622.764,1445.72 624.244,1445.72 625.724,1445.72 627.203,1445.72 628.683,1445.72 630.163,1445.72 631.643,1445.72 633.123,1445.72 634.603,1445.72 636.083,1445.72 637.563,1445.72 639.043,1445.72 640.522,1445.72 642.002,1445.72 643.482,1445.72 644.962,1445.72 646.442,1445.72 647.922,1445.72 649.402,1445.72 650.882,1445.72 652.362,1445.72 653.841,1445.72 655.321,1445.72 656.801,1445.72 658.281,1445.72 659.761,1445.72 661.241,1445.72 662.721,1445.72 664.201,1445.72 665.681,1445.72 667.16,1445.72 668.64,1445.72 670.12,1445.72 671.6,1445.72 673.08,1445.72 674.56,1445.72 676.04,1445.72 677.52,1445.72 679,1445.72 680.48,1445.72 681.959,1445.72 683.439,1445.72 684.919,1445.72 686.399,1445.72 687.879,1445.72 689.359,1445.72 690.839,1445.72 692.319,1445.72 693.799,1445.72 695.278,1445.72 696.758,1445.72 698.238,1445.72 699.718,1445.72 701.198,1445.72 702.678,1445.72 704.158,1445.72 705.638,1445.72 707.118,1445.72 708.597,1445.72 710.077,1445.72 711.557,1445.72 713.037,1445.72 714.517,1445.72 715.997,1445.72 717.477,1445.72 718.957,1445.72 720.437,1445.72 721.916,1445.72 723.396,1445.72 724.876,1445.72 726.356,1445.72 727.836,1445.72 729.316,1445.72 730.796,1445.72 732.276,1445.72 733.756,1445.72 735.235,1445.72 736.715,1445.72 738.195,1445.72 739.675,1445.72 741.155,1445.72 742.635,1445.72 744.115,1445.72 745.595,1445.72 747.075,1445.72 748.554,1445.72 750.034,1445.72 751.514,1445.72 752.994,1445.72 754.474,1445.72 755.954,1445.72 757.434,1445.72 758.914,1445.72 760.394,1445.72 761.873,1445.72 763.353,1445.72 764.833,1445.72 766.313,1445.72 767.793,1445.72 769.273,1445.72 770.753,1445.72 772.233,1445.72 773.713,1445.72 775.193,1445.72 776.672,1445.72 778.152,1445.72 779.632,1445.72 781.112,1445.72 782.592,1445.72 784.072,1445.72 785.552,1445.72 787.032,1445.72 788.512,1445.72 789.991,1445.72 791.471,1445.72 792.951,1445.72 794.431,1445.72 795.911,1445.72 797.391,1445.72 798.871,1445.72 800.351,1445.72 801.831,1445.72 803.31,1445.72 804.79,1445.72 806.27,1445.72 807.75,1445.72 809.23,1445.72 810.71,1445.72 812.19,1445.72 813.67,1445.72 815.15,1445.72 816.629,1445.72 818.109,1445.72 819.589,1445.72 821.069,1445.72 822.549,1445.72 824.029,1445.72 825.509,1445.72 826.989,1445.72 828.469,1445.72 829.948,1445.72 831.428,1445.72 832.908,1445.72 834.388,1445.72 835.868,1445.72 837.348,1445.72 838.828,1445.72 840.308,1445.72 841.788,1445.72 843.267,1445.72 844.747,1445.72 846.227,1445.72 847.707,1445.72 849.187,1445.72 850.667,1445.72 852.147,1445.72 853.627,1445.72 855.107,1445.72 856.586,1445.72 858.066,1445.72 859.546,1445.72 861.026,1445.72 862.506,1445.72 863.986,1445.72 865.466,1445.72 866.946,1445.72 868.426,1445.72 869.906,1445.72 871.385,1445.72 872.865,1445.72 874.345,1445.72 875.825,1445.72 877.305,1445.72 878.785,1445.72 880.265,1445.72 881.745,1445.72 883.225,1445.72 884.704,1445.72 886.184,1445.72 887.664,1445.72 889.144,1445.72 890.624,1445.72 892.104,1445.72 893.584,1445.72 895.064,1445.72 896.544,1445.72 898.023,1445.72 899.503,1445.72 900.983,1445.72 902.463,1445.71 903.943,1445.71 905.423,1445.71 906.903,1445.71 908.383,1445.71 909.863,1445.71 911.342,1445.71 912.822,1445.71 914.302,1445.71 915.782,1445.71 917.262,1445.71 918.742,1445.71 920.222,1445.71 921.702,1445.71 923.182,1445.71 924.661,1445.71 926.141,1445.71 927.621,1445.71 929.101,1445.71 930.581,1445.71 932.061,1445.71 933.541,1445.71 935.021,1445.71 936.501,1445.71 937.98,1445.71 939.46,1445.71 940.94,1445.71 942.42,1445.71 943.9,1445.71 945.38,1445.71 946.86,1445.71 948.34,1445.71 949.82,1445.71 951.299,1445.71 952.779,1445.71 954.259,1445.71 955.739,1445.71 957.219,1445.71 958.699,1445.71 960.179,1445.71 961.659,1445.71 963.139,1445.71 964.619,1445.71 966.098,1445.71 967.578,1445.71 969.058,1445.71 970.538,1445.71 972.018,1445.71 973.498,1445.71 974.978,1445.71 976.458,1445.71 977.938,1445.71 979.417,1445.71 980.897,1445.71 982.377,1445.7 983.857,1445.7 985.337,1445.7 986.817,1445.7 988.297,1445.7 989.777,1445.7 991.257,1445.7 992.736,1445.7 994.216,1445.7 995.696,1445.7 997.176,1445.7 998.656,1445.7 1000.14,1445.7 1001.62,1445.7 1003.1,1445.69 1004.58,1445.69 1006.06,1445.69 1007.54,1445.69 1009.02,1445.69 1010.5,1445.69 1011.98,1445.69 1013.45,1445.69 1014.93,1445.68 1016.41,1445.68 1017.89,1445.68 1019.37,1445.68 1020.85,1445.68 1022.33,1445.68 1023.81,1445.67 1025.29,1445.67 1026.77,1445.67 1028.25,1445.67 1029.73,1445.67 1031.21,1445.66 1032.69,1445.66 1034.17,1445.66 1035.65,1445.66 1037.13,1445.65 1038.61,1445.65 1040.09,1445.65 1041.57,1445.64 1043.05,1445.64 1044.53,1445.64 1046.01,1445.63 1047.49,1445.63 1048.97,1445.63 1050.45,1445.62 1051.93,1445.62 1053.41,1445.61 1054.89,1445.61 1056.37,1445.61 1057.85,1445.6 1059.33,1445.6 1060.81,1445.59 1062.29,1445.58 1063.77,1445.58 1065.25,1445.57 1066.73,1445.57 1068.21,1445.56 1069.69,1445.55 1071.17,1445.55 1072.65,1445.54 1074.13,1445.53 1075.61,1445.52 1077.09,1445.52 1078.57,1445.51 1080.05,1445.5 1081.53,1445.49 1083.01,1445.48 1084.49,1445.47 1085.97,1445.46 1087.45,1445.45 1088.93,1445.44 1090.41,1445.42 1091.89,1445.41 1093.37,1445.4 1094.85,1445.39 1096.33,1445.37 1097.81,1445.36 1099.29,1445.34 1100.77,1445.33 1102.25,1445.31 1103.73,1445.3 1105.21,1445.28 1106.69,1445.26 1108.17,1445.24 1109.65,1445.22 1111.13,1445.2 1112.61,1445.18 1114.09,1445.16 1115.57,1445.14 1117.05,1445.11 1118.53,1445.09 1120.01,1445.07 1121.49,1445.04 1122.97,1445.01 1124.45,1444.98 1125.93,1444.96 1127.41,1444.93 1128.89,1444.9 1130.37,1444.86 1131.85,1444.83 1133.33,1444.79 1134.81,1444.76 1136.29,1444.72 1137.77,1444.68 1139.25,1444.64 1140.73,1444.6 1142.21,1444.56 1143.69,1444.52 1145.17,1444.47 1146.65,1444.42 1148.12,1444.37 1149.6,1444.32 1151.08,1444.27 1152.56,1444.22 1154.04,1444.16 1155.52,1444.1 1157,1444.04 1158.48,1443.98 1159.96,1443.91 1161.44,1443.85 1162.92,1443.78 1164.4,1443.71 1165.88,1443.63 1167.36,1443.56 1168.84,1443.48 1170.32,1443.4 1171.8,1443.31 1173.28,1443.23 1174.76,1443.14 1176.24,1443.04 1177.72,1442.95 1179.2,1442.85 1180.68,1442.75 1182.16,1442.64 1183.64,1442.53 1185.12,1442.42 1186.6,1442.3 1188.08,1442.18 1189.56,1442.06 1191.04,1441.93 1192.52,1441.8 1194,1441.66 1195.48,1441.52 1196.96,1441.38 1198.44,1441.23 1199.92,1441.07 1201.4,1440.91 1202.88,1440.75 1204.36,1440.58 1205.84,1440.41 1207.32,1440.23 1208.8,1440.04 1210.28,1439.85 1211.76,1439.66 1213.24,1439.45 1214.72,1439.25 1216.2,1439.03 1217.68,1438.81 1219.16,1438.58 1220.64,1438.35 1222.12,1438.11 1223.6,1437.86 1225.08,1437.6 1226.56,1437.34 1228.04,1437.07 1229.52,1436.79 1231,1436.5 1232.48,1436.21 1233.96,1435.9 1235.44,1435.59 1236.92,1435.27 1238.4,1434.94 1239.88,1434.6 1241.36,1434.25 1242.84,1433.89 1244.32,1433.52 1245.8,1433.14 1247.28,1432.75 1248.76,1432.35 1250.24,1431.94 1251.72,1431.51 1253.2,1431.08 1254.68,1430.63 1256.16,1430.17 1257.64,1429.7 1259.12,1429.22 1260.6,1428.72 1262.08,1428.21 1263.56,1427.69 1265.04,1427.15 1266.52,1426.6 1268,1426.04 1269.48,1425.46 1270.96,1424.86 1272.44,1424.25 1273.92,1423.63 1275.4,1422.98 1276.88,1422.33 1278.36,1421.65 1279.84,1420.96 1281.32,1420.25 1282.79,1419.52 1284.27,1418.78 1285.75,1418.01 1287.23,1417.23 1288.71,1416.43 1290.19,1415.61 1291.67,1414.77 1293.15,1413.91 1294.63,1413.02 1296.11,1412.12 1297.59,1411.2 1299.07,1410.25 1300.55,1409.28 1302.03,1408.29 1303.51,1407.27 1304.99,1406.24 1306.47,1405.17 1307.95,1404.09 1309.43,1402.98 1310.91,1401.84 1312.39,1400.68 1313.87,1399.49 1315.35,1398.27 1316.83,1397.03 1318.31,1395.76 1319.79,1394.46 1321.27,1393.14 1322.75,1391.78 1324.23,1390.4 1325.71,1388.98 1327.19,1387.54 1328.67,1386.06 1330.15,1384.55 1331.63,1383.01 1333.11,1381.44 1334.59,1379.84 1336.07,1378.2 1337.55,1376.53 1339.03,1374.83 1340.51,1373.09 1341.99,1371.31 1343.47,1369.5 1344.95,1367.65 1346.43,1365.77 1347.91,1363.85 1349.39,1361.89 1350.87,1359.89 1352.35,1357.85 1353.83,1355.78 1355.31,1353.66 1356.79,1351.51 1358.27,1349.31 1359.75,1347.07 1361.23,1344.79 1362.71,1342.47 1364.19,1340.1 1365.67,1337.69 1367.15,1335.24 1368.63,1332.75 1370.11,1330.2 1371.59,1327.62 1373.07,1324.98 1374.55,1322.3 1376.03,1319.58 1377.51,1316.8 1378.99,1313.98 1380.47,1311.11 1381.95,1308.19 1383.43,1305.23 1384.91,1302.21 1386.39,1299.14 1387.87,1296.02 1389.35,1292.86 1390.83,1289.63 1392.31,1286.36 1393.79,1283.04 1395.27,1279.66 1396.75,1276.23 1398.23,1272.74 1399.71,1269.21 1401.19,1265.61 1402.67,1261.97 1404.15,1258.26 1405.63,1254.5 1407.11,1250.69 1408.59,1246.82 1410.07,1242.89 1411.55,1238.91 1413.03,1234.87 1414.51,1230.77 1415.99,1226.62 1417.47,1222.4 1418.94,1218.13 1420.42,1213.8 1421.9,1209.41 1423.38,1204.96 1424.86,1200.45 1426.34,1195.89 1427.82,1191.26 1429.3,1186.57 1430.78,1181.83 1432.26,1177.02 1433.74,1172.15 1435.22,1167.23 1436.7,1162.24 1438.18,1157.19 1439.66,1152.08 1441.14,1146.92 1442.62,1141.69 1444.1,1136.4 1445.58,1131.05 1447.06,1125.63 1448.54,1120.16 1450.02,1114.63 1451.5,1109.04 1452.98,1103.39 1454.46,1097.67 1455.94,1091.9 1457.42,1086.07 1458.9,1080.18 1460.38,1074.22 1461.86,1068.21 1463.34,1062.14 1464.82,1056.02 1466.3,1049.83 1467.78,1043.59 1469.26,1037.28 1470.74,1030.92 1472.22,1024.51 1473.7,1018.04 1475.18,1011.51 1476.66,1004.92 1478.14,998.283 1479.62,991.589 1481.1,984.841 1482.58,978.039 1484.06,971.185 1485.54,964.278 1487.02,957.32 1488.5,950.31 1489.98,943.25 1491.46,936.141 1492.94,928.983 1494.42,921.776 1495.9,914.522 1497.38,907.222 1498.86,899.875 1500.34,892.484 1501.82,885.049 1503.3,877.571 1504.78,870.05 1506.26,862.489 1507.74,854.887 1509.22,847.246 1510.7,839.566 1512.18,831.85 1513.66,824.097 1515.14,816.31 1516.62,808.488 1518.1,800.634 1519.58,792.749 1521.06,784.833 1522.54,776.888 1524.02,768.915 1525.5,760.916 1526.98,752.891 1528.46,744.843 1529.94,736.772 1531.42,728.679 1532.9,720.567 1534.38,712.437 1535.86,704.289 1537.34,696.126 1538.82,687.948 1540.3,679.758 1541.78,671.557 1543.26,663.347 1544.74,655.128 1546.22,646.903 1547.7,638.673 1549.18,630.44 1550.66,622.205 1552.14,613.97 1553.61,605.737 1555.09,597.507 1556.57,589.282 1558.05,581.064 1559.53,572.855 1561.01,564.655 1562.49,556.467 1563.97,548.294 1565.45,540.135 1566.93,531.994 1568.41,523.871 1569.89,515.77 1571.37,507.691 1572.85,499.636 1574.33,491.608 1575.81,483.608 1577.29,475.638 1578.77,467.699 1580.25,459.795 1581.73,451.925 1583.21,444.093 1584.69,436.301 1586.17,428.549 1587.65,420.84 1589.13,413.176 1590.61,405.559 1592.09,397.991 1593.57,390.473 1595.05,383.007 1596.53,375.595 1598.01,368.24 1599.49,360.942 1600.97,353.705 1602.45,346.528 1603.93,339.416 1605.41,332.368 1606.89,325.388 1608.37,318.477 1609.85,311.636 1611.33,304.869 1612.81,298.175 1614.29,291.558 1615.77,285.018 1617.25,278.558 1618.73,272.18 1620.21,265.885 1621.69,259.674 1623.17,253.551 1624.65,247.515 1626.13,241.569 1627.61,235.715 1629.09,229.954 1630.57,224.287 1632.05,218.717 1633.53,213.245 1635.01,207.872 1636.49,202.6 1637.97,197.43 1639.45,192.364 1640.93,187.404 1642.41,182.55 1643.89,177.804 1645.37,173.168 1646.85,168.642 1648.33,164.229 1649.81,159.929 1651.29,155.743 1652.77,151.673 1654.25,147.72 1655.73,143.885 1657.21,140.17 1658.69,136.575 1660.17,133.101 1661.65,129.75 1663.13,126.521 1664.61,123.418 1666.09,120.439 1667.57,117.587 1669.05,114.861 1670.53,112.264 1672.01,109.794 1673.49,107.455 1674.97,105.245 1676.45,103.165 1677.93,101.217 1679.41,99.4008 1680.89,97.7169 1682.37,96.1658 1683.85,94.748 1685.33,93.464 1686.81,92.3141 1688.29,91.2987 1689.76,90.418 1691.24,89.6724 1692.72,89.062 1694.2,88.5871 1695.68,88.2478 1697.16,88.0442 1698.64,87.9763 1700.12,88.0442 1701.6,88.2478 1703.08,88.5871 1704.56,89.062 1706.04,89.6724 1707.52,90.418 1709,91.2987 1710.48,92.3141 1711.96,93.464 1713.44,94.748 1714.92,96.1658 1716.4,97.7169 1717.88,99.4008 1719.36,101.217 1720.84,103.165 1722.32,105.245 1723.8,107.455 1725.28,109.794 1726.76,112.264 1728.24,114.861 1729.72,117.587 1731.2,120.439 1732.68,123.418 1734.16,126.521 1735.64,129.75 1737.12,133.101 1738.6,136.575 1740.08,140.17 1741.56,143.885 1743.04,147.72 1744.52,151.673 1746,155.743 1747.48,159.929 1748.96,164.229 1750.44,168.642 1751.92,173.168 1753.4,177.804 1754.88,182.55 1756.36,187.404 1757.84,192.364 1759.32,197.43 1760.8,202.6 1762.28,207.872 1763.76,213.245 1765.24,218.717 1766.72,224.287 1768.2,229.954 1769.68,235.715 1771.16,241.569 1772.64,247.515 1774.12,253.551 1775.6,259.674 1777.08,265.885 1778.56,272.18 1780.04,278.558 1781.52,285.018 1783,291.558 1784.48,298.175 1785.96,304.869 1787.44,311.636 1788.92,318.477 1790.4,325.388 1791.88,332.368 1793.36,339.416 1794.84,346.528 1796.32,353.705 1797.8,360.942 1799.28,368.24 1800.76,375.595 1802.24,383.007 1803.72,390.473 1805.2,397.991 1806.68,405.559 1808.16,413.176 1809.64,420.84 1811.12,428.549 1812.6,436.301 1814.08,444.093 1815.56,451.925 1817.04,459.795 1818.52,467.699 1820,475.638 1821.48,483.608 1822.96,491.608 1824.43,499.636 1825.91,507.691 1827.39,515.77 1828.87,523.871 1830.35,531.994 1831.83,540.135 1833.31,548.294 1834.79,556.467 1836.27,564.655 1837.75,572.855 1839.23,581.064 1840.71,589.282 1842.19,597.507 1843.67,605.737 1845.15,613.97 1846.63,622.205 1848.11,630.44 1849.59,638.673 1851.07,646.903 1852.55,655.128 1854.03,663.347 1855.51,671.557 1856.99,679.758 1858.47,687.948 1859.95,696.126 1861.43,704.289 1862.91,712.437 1864.39,720.567 1865.87,728.679 1867.35,736.772 1868.83,744.843 1870.31,752.891 1871.79,760.916 1873.27,768.915 1874.75,776.888 1876.23,784.833 1877.71,792.749 1879.19,800.634 1880.67,808.488 1882.15,816.31 1883.63,824.097 1885.11,831.85 1886.59,839.566 1888.07,847.246 1889.55,854.887 1891.03,862.489 1892.51,870.05 1893.99,877.571 1895.47,885.049 1896.95,892.484 1898.43,899.875 1899.91,907.222 1901.39,914.522 1902.87,921.776 1904.35,928.983 1905.83,936.141 1907.31,943.25 1908.79,950.31 1910.27,957.32 1911.75,964.278 1913.23,971.185 1914.71,978.039 1916.19,984.841 1917.67,991.589 1919.15,998.283 1920.63,1004.92 1922.11,1011.51 1923.59,1018.04 1925.07,1024.51 1926.55,1030.92 1928.03,1037.28 1929.51,1043.59 1930.99,1049.83 1932.47,1056.02 1933.95,1062.14 1935.43,1068.21 1936.91,1074.22 1938.39,1080.18 1939.87,1086.07 1941.35,1091.9 1942.83,1097.67 1944.31,1103.39 1945.79,1109.04 1947.27,1114.63 1948.75,1120.16 1950.23,1125.63 1951.71,1131.05 1953.19,1136.4 1954.67,1141.69 1956.15,1146.92 1957.63,1152.08 1959.11,1157.19 1960.58,1162.24 1962.06,1167.23 1963.54,1172.15 1965.02,1177.02 1966.5,1181.83 1967.98,1186.57 1969.46,1191.26 1970.94,1195.89 1972.42,1200.45 1973.9,1204.96 1975.38,1209.41 1976.86,1213.8 1978.34,1218.13 1979.82,1222.4 1981.3,1226.62 1982.78,1230.77 1984.26,1234.87 1985.74,1238.91 1987.22,1242.89 1988.7,1246.82 1990.18,1250.69 1991.66,1254.5 1993.14,1258.26 1994.62,1261.97 1996.1,1265.61 1997.58,1269.21 1999.06,1272.74 2000.54,1276.23 2002.02,1279.66 2003.5,1283.04 2004.98,1286.36 2006.46,1289.63 2007.94,1292.86 2009.42,1296.02 2010.9,1299.14 2012.38,1302.21 2013.86,1305.23 2015.34,1308.19 2016.82,1311.11 2018.3,1313.98 2019.78,1316.8 2021.26,1319.58 2022.74,1322.3 2024.22,1324.98 2025.7,1327.62 2027.18,1330.2 2028.66,1332.75 2030.14,1335.24 2031.62,1337.69 2033.1,1340.1 2034.58,1342.47 2036.06,1344.79 2037.54,1347.07 2039.02,1349.31 2040.5,1351.51 2041.98,1353.66 2043.46,1355.78 2044.94,1357.85 2046.42,1359.89 2047.9,1361.89 2049.38,1363.85 2050.86,1365.77 2052.34,1367.65 2053.82,1369.5 2055.3,1371.31 2056.78,1373.09 2058.26,1374.83 2059.74,1376.53 2061.22,1378.2 2062.7,1379.84 2064.18,1381.44 2065.66,1383.01 2067.14,1384.55 2068.62,1386.06 2070.1,1387.54 2071.58,1388.98 2073.06,1390.4 2074.54,1391.78 2076.02,1393.14 2077.5,1394.46 2078.98,1395.76 2080.46,1397.03 2081.94,1398.27 2083.42,1399.49 2084.9,1400.68 2086.38,1401.84 2087.86,1402.98 2089.34,1404.09 2090.82,1405.17 2092.3,1406.24 2093.78,1407.27 2095.25,1408.29 2096.73,1409.28 2098.21,1410.25 2099.69,1411.2 2101.17,1412.12 2102.65,1413.02 2104.13,1413.91 2105.61,1414.77 2107.09,1415.61 2108.57,1416.43 2110.05,1417.23 2111.53,1418.01 2113.01,1418.78 2114.49,1419.52 2115.97,1420.25 2117.45,1420.96 2118.93,1421.65 2120.41,1422.33 2121.89,1422.98 2123.37,1423.63 2124.85,1424.25 2126.33,1424.86 2127.81,1425.46 2129.29,1426.04 2130.77,1426.6 2132.25,1427.15 2133.73,1427.69 2135.21,1428.21 2136.69,1428.72 2138.17,1429.22 2139.65,1429.7 2141.13,1430.17 2142.61,1430.63 2144.09,1431.08 2145.57,1431.51 2147.05,1431.94 2148.53,1432.35 2150.01,1432.75 2151.49,1433.14 2152.97,1433.52 2154.45,1433.89 2155.93,1434.25 2157.41,1434.6 2158.89,1434.94 2160.37,1435.27 2161.85,1435.59 2163.33,1435.9 2164.81,1436.21 2166.29,1436.5 2167.77,1436.79 2169.25,1437.07 2170.73,1437.34 2172.21,1437.6 2173.69,1437.86 2175.17,1438.11 2176.65,1438.35 2178.13,1438.58 2179.61,1438.81 2181.09,1439.03 2182.57,1439.25 2184.05,1439.45 2185.53,1439.66 2187.01,1439.85 2188.49,1440.04 2189.97,1440.23 2191.45,1440.41 2192.93,1440.58 2194.41,1440.75 2195.89,1440.91 2197.37,1441.07 2198.85,1441.23 2200.33,1441.38 2201.81,1441.52 2203.29,1441.66 2204.77,1441.8 2206.25,1441.93 2207.73,1442.06 2209.21,1442.18 2210.69,1442.3 2212.17,1442.42 2213.65,1442.53 2215.13,1442.64 2216.61,1442.75 2218.09,1442.85 2219.57,1442.95 2221.05,1443.04 2222.53,1443.14 2224.01,1443.23 2225.49,1443.31 2226.97,1443.4 2228.45,1443.48 2229.92,1443.56 2231.4,1443.63 2232.88,1443.71 2234.36,1443.78 2235.84,1443.85 2237.32,1443.91 2238.8,1443.98 2240.28,1444.04 2241.76,1444.1 2243.24,1444.16 2244.72,1444.22 2246.2,1444.27 2247.68,1444.32 2249.16,1444.37 2250.64,1444.42 2252.12,1444.47 2253.6,1444.52 2255.08,1444.56 2256.56,1444.6 2258.04,1444.64 2259.52,1444.68 2261,1444.72 2262.48,1444.76 2263.96,1444.79 2265.44,1444.83 2266.92,1444.86 2268.4,1444.9 2269.88,1444.93 2271.36,1444.96 2272.84,1444.98 2274.32,1445.01 2275.8,1445.04 2277.28,1445.07 2278.76,1445.09 2280.24,1445.11 2281.72,1445.14 2283.2,1445.16 2284.68,1445.18 2286.16,1445.2 2287.64,1445.22 2289.12,1445.24 2290.6,1445.26 "/>
<path clip-path="url(#clip680)" d="M229.803 302.578 L823.908 302.578 L823.908 95.2176 L229.803 95.2176  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip680)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="229.803,302.578 823.908,302.578 823.908,95.2176 229.803,95.2176 229.803,302.578 "/>
<polyline clip-path="url(#clip680)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,147.058 400.616,147.058 "/>
<path clip-path="url(#clip680)" d="M445.365 143.389 Q446.962 140.518 449.184 139.153 Q451.406 137.787 454.415 137.787 Q458.466 137.787 460.665 140.634 Q462.865 143.458 462.865 148.689 L462.865 164.338 L458.582 164.338 L458.582 148.828 Q458.582 145.102 457.263 143.296 Q455.943 141.49 453.235 141.49 Q449.925 141.49 448.003 143.69 Q446.082 145.889 446.082 149.685 L446.082 164.338 L441.8 164.338 L441.8 148.828 Q441.8 145.078 440.48 143.296 Q439.161 141.49 436.406 141.49 Q433.142 141.49 431.221 143.713 Q429.3 145.912 429.3 149.685 L429.3 164.338 L425.018 164.338 L425.018 138.412 L429.3 138.412 L429.3 142.44 Q430.758 140.055 432.795 138.921 Q434.832 137.787 437.633 137.787 Q440.457 137.787 442.425 139.222 Q444.416 140.657 445.365 143.389 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M493.536 150.31 L493.536 152.393 L473.952 152.393 Q474.23 156.791 476.591 159.106 Q478.976 161.398 483.212 161.398 Q485.665 161.398 487.957 160.796 Q490.272 160.194 492.54 158.99 L492.54 163.018 Q490.249 163.99 487.841 164.5 Q485.434 165.009 482.957 165.009 Q476.753 165.009 473.119 161.398 Q469.508 157.787 469.508 151.629 Q469.508 145.264 472.934 141.537 Q476.383 137.787 482.216 137.787 Q487.448 137.787 490.48 141.166 Q493.536 144.523 493.536 150.31 M489.276 149.06 Q489.23 145.565 487.309 143.481 Q485.411 141.398 482.263 141.398 Q478.698 141.398 476.545 143.412 Q474.415 145.426 474.091 149.083 L489.276 149.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M512.309 151.305 Q507.147 151.305 505.156 152.486 Q503.165 153.666 503.165 156.514 Q503.165 158.782 504.647 160.125 Q506.151 161.444 508.721 161.444 Q512.262 161.444 514.392 158.944 Q516.545 156.421 516.545 152.254 L516.545 151.305 L512.309 151.305 M520.804 149.546 L520.804 164.338 L516.545 164.338 L516.545 160.402 Q515.086 162.763 512.911 163.898 Q510.735 165.009 507.587 165.009 Q503.605 165.009 501.244 162.787 Q498.906 160.541 498.906 156.791 Q498.906 152.416 501.823 150.194 Q504.762 147.972 510.573 147.972 L516.545 147.972 L516.545 147.555 Q516.545 144.615 514.6 143.018 Q512.679 141.398 509.184 141.398 Q506.962 141.398 504.855 141.93 Q502.749 142.463 500.804 143.527 L500.804 139.592 Q503.142 138.69 505.341 138.25 Q507.54 137.787 509.624 137.787 Q515.249 137.787 518.026 140.703 Q520.804 143.62 520.804 149.546 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M551.128 148.689 L551.128 164.338 L546.869 164.338 L546.869 148.828 Q546.869 145.148 545.434 143.319 Q543.998 141.49 541.128 141.49 Q537.679 141.49 535.688 143.69 Q533.697 145.889 533.697 149.685 L533.697 164.338 L529.415 164.338 L529.415 138.412 L533.697 138.412 L533.697 142.44 Q535.225 140.102 537.285 138.944 Q539.369 137.787 542.077 137.787 Q546.545 137.787 548.836 140.565 Q551.128 143.319 551.128 148.689 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M585.294 132.856 Q581.683 132.856 579.855 136.421 Q578.049 139.963 578.049 147.092 Q578.049 154.199 579.855 157.764 Q581.683 161.305 585.294 161.305 Q588.929 161.305 590.734 157.764 Q592.563 154.199 592.563 147.092 Q592.563 139.963 590.734 136.421 Q588.929 132.856 585.294 132.856 M585.294 129.153 Q591.105 129.153 594.16 133.759 Q597.239 138.342 597.239 147.092 Q597.239 155.819 594.16 160.426 Q591.105 165.009 585.294 165.009 Q579.484 165.009 576.406 160.426 Q573.35 155.819 573.35 147.092 Q573.35 138.342 576.406 133.759 Q579.484 129.153 585.294 129.153 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M636.452 139.176 L636.452 143.203 Q634.646 142.277 632.702 141.815 Q630.757 141.352 628.674 141.352 Q625.503 141.352 623.905 142.324 Q622.331 143.296 622.331 145.24 Q622.331 146.722 623.466 147.578 Q624.6 148.412 628.026 149.176 L629.484 149.5 Q634.021 150.472 635.919 152.254 Q637.84 154.014 637.84 157.185 Q637.84 160.796 634.97 162.902 Q632.123 165.009 627.123 165.009 Q625.04 165.009 622.771 164.592 Q620.526 164.199 618.026 163.388 L618.026 158.99 Q620.387 160.217 622.678 160.842 Q624.97 161.444 627.215 161.444 Q630.225 161.444 631.845 160.426 Q633.465 159.384 633.465 157.509 Q633.465 155.773 632.285 154.847 Q631.128 153.921 627.169 153.064 L625.688 152.717 Q621.729 151.884 619.97 150.171 Q618.211 148.435 618.211 145.426 Q618.211 141.768 620.803 139.778 Q623.396 137.787 628.165 137.787 Q630.526 137.787 632.609 138.134 Q634.692 138.481 636.452 139.176 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M644.623 138.412 L648.882 138.412 L648.882 164.338 L644.623 164.338 L644.623 138.412 M644.623 128.319 L648.882 128.319 L648.882 133.713 L644.623 133.713 L644.623 128.319 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M674.854 151.074 Q674.854 146.444 672.933 143.898 Q671.035 141.352 667.586 141.352 Q664.16 141.352 662.238 143.898 Q660.34 146.444 660.34 151.074 Q660.34 155.68 662.238 158.226 Q664.16 160.773 667.586 160.773 Q671.035 160.773 672.933 158.226 Q674.854 155.68 674.854 151.074 M679.113 161.12 Q679.113 167.74 676.174 170.958 Q673.234 174.199 667.169 174.199 Q664.924 174.199 662.933 173.851 Q660.942 173.527 659.067 172.833 L659.067 168.689 Q660.942 169.708 662.771 170.194 Q664.6 170.68 666.498 170.68 Q670.687 170.68 672.771 168.481 Q674.854 166.305 674.854 161.884 L674.854 159.777 Q673.535 162.069 671.475 163.203 Q669.414 164.338 666.544 164.338 Q661.775 164.338 658.859 160.703 Q655.942 157.069 655.942 151.074 Q655.942 145.055 658.859 141.421 Q661.775 137.787 666.544 137.787 Q669.414 137.787 671.475 138.921 Q673.535 140.055 674.854 142.347 L674.854 138.412 L679.113 138.412 L679.113 161.12 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M708.072 143.389 Q709.669 140.518 711.891 139.153 Q714.113 137.787 717.122 137.787 Q721.173 137.787 723.372 140.634 Q725.571 143.458 725.571 148.689 L725.571 164.338 L721.289 164.338 L721.289 148.828 Q721.289 145.102 719.97 143.296 Q718.65 141.49 715.942 141.49 Q712.632 141.49 710.71 143.69 Q708.789 145.889 708.789 149.685 L708.789 164.338 L704.507 164.338 L704.507 148.828 Q704.507 145.078 703.187 143.296 Q701.868 141.49 699.113 141.49 Q695.849 141.49 693.928 143.713 Q692.007 145.912 692.007 149.685 L692.007 164.338 L687.724 164.338 L687.724 138.412 L692.007 138.412 L692.007 142.44 Q693.465 140.055 695.502 138.921 Q697.539 137.787 700.34 137.787 Q703.164 137.787 705.132 139.222 Q707.122 140.657 708.072 143.389 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M745.849 151.305 Q740.687 151.305 738.696 152.486 Q736.706 153.666 736.706 156.514 Q736.706 158.782 738.187 160.125 Q739.692 161.444 742.261 161.444 Q745.803 161.444 747.932 158.944 Q750.085 156.421 750.085 152.254 L750.085 151.305 L745.849 151.305 M754.344 149.546 L754.344 164.338 L750.085 164.338 L750.085 160.402 Q748.627 162.763 746.451 163.898 Q744.275 165.009 741.127 165.009 Q737.145 165.009 734.784 162.787 Q732.446 160.541 732.446 156.791 Q732.446 152.416 735.363 150.194 Q738.303 147.972 744.113 147.972 L750.085 147.972 L750.085 147.555 Q750.085 144.615 748.141 143.018 Q746.219 141.398 742.724 141.398 Q740.502 141.398 738.395 141.93 Q736.289 142.463 734.345 143.527 L734.345 139.592 Q736.683 138.69 738.882 138.25 Q741.081 137.787 743.164 137.787 Q748.789 137.787 751.567 140.703 Q754.344 143.62 754.344 149.546 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M779.599 160.402 L787.238 160.402 L787.238 134.037 L778.928 135.703 L778.928 131.444 L787.191 129.778 L791.867 129.778 L791.867 160.402 L799.506 160.402 L799.506 164.338 L779.599 164.338 L779.599 160.402 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip680)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,198.898 400.616,198.898 "/>
<path clip-path="url(#clip680)" d="M445.365 195.229 Q446.962 192.358 449.184 190.993 Q451.406 189.627 454.415 189.627 Q458.466 189.627 460.665 192.474 Q462.865 195.298 462.865 200.529 L462.865 216.178 L458.582 216.178 L458.582 200.668 Q458.582 196.942 457.263 195.136 Q455.943 193.33 453.235 193.33 Q449.925 193.33 448.003 195.53 Q446.082 197.729 446.082 201.525 L446.082 216.178 L441.8 216.178 L441.8 200.668 Q441.8 196.918 440.48 195.136 Q439.161 193.33 436.406 193.33 Q433.142 193.33 431.221 195.553 Q429.3 197.752 429.3 201.525 L429.3 216.178 L425.018 216.178 L425.018 190.252 L429.3 190.252 L429.3 194.28 Q430.758 191.895 432.795 190.761 Q434.832 189.627 437.633 189.627 Q440.457 189.627 442.425 191.062 Q444.416 192.497 445.365 195.229 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M493.536 202.15 L493.536 204.233 L473.952 204.233 Q474.23 208.631 476.591 210.946 Q478.976 213.238 483.212 213.238 Q485.665 213.238 487.957 212.636 Q490.272 212.034 492.54 210.83 L492.54 214.858 Q490.249 215.83 487.841 216.34 Q485.434 216.849 482.957 216.849 Q476.753 216.849 473.119 213.238 Q469.508 209.627 469.508 203.469 Q469.508 197.104 472.934 193.377 Q476.383 189.627 482.216 189.627 Q487.448 189.627 490.48 193.006 Q493.536 196.363 493.536 202.15 M489.276 200.9 Q489.23 197.405 487.309 195.321 Q485.411 193.238 482.263 193.238 Q478.698 193.238 476.545 195.252 Q474.415 197.266 474.091 200.923 L489.276 200.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M512.309 203.145 Q507.147 203.145 505.156 204.326 Q503.165 205.506 503.165 208.354 Q503.165 210.622 504.647 211.965 Q506.151 213.284 508.721 213.284 Q512.262 213.284 514.392 210.784 Q516.545 208.261 516.545 204.094 L516.545 203.145 L512.309 203.145 M520.804 201.386 L520.804 216.178 L516.545 216.178 L516.545 212.242 Q515.086 214.603 512.911 215.738 Q510.735 216.849 507.587 216.849 Q503.605 216.849 501.244 214.627 Q498.906 212.381 498.906 208.631 Q498.906 204.256 501.823 202.034 Q504.762 199.812 510.573 199.812 L516.545 199.812 L516.545 199.395 Q516.545 196.455 514.6 194.858 Q512.679 193.238 509.184 193.238 Q506.962 193.238 504.855 193.77 Q502.749 194.303 500.804 195.367 L500.804 191.432 Q503.142 190.53 505.341 190.09 Q507.54 189.627 509.624 189.627 Q515.249 189.627 518.026 192.543 Q520.804 195.46 520.804 201.386 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M551.128 200.529 L551.128 216.178 L546.869 216.178 L546.869 200.668 Q546.869 196.988 545.434 195.159 Q543.998 193.33 541.128 193.33 Q537.679 193.33 535.688 195.53 Q533.697 197.729 533.697 201.525 L533.697 216.178 L529.415 216.178 L529.415 190.252 L533.697 190.252 L533.697 194.28 Q535.225 191.942 537.285 190.784 Q539.369 189.627 542.077 189.627 Q546.545 189.627 548.836 192.405 Q551.128 195.159 551.128 200.529 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M585.294 184.696 Q581.683 184.696 579.855 188.261 Q578.049 191.803 578.049 198.932 Q578.049 206.039 579.855 209.604 Q581.683 213.145 585.294 213.145 Q588.929 213.145 590.734 209.604 Q592.563 206.039 592.563 198.932 Q592.563 191.803 590.734 188.261 Q588.929 184.696 585.294 184.696 M585.294 180.993 Q591.105 180.993 594.16 185.599 Q597.239 190.182 597.239 198.932 Q597.239 207.659 594.16 212.266 Q591.105 216.849 585.294 216.849 Q579.484 216.849 576.406 212.266 Q573.35 207.659 573.35 198.932 Q573.35 190.182 576.406 185.599 Q579.484 180.993 585.294 180.993 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M636.452 191.016 L636.452 195.043 Q634.646 194.117 632.702 193.655 Q630.757 193.192 628.674 193.192 Q625.503 193.192 623.905 194.164 Q622.331 195.136 622.331 197.08 Q622.331 198.562 623.466 199.418 Q624.6 200.252 628.026 201.016 L629.484 201.34 Q634.021 202.312 635.919 204.094 Q637.84 205.854 637.84 209.025 Q637.84 212.636 634.97 214.742 Q632.123 216.849 627.123 216.849 Q625.04 216.849 622.771 216.432 Q620.526 216.039 618.026 215.228 L618.026 210.83 Q620.387 212.057 622.678 212.682 Q624.97 213.284 627.215 213.284 Q630.225 213.284 631.845 212.266 Q633.465 211.224 633.465 209.349 Q633.465 207.613 632.285 206.687 Q631.128 205.761 627.169 204.904 L625.688 204.557 Q621.729 203.724 619.97 202.011 Q618.211 200.275 618.211 197.266 Q618.211 193.608 620.803 191.618 Q623.396 189.627 628.165 189.627 Q630.526 189.627 632.609 189.974 Q634.692 190.321 636.452 191.016 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M644.623 190.252 L648.882 190.252 L648.882 216.178 L644.623 216.178 L644.623 190.252 M644.623 180.159 L648.882 180.159 L648.882 185.553 L644.623 185.553 L644.623 180.159 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M674.854 202.914 Q674.854 198.284 672.933 195.738 Q671.035 193.192 667.586 193.192 Q664.16 193.192 662.238 195.738 Q660.34 198.284 660.34 202.914 Q660.34 207.52 662.238 210.066 Q664.16 212.613 667.586 212.613 Q671.035 212.613 672.933 210.066 Q674.854 207.52 674.854 202.914 M679.113 212.96 Q679.113 219.58 676.174 222.798 Q673.234 226.039 667.169 226.039 Q664.924 226.039 662.933 225.691 Q660.942 225.367 659.067 224.673 L659.067 220.529 Q660.942 221.548 662.771 222.034 Q664.6 222.52 666.498 222.52 Q670.687 222.52 672.771 220.321 Q674.854 218.145 674.854 213.724 L674.854 211.617 Q673.535 213.909 671.475 215.043 Q669.414 216.178 666.544 216.178 Q661.775 216.178 658.859 212.543 Q655.942 208.909 655.942 202.914 Q655.942 196.895 658.859 193.261 Q661.775 189.627 666.544 189.627 Q669.414 189.627 671.475 190.761 Q673.535 191.895 674.854 194.187 L674.854 190.252 L679.113 190.252 L679.113 212.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M708.072 195.229 Q709.669 192.358 711.891 190.993 Q714.113 189.627 717.122 189.627 Q721.173 189.627 723.372 192.474 Q725.571 195.298 725.571 200.529 L725.571 216.178 L721.289 216.178 L721.289 200.668 Q721.289 196.942 719.97 195.136 Q718.65 193.33 715.942 193.33 Q712.632 193.33 710.71 195.53 Q708.789 197.729 708.789 201.525 L708.789 216.178 L704.507 216.178 L704.507 200.668 Q704.507 196.918 703.187 195.136 Q701.868 193.33 699.113 193.33 Q695.849 193.33 693.928 195.553 Q692.007 197.752 692.007 201.525 L692.007 216.178 L687.724 216.178 L687.724 190.252 L692.007 190.252 L692.007 194.28 Q693.465 191.895 695.502 190.761 Q697.539 189.627 700.34 189.627 Q703.164 189.627 705.132 191.062 Q707.122 192.497 708.072 195.229 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M745.849 203.145 Q740.687 203.145 738.696 204.326 Q736.706 205.506 736.706 208.354 Q736.706 210.622 738.187 211.965 Q739.692 213.284 742.261 213.284 Q745.803 213.284 747.932 210.784 Q750.085 208.261 750.085 204.094 L750.085 203.145 L745.849 203.145 M754.344 201.386 L754.344 216.178 L750.085 216.178 L750.085 212.242 Q748.627 214.603 746.451 215.738 Q744.275 216.849 741.127 216.849 Q737.145 216.849 734.784 214.627 Q732.446 212.381 732.446 208.631 Q732.446 204.256 735.363 202.034 Q738.303 199.812 744.113 199.812 L750.085 199.812 L750.085 199.395 Q750.085 196.455 748.141 194.858 Q746.219 193.238 742.724 193.238 Q740.502 193.238 738.395 193.77 Q736.289 194.303 734.345 195.367 L734.345 191.432 Q736.683 190.53 738.882 190.09 Q741.081 189.627 743.164 189.627 Q748.789 189.627 751.567 192.543 Q754.344 195.46 754.344 201.386 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M782.817 212.242 L799.136 212.242 L799.136 216.178 L777.192 216.178 L777.192 212.242 Q779.854 209.488 784.437 204.858 Q789.043 200.205 790.224 198.863 Q792.469 196.34 793.349 194.604 Q794.252 192.844 794.252 191.155 Q794.252 188.4 792.307 186.664 Q790.386 184.928 787.284 184.928 Q785.085 184.928 782.631 185.692 Q780.201 186.455 777.423 188.006 L777.423 183.284 Q780.247 182.15 782.701 181.571 Q785.154 180.993 787.191 180.993 Q792.562 180.993 795.756 183.678 Q798.951 186.363 798.951 190.854 Q798.951 192.983 798.141 194.905 Q797.353 196.803 795.247 199.395 Q794.668 200.067 791.566 203.284 Q788.465 206.479 782.817 212.242 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip680)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,250.738 400.616,250.738 "/>
<path clip-path="url(#clip680)" d="M445.365 247.069 Q446.962 244.198 449.184 242.833 Q451.406 241.467 454.415 241.467 Q458.466 241.467 460.665 244.314 Q462.865 247.138 462.865 252.369 L462.865 268.018 L458.582 268.018 L458.582 252.508 Q458.582 248.782 457.263 246.976 Q455.943 245.17 453.235 245.17 Q449.925 245.17 448.003 247.37 Q446.082 249.569 446.082 253.365 L446.082 268.018 L441.8 268.018 L441.8 252.508 Q441.8 248.758 440.48 246.976 Q439.161 245.17 436.406 245.17 Q433.142 245.17 431.221 247.393 Q429.3 249.592 429.3 253.365 L429.3 268.018 L425.018 268.018 L425.018 242.092 L429.3 242.092 L429.3 246.12 Q430.758 243.735 432.795 242.601 Q434.832 241.467 437.633 241.467 Q440.457 241.467 442.425 242.902 Q444.416 244.337 445.365 247.069 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M493.536 253.99 L493.536 256.073 L473.952 256.073 Q474.23 260.471 476.591 262.786 Q478.976 265.078 483.212 265.078 Q485.665 265.078 487.957 264.476 Q490.272 263.874 492.54 262.67 L492.54 266.698 Q490.249 267.67 487.841 268.18 Q485.434 268.689 482.957 268.689 Q476.753 268.689 473.119 265.078 Q469.508 261.467 469.508 255.309 Q469.508 248.944 472.934 245.217 Q476.383 241.467 482.216 241.467 Q487.448 241.467 490.48 244.846 Q493.536 248.203 493.536 253.99 M489.276 252.74 Q489.23 249.245 487.309 247.161 Q485.411 245.078 482.263 245.078 Q478.698 245.078 476.545 247.092 Q474.415 249.106 474.091 252.763 L489.276 252.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M512.309 254.985 Q507.147 254.985 505.156 256.166 Q503.165 257.346 503.165 260.194 Q503.165 262.462 504.647 263.805 Q506.151 265.124 508.721 265.124 Q512.262 265.124 514.392 262.624 Q516.545 260.101 516.545 255.934 L516.545 254.985 L512.309 254.985 M520.804 253.226 L520.804 268.018 L516.545 268.018 L516.545 264.082 Q515.086 266.443 512.911 267.578 Q510.735 268.689 507.587 268.689 Q503.605 268.689 501.244 266.467 Q498.906 264.221 498.906 260.471 Q498.906 256.096 501.823 253.874 Q504.762 251.652 510.573 251.652 L516.545 251.652 L516.545 251.235 Q516.545 248.295 514.6 246.698 Q512.679 245.078 509.184 245.078 Q506.962 245.078 504.855 245.61 Q502.749 246.143 500.804 247.207 L500.804 243.272 Q503.142 242.37 505.341 241.93 Q507.54 241.467 509.624 241.467 Q515.249 241.467 518.026 244.383 Q520.804 247.3 520.804 253.226 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M551.128 252.369 L551.128 268.018 L546.869 268.018 L546.869 252.508 Q546.869 248.828 545.434 246.999 Q543.998 245.17 541.128 245.17 Q537.679 245.17 535.688 247.37 Q533.697 249.569 533.697 253.365 L533.697 268.018 L529.415 268.018 L529.415 242.092 L533.697 242.092 L533.697 246.12 Q535.225 243.782 537.285 242.624 Q539.369 241.467 542.077 241.467 Q546.545 241.467 548.836 244.245 Q551.128 246.999 551.128 252.369 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M589.461 249.383 Q592.818 250.101 594.693 252.369 Q596.591 254.638 596.591 257.971 Q596.591 263.087 593.072 265.888 Q589.554 268.689 583.072 268.689 Q580.896 268.689 578.581 268.249 Q576.29 267.832 573.836 266.976 L573.836 262.462 Q575.781 263.596 578.095 264.175 Q580.41 264.754 582.933 264.754 Q587.331 264.754 589.623 263.018 Q591.938 261.281 591.938 257.971 Q591.938 254.916 589.785 253.203 Q587.656 251.467 583.836 251.467 L579.808 251.467 L579.808 247.624 L584.021 247.624 Q587.47 247.624 589.299 246.258 Q591.128 244.87 591.128 242.277 Q591.128 239.615 589.23 238.203 Q587.355 236.768 583.836 236.768 Q581.915 236.768 579.716 237.184 Q577.517 237.601 574.878 238.481 L574.878 234.314 Q577.54 233.573 579.855 233.203 Q582.193 232.833 584.253 232.833 Q589.577 232.833 592.679 235.263 Q595.78 237.67 595.78 241.791 Q595.78 244.661 594.137 246.652 Q592.493 248.62 589.461 249.383 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M636.452 242.856 L636.452 246.883 Q634.646 245.957 632.702 245.495 Q630.757 245.032 628.674 245.032 Q625.503 245.032 623.905 246.004 Q622.331 246.976 622.331 248.92 Q622.331 250.402 623.466 251.258 Q624.6 252.092 628.026 252.856 L629.484 253.18 Q634.021 254.152 635.919 255.934 Q637.84 257.694 637.84 260.865 Q637.84 264.476 634.97 266.582 Q632.123 268.689 627.123 268.689 Q625.04 268.689 622.771 268.272 Q620.526 267.879 618.026 267.068 L618.026 262.67 Q620.387 263.897 622.678 264.522 Q624.97 265.124 627.215 265.124 Q630.225 265.124 631.845 264.106 Q633.465 263.064 633.465 261.189 Q633.465 259.453 632.285 258.527 Q631.128 257.601 627.169 256.744 L625.688 256.397 Q621.729 255.564 619.97 253.851 Q618.211 252.115 618.211 249.106 Q618.211 245.448 620.803 243.458 Q623.396 241.467 628.165 241.467 Q630.526 241.467 632.609 241.814 Q634.692 242.161 636.452 242.856 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M644.623 242.092 L648.882 242.092 L648.882 268.018 L644.623 268.018 L644.623 242.092 M644.623 231.999 L648.882 231.999 L648.882 237.393 L644.623 237.393 L644.623 231.999 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M674.854 254.754 Q674.854 250.124 672.933 247.578 Q671.035 245.032 667.586 245.032 Q664.16 245.032 662.238 247.578 Q660.34 250.124 660.34 254.754 Q660.34 259.36 662.238 261.906 Q664.16 264.453 667.586 264.453 Q671.035 264.453 672.933 261.906 Q674.854 259.36 674.854 254.754 M679.113 264.8 Q679.113 271.42 676.174 274.638 Q673.234 277.879 667.169 277.879 Q664.924 277.879 662.933 277.531 Q660.942 277.207 659.067 276.513 L659.067 272.369 Q660.942 273.388 662.771 273.874 Q664.6 274.36 666.498 274.36 Q670.687 274.36 672.771 272.161 Q674.854 269.985 674.854 265.564 L674.854 263.457 Q673.535 265.749 671.475 266.883 Q669.414 268.018 666.544 268.018 Q661.775 268.018 658.859 264.383 Q655.942 260.749 655.942 254.754 Q655.942 248.735 658.859 245.101 Q661.775 241.467 666.544 241.467 Q669.414 241.467 671.475 242.601 Q673.535 243.735 674.854 246.027 L674.854 242.092 L679.113 242.092 L679.113 264.8 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M708.072 247.069 Q709.669 244.198 711.891 242.833 Q714.113 241.467 717.122 241.467 Q721.173 241.467 723.372 244.314 Q725.571 247.138 725.571 252.369 L725.571 268.018 L721.289 268.018 L721.289 252.508 Q721.289 248.782 719.97 246.976 Q718.65 245.17 715.942 245.17 Q712.632 245.17 710.71 247.37 Q708.789 249.569 708.789 253.365 L708.789 268.018 L704.507 268.018 L704.507 252.508 Q704.507 248.758 703.187 246.976 Q701.868 245.17 699.113 245.17 Q695.849 245.17 693.928 247.393 Q692.007 249.592 692.007 253.365 L692.007 268.018 L687.724 268.018 L687.724 242.092 L692.007 242.092 L692.007 246.12 Q693.465 243.735 695.502 242.601 Q697.539 241.467 700.34 241.467 Q703.164 241.467 705.132 242.902 Q707.122 244.337 708.072 247.069 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M745.849 254.985 Q740.687 254.985 738.696 256.166 Q736.706 257.346 736.706 260.194 Q736.706 262.462 738.187 263.805 Q739.692 265.124 742.261 265.124 Q745.803 265.124 747.932 262.624 Q750.085 260.101 750.085 255.934 L750.085 254.985 L745.849 254.985 M754.344 253.226 L754.344 268.018 L750.085 268.018 L750.085 264.082 Q748.627 266.443 746.451 267.578 Q744.275 268.689 741.127 268.689 Q737.145 268.689 734.784 266.467 Q732.446 264.221 732.446 260.471 Q732.446 256.096 735.363 253.874 Q738.303 251.652 744.113 251.652 L750.085 251.652 L750.085 251.235 Q750.085 248.295 748.141 246.698 Q746.219 245.078 742.724 245.078 Q740.502 245.078 738.395 245.61 Q736.289 246.143 734.345 247.207 L734.345 243.272 Q736.683 242.37 738.882 241.93 Q741.081 241.467 743.164 241.467 Q748.789 241.467 751.567 244.383 Q754.344 247.3 754.344 253.226 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip680)" d="M779.599 264.082 L787.238 264.082 L787.238 237.717 L778.928 239.383 L778.928 235.124 L787.191 233.458 L791.867 233.458 L791.867 264.082 L799.506 264.082 L799.506 268.018 L779.599 268.018 L779.599 264.082 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><p>Note that changing the mean corresponds to a shift along the <span>$x$</span>-axis, and increasing the variance spreads the distribution out, lowering its peak.</p><p>One way to motivate linear regression with squared loss is to assume that observations arise from noisy measurements, where the noise <span>$\epsilon$</span> follows the normal distribution  <span>$\mathcal{N}(0, \sigma^2)$</span>:</p><p class="math-container">\[y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \textrm{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).\]</p><p>Thus, we can now write out the <em>likelihood</em> of seeing a particular <span>$y$</span> for a given <span>$\mathbf{x}$</span> via</p><p class="math-container">\[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).\]</p><p>As such, the likelihood factorizes. According to <em>the principle of maximum likelihood</em>, the best values of parameters <span>$\mathbf{w}$</span> and <span>$b$</span> are those that maximize the <em>likelihood</em> of the entire dataset:</p><p class="math-container">\[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)}).\]</p><p>The equality follows since all pairs <span>$(\mathbf{x}^{(i)}, y^{(i)})$</span> were drawn independently of each other. Estimators chosen according to the principle of maximum likelihood are called <em>maximum likelihood estimators</em>. While, maximizing the product of many exponential functions, might look difficult, we can simplify things significantly, without changing the objective, by maximizing the logarithm of the likelihood instead. For historical reasons, optimizations are more often expressed as minimization rather than maximization. So, without changing anything, we can <em>minimize</em> the <em>negative log-likelihood</em>, which we can express as follows:</p><p class="math-container">\[-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.\]</p><p>If we assume that <span>$\sigma$</span> is fixed, we can ignore the first term, because it does not depend on <span>$\mathbf{w}$</span> or <span>$b$</span>. The second term is identical to the squared error loss introduced earlier, except for the multiplicative constant <span>$\frac{1}{\sigma^2}$</span>. Fortunately, the solution does not depend on <span>$\sigma$</span> either. It follows that minimizing the mean squared error is equivalent to the maximum likelihood estimation of a linear model under the assumption of additive Gaussian noise.</p><h2 id="Linear-Regression-as-a-Neural-Network"><a class="docs-heading-anchor" href="#Linear-Regression-as-a-Neural-Network">Linear Regression as a Neural Network</a><a id="Linear-Regression-as-a-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Regression-as-a-Neural-Network" title="Permalink"></a></h2><p>While linear models are not sufficiently rich to express the many complicated networks that we will introduce in this book, (artificial) neural networks are rich enough to subsume linear models as networks in which every feature is represented by an input neuron, all of which are connected directly to the output.</p><p>:numref:<code>fig_single_neuron</code> depicts linear regression as a neural network. The diagram highlights the connectivity pattern, such as how each input is connected to the output, but not the specific values taken by the weights or biases.</p><p><img src="../../img/singleneuron.svg" alt="Linear regression is a single-layer neural network."/> :label:<code>fig_single_neuron</code></p><p>The inputs are <span>$x_1, \ldots, x_d$</span>. We refer to <span>$d$</span> as the <em>number of inputs</em> or the <em>feature dimensionality</em> in the input layer. The output of the network is <span>$o_1$</span>. Because we are just trying to predict a single numerical value, we have only one output neuron. Note that the input values are all <em>given</em>. There is just a single <em>computed</em> neuron. In summary, we can think of linear regression as a single-layer fully connected neural network. We will encounter networks with far more layers in later chapters.</p><h3 id="Biology"><a class="docs-heading-anchor" href="#Biology">Biology</a><a id="Biology-1"></a><a class="docs-heading-anchor-permalink" href="#Biology" title="Permalink"></a></h3><p>Because linear regression predates computational neuroscience, it might seem anachronistic to describe linear regression in terms of neural networks. Nonetheless, they were a natural place to start when the cyberneticists and neurophysiologists Warren McCulloch and Walter Pitts began to develop models of artificial neurons. Consider the cartoonish picture of a biological neuron in :numref:<code>fig_Neuron</code>, consisting of <em>dendrites</em> (input terminals), the <em>nucleus</em> (CPU), the <em>axon</em> (output wire), and the <em>axon terminals</em> (output terminals), enabling connections to other neurons via <em>synapses</em>.</p><p><img src="../../img/neuron.svg" alt="The real neuron (source: &quot;Anatomy and Physiology&quot; by the US National Cancer Institute&#39;s Surveillance, Epidemiology and End Results (SEER) Program)."/> :label:<code>fig_Neuron</code></p><p>Information <span>$x_i$</span> arriving from other neurons (or environmental sensors) is received in the dendrites. In particular, that information is weighted by <em>synaptic weights</em> <span>$w_i$</span>, determining the effect of the inputs, e.g., activation or inhibition via the product <span>$x_i w_i$</span>. The weighted inputs arriving from multiple sources are aggregated in the nucleus as a weighted sum <span>$y = \sum_i x_i w_i + b$</span>, possibly subject to some nonlinear postprocessing via a function <span>$\sigma(y)$</span>. This information is then sent via the axon to the axon terminals, where it reaches its destination (e.g., an actuator such as a muscle) or it is fed into another neuron via its dendrites.</p><p>Certainly, the high-level idea that many such units could be combined, provided they have the correct connectivity and learning algorithm, to produce far more interesting and complex behavior than any one neuron alone could express arises from our study of real biological neural systems. At the same time, most research in deep learning today draws inspiration from a much wider source. We invoke :citet:<code>Russell.Norvig.2016</code> who pointed out that although airplanes might have been <em>inspired</em> by birds, ornithology has not been the primary driver of aeronautics innovation for some centuries. Likewise, inspiration in deep learning these days comes in equal or greater measure from mathematics, linguistics, psychology, statistics, computer science, and many other fields.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>In this section, we introduced traditional linear regression, where the parameters of a linear function are chosen to minimize squared loss on the training set. We also motivated this choice of objective both via some practical considerations and through an interpretation of linear regression as maximimum likelihood estimation under an assumption of linearity and Gaussian noise. After discussing both computational considerations and connections to statistics, we showed how such linear models could be expressed as simple neural networks where the inputs are directly wired to the output(s). While we will soon move past linear models altogether, they are sufficient to introduce most of the components that all of our models require: parametric forms, differentiable objectives, optimization via minibatch stochastic gradient descent, and ultimately, evaluation on previously unseen data.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Assume that we have some data <span>$x_1, \ldots, x_n \in \mathbb{R}$</span>. Our goal is to find a constant <span>$b$</span> such that <span>$\sum_i (x_i - b)^2$</span> is minimized.<ol><li>Find an analytic solution for the optimal value of <span>$b$</span>.</li><li>How does this problem and its solution relate to the normal distribution?</li><li>What if we change the loss from <span>$\sum_i (x_i - b)^2$</span> to <span>$\sum_i |x_i-b|$</span>? Can you find the optimal solution for <span>$b$</span>?</li></ol></li><li>Prove that the affine functions that can be expressed by <span>$\mathbf{x}^\top \mathbf{w} + b$</span> are equivalent to linear functions on <span>$(\mathbf{x}, 1)$</span>.</li><li>Assume that you want to find quadratic functions of <span>$\mathbf{x}$</span>, i.e., <span>$f(\mathbf{x}) = b + \sum_i w_i x_i + \sum_{j \leq i} w_{ij} x_{i} x_{j}$</span>. How would you formulate this in a deep network?</li><li>Recall that one of the conditions for the linear regression problem to be solvable was that the design matrix <span>$\mathbf{X}^\top \mathbf{X}$</span> has full rank.<ol><li>What happens if this is not the case?</li><li>How could you fix it? What happens if you add a small amount of coordinate-wise independent Gaussian noise to all entries of <span>$\mathbf{X}$</span>?</li><li>What is the expected value of the design matrix <span>$\mathbf{X}^\top \mathbf{X}$</span> in this case?</li><li>What happens with stochastic gradient descent when <span>$\mathbf{X}^\top \mathbf{X}$</span> does not have full rank?</li></ol></li><li>Assume that the noise model governing the additive noise <span>$\epsilon$</span> is the exponential distribution. That is, <span>$p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$</span>.<ol><li>Write out the negative log-likelihood of the data under the model <span>$-\log P(\mathbf y \mid \mathbf X)$</span>.</li><li>Can you find a closed form solution?</li><li>Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?</li></ol></li><li>Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work?</li><li>What happens if you want to use regression for realistic price estimation of houses or stock prices?<ol><li>Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have negative prices? What about fluctuations?</li><li>Why would regression to the logarithm of the price be much better, i.e., <span>$y = \log \textrm{price}$</span>?</li><li>What do you need to worry about when dealing with pennystock, i.e., stock with very low prices? Hint: can you trade at all possible prices? Why is this a bigger problem for cheap stock? For more information review the celebrated Black–Scholes model for option pricing :cite:<code>Black.Scholes.1973</code>.</li></ol></li><li>Suppose we want to use regression to estimate the <em>number</em> of apples sold in a grocery store.<ol><li>What are the problems with a Gaussian additive noise model? Hint: you are selling apples, not oil.</li><li>The <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> captures distributions over counts. It is given by <span>$p(k \mid \lambda) = \lambda^k e^{-\lambda}/k!$</span>. Here <span>$\lambda$</span> is the rate function and <span>$k$</span> is the number of events you see. Prove that <span>$\lambda$</span> is the expected value of counts <span>$k$</span>.</li><li>Design a loss function associated with the Poisson distribution.</li><li>Design a loss function for estimating <span>$\log \lambda$</span> instead.</li></ol></li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="../LNN_2/">Multiple Dispatch Design for Implementation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
