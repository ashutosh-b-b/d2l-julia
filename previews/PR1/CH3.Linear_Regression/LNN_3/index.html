<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Synthetic Regression Data · d2l Julia</title><meta name="title" content="Synthetic Regression Data · d2l Julia"/><meta property="og:title" content="Synthetic Regression Data · d2l Julia"/><meta property="twitter:title" content="Synthetic Regression Data · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../LNN_2/">Multiple Dispatch Design for Implementation</a></li><li class="is-active"><a class="tocitem" href>Synthetic Regression Data</a><ul class="internal"><li><a class="tocitem" href="#Generating-the-Dataset"><span>Generating the Dataset</span></a></li><li><a class="tocitem" href="#Reading-the-Dataset"><span>Reading the Dataset</span></a></li><li><a class="tocitem" href="#Concise-Implementation-of-the-Data-Loader"><span>Concise Implementation of the Data Loader</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../LNN_6/">Generalization</a></li><li><a class="tocitem" href="../LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Linear Neural Networks for Regression</a></li><li class="is-active"><a href>Synthetic Regression Data</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Synthetic Regression Data</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Synthetic-Regression-Data"><a class="docs-heading-anchor" href="#Synthetic-Regression-Data">Synthetic Regression Data</a><a id="Synthetic-Regression-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Synthetic-Regression-Data" title="Permalink"></a></h1><p>:label:<code>sec_synthetic-regression-data</code></p><p>Machine learning is all about extracting information from data. So you might wonder, what could we possibly learn from synthetic data? While we might not care intrinsically about the patterns  that we ourselves baked into an artificial data generating model, such datasets are nevertheless useful for didactic purposes, helping us to evaluate the properties of our learning  algorithms and to confirm that our implementations work as expected. For example, if we create data for which the correct parameters are known <em>a priori</em>, then we can check that our model can in fact recover them.</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Random, Flux</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/workspace/d2l-julia/d2lai`</code></pre><h2 id="Generating-the-Dataset"><a class="docs-heading-anchor" href="#Generating-the-Dataset">Generating the Dataset</a><a id="Generating-the-Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-the-Dataset" title="Permalink"></a></h2><p>For this example, we will work in low dimension for succinctness. The following code snippet generates 1000 examples with 2-dimensional features drawn  from a standard normal distribution. The resulting design matrix <span>$\mathbf{X}$</span> belongs to <span>$\mathbb{R}^{1000 \times 2}$</span>.  We generate each label by applying  a <em>ground truth</em> linear function,  corrupting them via additive noise <span>$\boldsymbol{\epsilon}$</span>,  drawn independently and identically for each example:</p><p class="math-container">\[\mathbf{y}= \mathbf{X} \mathbf{w} + b + \boldsymbol{\epsilon}.\]</p><p>For convenience we assume that <span>$\boldsymbol{\epsilon}$</span> is drawn  from a normal distribution with mean <span>$\mu= 0$</span>  and standard deviation <span>$\sigma = 0.01$</span>.</p><pre><code class="language-julia hljs">struct SyntheticRegressionData &lt;: d2lai.AbstractData
    X::AbstractArray 
    y::AbstractArray 
    args::NamedTuple
    function SyntheticRegressionData(w, b, noise = 0.01, num_train = 1000, num_val = 1000, batchsize = 32)
        args = (noise = noise, num_train = num_train, num_val = num_val, batchsize = batchsize)
        n = args.num_train + args.num_val 
        X = randn(length(w), n)
        y = w*X .+ b .+ randn(1, n).*noise
        new(X, y, args)
    end
end

</code></pre><p>Below, we set the true parameters to <span>$\mathbf{w} = [2, -3.4]^\top$</span> and <span>$b = 4.2$</span>. Later, we can check our estimated parameters against these <em>ground truth</em> values.</p><pre><code class="language-julia hljs">data = SyntheticRegressionData([2 -3.4], 4.3)
</code></pre><pre><code class="nohighlight hljs">Data object of type SyntheticRegressionData</code></pre><p>Each row in <code>features</code> consists of a vector in <span>$\mathbb{R}^2$</span> and each row in <code>labels</code> is a scalar. Let&#39;s have a look at the first entry.</p><pre><code class="language-julia hljs">println(&quot;features: $(data.X[:, 1]), labels: $(data.y[1])&quot;)</code></pre><pre><code class="nohighlight hljs">features: [0.2633521971652844, -1.110529094600684], labels: 8.600873573059339</code></pre><h2 id="Reading-the-Dataset"><a class="docs-heading-anchor" href="#Reading-the-Dataset">Reading the Dataset</a><a id="Reading-the-Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Reading-the-Dataset" title="Permalink"></a></h2><p>Training machine learning models often requires multiple passes over a dataset,  grabbing one minibatch of examples at a time.  This data is then used to update the model.  To illustrate how this works, we  implement the <code>get_dataloader</code> method, It takes a batch size, a matrix of features, and a vector of labels, and generates minibatches of size <code>batch_size</code>. As such, each minibatch consists of a tuple of features and labels.  Note that we need to be mindful of whether we&#39;re in training or validation mode:  in the former, we will want to read the data in random order,  whereas for the latter, being able to read data in a pre-defined order  may be important for debugging purposes.</p><pre><code class="language-julia hljs">function d2lai.get_dataloader(data::d2lai.AbstractData; train = true)
        indices = train ? Random.shuffle(1:data.args.num_train) : (data.args.num_train+1):(data.args.num_train+data.args.num_val)
        partitioned_indices = collect(Iterators.partition(indices, data.args.batchsize))
        data = map(partitioned_indices) do idx 
            data.X[:, idx], data.y[:, idx]
        end
        data
end</code></pre><p>To build some intuition, let&#39;s inspect the first minibatch of data. Each minibatch of features provides us with both its size and the dimensionality of input features. Likewise, our minibatch of labels will have a matching shape given by <code>batch_size</code>.</p><pre><code class="language-julia hljs">d2lai.train_dataloader(data)[1][1]</code></pre><pre><code class="nohighlight hljs">2×32 Matrix{Float64}:
 -0.909321  -0.500413  -0.790988  …  0.313599  -0.0526522  -0.169194
 -2.33647   -0.913567   0.516172     1.45126   -0.156959   -2.91507</code></pre><p>While seemingly innocuous, the invocation  of <code>d2lai.train_dataloader</code>  illustrates the power of multiple dispatch.</p><p>Throughout the iteration we obtain distinct minibatches until the entire dataset has been exhausted (try this). While the iteration implemented above is good for didactic purposes, it is inefficient in ways that might get us into trouble with real problems. For example, it requires that we load all the data in memory and that we perform lots of random memory access. The built-in iterators implemented in a deep learning framework are considerably more efficient and they can deal with sources such as data stored in files,  data received via a stream,  and data generated or processed on the fly.  Next let&#39;s try to implement the same method using built-in iterators.</p><h2 id="Concise-Implementation-of-the-Data-Loader"><a class="docs-heading-anchor" href="#Concise-Implementation-of-the-Data-Loader">Concise Implementation of the Data Loader</a><a id="Concise-Implementation-of-the-Data-Loader-1"></a><a class="docs-heading-anchor-permalink" href="#Concise-Implementation-of-the-Data-Loader" title="Permalink"></a></h2><p>Rather than writing our own iterator, we can call the existing API in a framework to load data. As before, we need a dataset with features <code>X</code> and labels <code>y</code>.  Beyond that, we set <code>batchsize</code> in the built-in data loader  and let it take care of shuffling examples  efficiently.</p><pre><code class="language-julia hljs">function d2lai.get_dataloader(data::SyntheticRegressionData; train = true)
    indices = train ? Random.shuffle(1:data.args.num_train) : (data.args.num_train+1):(data.args.num_train+data.args.num_val)
    Flux.DataLoader((data.X[:, indices], data.y[indices]); batchsize = data.args.batchsize, )

end</code></pre><p>The new data loader behaves just like the previous one, except that it is more efficient and has some added functionality.</p><pre><code class="language-julia hljs">first(d2lai.train_dataloader(data))[1]</code></pre><pre><code class="nohighlight hljs">2×32 Matrix{Float64}:
 -0.71515    0.407084  -1.88941  …  0.0295728  0.346882  -0.346259
  0.372792  -0.688122   1.75975     0.255949   0.702533  -0.0318633</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>Data loaders are a convenient way of abstracting out  the process of loading and manipulating data.  This way the same machine learning <em>algorithm</em>  is capable of processing many different types and sources of data  without the need for modification.  One of the nice things about data loaders  is that they can be composed.  For instance, we might be loading images  and then have a postprocessing filter  that crops them or modifies them in other ways.  As such, data loaders can be used  to describe an entire data processing pipeline. </p><p>As for the model itself, the two-dimensional linear model  is about the simplest we might encounter.  It lets us test out the accuracy of regression models  without worrying about having insufficient amounts of data  or an underdetermined system of equations.  We will put this to good use in the next section.  </p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>What will happen if the number of examples cannot be divided by the batch size. How would you change this behavior by specifying a different argument by using the framework&#39;s API?</li><li>Suppose that we want to generate a huge dataset, where both the size of the parameter vector <code>w</code> and the number of examples <code>num_examples</code> are large.<ol><li>What happens if we cannot hold all data in memory?</li><li>How would you shuffle the data if it is held on disk? Your task is to design an <em>efficient</em> algorithm that does not require too many random reads or writes. Hint: <a href="https://en.wikipedia.org/wiki/Pseudorandom_permutation">pseudorandom permutation generators</a> allow you to design a reshuffle without the need to store the permutation table explicitly :cite:<code>Naor.Reingold.1999</code>. </li></ol></li><li>Implement a data generator that produces new data on the fly, every time the iterator is called. </li><li>How would you design a random data generator that generates <em>the same</em> data each time it is called?</li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../LNN_2/">« Multiple Dispatch Design for Implementation</a><a class="docs-footer-nextpage" href="../LNN_4/">Linear Regression Implementation from Scratch »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
