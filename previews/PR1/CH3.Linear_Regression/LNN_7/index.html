<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Weight Decay · d2l Julia</title><meta name="title" content="Weight Decay · d2l Julia"/><meta property="og:title" content="Weight Decay · d2l Julia"/><meta property="twitter:title" content="Weight Decay · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../LNN_6/">Generalization</a></li><li class="is-active"><a class="tocitem" href>Weight Decay</a><ul class="internal"><li><a class="tocitem" href="#Norms-and-Weight-Decay"><span>Norms and Weight Decay</span></a></li><li><a class="tocitem" href="#High-Dimensional-Linear-Regression"><span>High-Dimensional Linear Regression</span></a></li><li><a class="tocitem" href="#Implementation-from-Scratch"><span>Implementation from Scratch</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Linear Neural Networks for Regression</a></li><li class="is-active"><a href>Weight Decay</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Weight Decay</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Weight-Decay"><a class="docs-heading-anchor" href="#Weight-Decay">Weight Decay</a><a id="Weight-Decay-1"></a><a class="docs-heading-anchor-permalink" href="#Weight-Decay" title="Permalink"></a></h1><p>:label:<code>sec_weight_decay</code></p><p>Now that we have characterized the problem of overfitting, we can introduce our first <em>regularization</em> technique. Recall that we can always mitigate overfitting by collecting more training data. However, that can be costly, time consuming, or entirely out of our control, making it impossible in the short run. For now, we can assume that we already have as much high-quality data as our resources permit and focus the tools at our disposal when the dataset is taken as a given.</p><p>Recall that in our polynomial regression example (:numref:<code>subsec_polynomial-curve-fitting</code>) we could limit our model&#39;s capacity by tweaking the degree of the fitted polynomial. Indeed, limiting the number of features is a popular technique for mitigating overfitting. However, simply tossing aside features can be too blunt an instrument. Sticking with the polynomial regression example, consider what might happen with high-dimensional input. The natural extensions of polynomials to multivariate data are called <em>monomials</em>, which are simply products of powers of variables. The degree of a monomial is the sum of the powers. For example, <span>$x_1^2 x_2$</span>, and <span>$x_3 x_5^2$</span> are both monomials of degree 3.</p><p>Note that the number of terms with degree <span>$d$</span> blows up rapidly as <span>$d$</span> grows larger. Given <span>$k$</span> variables, the number of monomials of degree <span>$d$</span> is <span>${k - 1 + d} \choose {k - 1}$</span>. Even small changes in degree, say from <span>$2$</span> to <span>$3$</span>, dramatically increase the complexity of our model. Thus we often need a more fine-grained tool for adjusting function complexity.</p><pre><code class="language-julia hljs">using Pkg;
Pkg.activate(&quot;../../d2lai&quot;)
using d2lai, Flux</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/workspace/d2l-julia/d2lai`</code></pre><h2 id="Norms-and-Weight-Decay"><a class="docs-heading-anchor" href="#Norms-and-Weight-Decay">Norms and Weight Decay</a><a id="Norms-and-Weight-Decay-1"></a><a class="docs-heading-anchor-permalink" href="#Norms-and-Weight-Decay" title="Permalink"></a></h2><p>(<strong>Rather than directly manipulating the number of parameters, <em>weight decay</em>, operates by restricting the values  that the parameters can take.</strong>) More commonly called <span>$\ell_2$</span> regularization outside of deep learning circles when optimized by minibatch stochastic gradient descent, weight decay might be the most widely used technique for regularizing parametric machine learning models. The technique is motivated by the basic intuition that among all functions <span>$f$</span>, the function <span>$f = 0$</span> (assigning the value <span>$0$</span> to all inputs) is in some sense the <em>simplest</em>, and that we can measure the complexity of a function by the distance of its parameters from zero. But how precisely should we measure the distance between a function and zero? There is no single right answer. In fact, entire branches of mathematics, including parts of functional analysis and the theory of Banach spaces, are devoted to addressing such issues.</p><p>One simple interpretation might be to measure the complexity of a linear function <span>$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$</span> by some norm of its weight vector, e.g., <span>$\| \mathbf{w} \|^2$</span>. Recall that we introduced the <span>$\ell_2$</span> norm and <span>$\ell_1$</span> norm, which are special cases of the more general <span>$\ell_p$</span> norm, in :numref:<code>subsec_lin-algebra-norms</code>. The most common method for ensuring a small weight vector is to add its norm as a penalty term to the problem of minimizing the loss. Thus we replace our original objective, <em>minimizing the prediction loss on the training labels</em>, with new objective, <em>minimizing the sum of the prediction loss and the penalty term</em>. Now, if our weight vector grows too large, our learning algorithm might focus on minimizing the weight norm <span>$\| \mathbf{w} \|^2$</span> rather than minimizing the training error. That is exactly what we want. To illustrate things in code, we revive our previous example from :numref:<code>sec_linear_regression</code> for linear regression. There, our loss was given by</p><p class="math-container">\[L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]</p><p>Recall that <span>$\mathbf{x}^{(i)}$</span> are the features, <span>$y^{(i)}$</span> is the label for any data example <span>$i$</span>, and <span>$(\mathbf{w}, b)$</span> are the weight and bias parameters, respectively. To penalize the size of the weight vector, we must somehow add <span>$\| \mathbf{w} \|^2$</span> to the loss function, but how should the model trade off the standard loss for this new additive penalty? In practice, we characterize this trade-off via the <em>regularization constant</em> <span>$\lambda$</span>, a nonnegative hyperparameter that we fit using validation data:</p><p class="math-container">\[L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2.\]</p><p>For <span>$\lambda = 0$</span>, we recover our original loss function. For <span>$\lambda &gt; 0$</span>, we restrict the size of <span>$\| \mathbf{w} \|$</span>. We divide by <span>$2$</span> by convention: when we take the derivative of a quadratic function, the <span>$2$</span> and <span>$1/2$</span> cancel out, ensuring that the expression for the update looks nice and simple. The astute reader might wonder why we work with the squared norm and not the standard norm (i.e., the Euclidean distance). We do this for computational convenience. By squaring the <span>$\ell_2$</span> norm, we remove the square root, leaving the sum of squares of each component of the weight vector. This makes the derivative of the penalty easy to compute:  the sum of derivatives equals the derivative of the sum.</p><p>Moreover, you might ask why we work with the <span>$\ell_2$</span> norm in the first place and not, say, the <span>$\ell_1$</span> norm. In fact, other choices are valid and popular throughout statistics. While <span>$\ell_2$</span>-regularized linear models constitute the classic <em>ridge regression</em> algorithm, <span>$\ell_1$</span>-regularized linear regression is a similarly fundamental method in statistics,  popularly known as <em>lasso regression</em>. One reason to work with the <span>$\ell_2$</span> norm is that it places an outsize penalty on large components of the weight vector. This biases our learning algorithm towards models that distribute weight evenly across a larger number of features. In practice, this might make them more robust to measurement error in a single variable. By contrast, <span>$\ell_1$</span> penalties lead to models that concentrate weights on a small set of features by clearing the other weights to zero. This gives us an effective method for <em>feature selection</em>, which may be desirable for other reasons. For example, if our model only relies on a few features, then we may not need to collect, store, or transmit data for the other (dropped) features. </p><p>Using the same notation in :eqref:<code>eq_linreg_batch_update</code>, minibatch stochastic gradient descent updates for <span>$\ell_2$</span>-regularized regression as follows:</p><p class="math-container">\[\begin{aligned}
\mathbf{w} &amp; \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}\]</p><p>As before, we update <span>$\mathbf{w}$</span> based on the amount by which our estimate differs from the observation. However, we also shrink the size of <span>$\mathbf{w}$</span> towards zero. That is why the method is sometimes called &quot;weight decay&quot;: given the penalty term alone, our optimization algorithm <em>decays</em> the weight at each step of training. In contrast to feature selection, weight decay offers us a mechanism for continuously adjusting the complexity of a function. Smaller values of <span>$\lambda$</span> correspond to less constrained <span>$\mathbf{w}$</span>, whereas larger values of <span>$\lambda$</span> constrain <span>$\mathbf{w}$</span> more considerably. Whether we include a corresponding bias penalty <span>$b^2$</span>  can vary across implementations,  and may vary across layers of a neural network. Often, we do not regularize the bias term. Besides, although <span>$\ell_2$</span> regularization may not be equivalent to weight decay for other optimization algorithms, the idea of regularization through shrinking the size of weights still holds true.</p><h2 id="High-Dimensional-Linear-Regression"><a class="docs-heading-anchor" href="#High-Dimensional-Linear-Regression">High-Dimensional Linear Regression</a><a id="High-Dimensional-Linear-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#High-Dimensional-Linear-Regression" title="Permalink"></a></h2><p>We can illustrate the benefits of weight decay  through a simple synthetic example.</p><p>First, we [<strong>generate some data as before</strong>]:</p><p>(<strong><span>$y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \textrm{ where } \epsilon \sim \mathcal{N}(0, 0.01^2).$</span></strong>)</p><p>In this synthetic dataset, our label is given  by an underlying linear function of our inputs, corrupted by Gaussian noise  with zero mean and standard deviation 0.01. For illustrative purposes,  we can make the effects of overfitting pronounced, by increasing the dimensionality of our problem to <span>$d = 200$</span> and working with a small training set with only 20 examples.</p><pre><code class="language-julia hljs">struct PolynomialData{XT, YT, A} &lt;: d2lai.AbstractData 
    X::XT 
    y::YT
    args::A 
    function PolynomialData(num_train, num_val, num_inputs, batch_size)
        args = (num_train = num_train, num_val = num_val, num_inputs = num_inputs, batchsize = batch_size)
        n = num_train + num_val 
        X = randn(num_inputs, n)
        b = zeros(1)
        y = 0.01*ones(1, num_inputs)*X .+ b .+ 0.01*randn(1, n)
        new{typeof(X), typeof(y), typeof(args)}(X, y, args)
    end
end
function d2lai.get_dataloader(data::PolynomialData; train = true)
    if train 
        return Flux.DataLoader((data.X[:, 1:data.args.num_train], data.y[:, 1:data.args.num_train]), batchsize = data.args.batchsize, shuffle=true)
    else
        return Flux.DataLoader((data.X[:, data.args.num_train + 1 : end], data.y[:, data.args.num_train + 1 : end]), batchsize = data.args.batchsize)
    end
end
</code></pre><h2 id="Implementation-from-Scratch"><a class="docs-heading-anchor" href="#Implementation-from-Scratch">Implementation from Scratch</a><a id="Implementation-from-Scratch-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-from-Scratch" title="Permalink"></a></h2><p>Now, let&#39;s try implementing weight decay from scratch. Since minibatch stochastic gradient descent is our optimizer, we just need to add the squared <span>$\ell_2$</span> penalty to the original loss function.</p><h3 id="Defining-\\ell_2-Norm-Penalty"><a class="docs-heading-anchor" href="#Defining-\\ell_2-Norm-Penalty">Defining <span>$\ell_2$</span> Norm Penalty</a><a id="Defining-\\ell_2-Norm-Penalty-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-\\ell_2-Norm-Penalty" title="Permalink"></a></h3><p>Perhaps the most convenient way of implementing this penalty is to square all terms in place and sum them.</p><pre><code class="language-julia hljs">function l2_penalty(w)
    sum(w.^2)
end</code></pre><pre><code class="nohighlight hljs">l2_penalty (generic function with 1 method)</code></pre><h3 id="Defining-the-Model"><a class="docs-heading-anchor" href="#Defining-the-Model">Defining the Model</a><a id="Defining-the-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-Model" title="Permalink"></a></h3><pre><code class="language-julia hljs">struct WeightDecayScratch{N, A} &lt;: AbstractModel 
    net::N
    args::A
end

function WeightDecayScratch(net, lambda::Real = 0.01)
    args = (lambda = lambda, )
    return WeightDecayScratch(net, args)
end
d2lai.forward(m::WeightDecayScratch, x) = m.net(x)

function d2lai.loss(m::WeightDecayScratch, y_pred, y)
    mse_loss = Flux.Losses.mse(y_pred, y)
    reg_loss = m.args.lambda*l2_penalty(m.net.weight)
    return mse_loss + reg_loss
end

</code></pre><p>The following code fits our model on the training set with 20 examples and evaluates it on the validation set with 100 examples.</p><pre><code class="language-julia hljs">function train_scratch(lambda)
    model = WeightDecayScratch(Dense(200 =&gt; 1), lambda)
    opt = Descent(0.01)
    data = PolynomialData(20, 100, 200, 5)
    trainer = Trainer(model, data, opt; max_epochs = 10)
    d2lai.fit(trainer)
end</code></pre><pre><code class="nohighlight hljs">train_scratch (generic function with 1 method)</code></pre><h3 id="Training-without-Regularization"><a class="docs-heading-anchor" href="#Training-without-Regularization">Training without Regularization</a><a id="Training-without-Regularization-1"></a><a class="docs-heading-anchor-permalink" href="#Training-without-Regularization" title="Permalink"></a></h3><p>We now run this code with <code>lambda = 0</code>, disabling weight decay. Note that we overfit badly, decreasing the training error but not the validation error–-a textbook case of overfitting.</p><pre><code class="language-julia hljs">train_scratch(0.)</code></pre><pre><code class="nohighlight hljs">┌ Warning: Layer with Float32 parameters got Float64 input.
│   The input will be converted, but any earlier layers may be very slow.
│   layer = Dense(200 =&gt; 1)     # 201 parameters
│   summary(x) = &quot;200×5 Matrix{Float64}&quot;
└ @ Flux ~/.julia/packages/Flux/3711C/src/layers/stateless.jl:60</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 0.17010443647907264, Val Loss: 0.3104697962097079
    [ Info: Train Loss: 0.00812942767681148, Val Loss: 0.3761586611835473
    [ Info: Train Loss: 0.00300404867619369, Val Loss: 0.36316384810156355
    [ Info: Train Loss: 9.086800392004014e-5, Val Loss: 0.36251333818878384
    [ Info: Train Loss: 3.768173429835639e-5, Val Loss: 0.3636249171162744
    [ Info: Train Loss: 9.808228607727102e-6, Val Loss: 0.36464670373184327
    [ Info: Train Loss: 6.473687428255859e-6, Val Loss: 0.3645929430537423
    [ Info: Train Loss: 1.6601482946387506e-7, Val Loss: 0.3646625322339013
    [ Info: Train Loss: 7.307524895225433e-7, Val Loss: 0.36467572724532893
    [ Info: Train Loss: 5.9002328313041295e-8, Val Loss: 0.3647579105026305</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip030">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip030)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip031">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip030)" d="M188.292 1423.18 L2352.76 1423.18 L2352.76 47.2441 L188.292 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip032">
    <rect x="188" y="47" width="2165" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="476.434,1423.18 476.434,47.2441 "/>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="930.2,1423.18 930.2,47.2441 "/>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1383.97,1423.18 1383.97,47.2441 "/>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1837.73,1423.18 1837.73,47.2441 "/>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2291.5,1423.18 2291.5,47.2441 "/>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="188.292,1010.37 2352.76,1010.37 "/>
<polyline clip-path="url(#clip032)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="188.292,130.925 2352.76,130.925 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.292,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="476.434,1423.18 476.434,1404.28 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="930.2,1423.18 930.2,1404.28 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1383.97,1423.18 1383.97,1404.28 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1837.73,1423.18 1837.73,1404.28 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2291.5,1423.18 2291.5,1404.28 "/>
<path clip-path="url(#clip030)" d="M471.086 1481.64 L487.406 1481.64 L487.406 1485.58 L465.461 1485.58 L465.461 1481.64 Q468.123 1478.89 472.707 1474.26 Q477.313 1469.61 478.494 1468.27 Q480.739 1465.74 481.619 1464.01 Q482.522 1462.25 482.522 1460.56 Q482.522 1457.8 480.577 1456.07 Q478.656 1454.33 475.554 1454.33 Q473.355 1454.33 470.901 1455.09 Q468.471 1455.86 465.693 1457.41 L465.693 1452.69 Q468.517 1451.55 470.971 1450.97 Q473.424 1450.39 475.461 1450.39 Q480.832 1450.39 484.026 1453.08 Q487.221 1455.77 487.221 1460.26 Q487.221 1462.39 486.41 1464.31 Q485.623 1466.2 483.517 1468.8 Q482.938 1469.47 479.836 1472.69 Q476.735 1475.88 471.086 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M933.209 1455.09 L921.403 1473.54 L933.209 1473.54 L933.209 1455.09 M931.982 1451.02 L937.862 1451.02 L937.862 1473.54 L942.792 1473.54 L942.792 1477.43 L937.862 1477.43 L937.862 1485.58 L933.209 1485.58 L933.209 1477.43 L917.607 1477.43 L917.607 1472.92 L931.982 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1384.37 1466.44 Q1381.22 1466.44 1379.37 1468.59 Q1377.54 1470.74 1377.54 1474.49 Q1377.54 1478.22 1379.37 1480.39 Q1381.22 1482.55 1384.37 1482.55 Q1387.52 1482.55 1389.35 1480.39 Q1391.2 1478.22 1391.2 1474.49 Q1391.2 1470.74 1389.35 1468.59 Q1387.52 1466.44 1384.37 1466.44 M1393.65 1451.78 L1393.65 1456.04 Q1391.89 1455.21 1390.09 1454.77 Q1388.31 1454.33 1386.55 1454.33 Q1381.92 1454.33 1379.46 1457.45 Q1377.03 1460.58 1376.69 1466.9 Q1378.05 1464.89 1380.11 1463.82 Q1382.17 1462.73 1384.65 1462.73 Q1389.86 1462.73 1392.87 1465.9 Q1395.9 1469.05 1395.9 1474.49 Q1395.9 1479.82 1392.75 1483.03 Q1389.6 1486.25 1384.37 1486.25 Q1378.38 1486.25 1375.2 1481.67 Q1372.03 1477.06 1372.03 1468.33 Q1372.03 1460.14 1375.92 1455.28 Q1379.81 1450.39 1386.36 1450.39 Q1388.12 1450.39 1389.9 1450.74 Q1391.71 1451.09 1393.65 1451.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1837.73 1469.17 Q1834.4 1469.17 1832.48 1470.95 Q1830.58 1472.73 1830.58 1475.86 Q1830.58 1478.98 1832.48 1480.77 Q1834.4 1482.55 1837.73 1482.55 Q1841.06 1482.55 1842.99 1480.77 Q1844.91 1478.96 1844.91 1475.86 Q1844.91 1472.73 1842.99 1470.95 Q1841.09 1469.17 1837.73 1469.17 M1833.06 1467.18 Q1830.05 1466.44 1828.36 1464.38 Q1826.69 1462.32 1826.69 1459.35 Q1826.69 1455.21 1829.63 1452.8 Q1832.59 1450.39 1837.73 1450.39 Q1842.89 1450.39 1845.83 1452.8 Q1848.77 1455.21 1848.77 1459.35 Q1848.77 1462.32 1847.08 1464.38 Q1845.42 1466.44 1842.43 1467.18 Q1845.81 1467.96 1847.69 1470.26 Q1849.58 1472.55 1849.58 1475.86 Q1849.58 1480.88 1846.5 1483.57 Q1843.45 1486.25 1837.73 1486.25 Q1832.01 1486.25 1828.94 1483.57 Q1825.88 1480.88 1825.88 1475.86 Q1825.88 1472.55 1827.78 1470.26 Q1829.68 1467.96 1833.06 1467.18 M1831.34 1459.79 Q1831.34 1462.48 1833.01 1463.98 Q1834.7 1465.49 1837.73 1465.49 Q1840.74 1465.49 1842.43 1463.98 Q1844.14 1462.48 1844.14 1459.79 Q1844.14 1457.11 1842.43 1455.6 Q1840.74 1454.1 1837.73 1454.1 Q1834.7 1454.1 1833.01 1455.6 Q1831.34 1457.11 1831.34 1459.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2266.19 1481.64 L2273.82 1481.64 L2273.82 1455.28 L2265.51 1456.95 L2265.51 1452.69 L2273.78 1451.02 L2278.45 1451.02 L2278.45 1481.64 L2286.09 1481.64 L2286.09 1485.58 L2266.19 1485.58 L2266.19 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2305.54 1454.1 Q2301.93 1454.1 2300.1 1457.66 Q2298.29 1461.2 2298.29 1468.33 Q2298.29 1475.44 2300.1 1479.01 Q2301.93 1482.55 2305.54 1482.55 Q2309.17 1482.55 2310.98 1479.01 Q2312.81 1475.44 2312.81 1468.33 Q2312.81 1461.2 2310.98 1457.66 Q2309.17 1454.1 2305.54 1454.1 M2305.54 1450.39 Q2311.35 1450.39 2314.4 1455 Q2317.48 1459.58 2317.48 1468.33 Q2317.48 1477.06 2314.4 1481.67 Q2311.35 1486.25 2305.54 1486.25 Q2299.73 1486.25 2296.65 1481.67 Q2293.59 1477.06 2293.59 1468.33 Q2293.59 1459.58 2296.65 1455 Q2299.73 1450.39 2305.54 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1190.71 1548.76 L1190.71 1551.62 L1163.79 1551.62 Q1164.17 1557.67 1167.42 1560.85 Q1170.69 1564 1176.52 1564 Q1179.89 1564 1183.04 1563.17 Q1186.23 1562.35 1189.35 1560.69 L1189.35 1566.23 Q1186.19 1567.57 1182.88 1568.27 Q1179.57 1568.97 1176.17 1568.97 Q1167.64 1568.97 1162.64 1564 Q1157.68 1559.04 1157.68 1550.57 Q1157.68 1541.82 1162.39 1536.69 Q1167.13 1531.54 1175.15 1531.54 Q1182.34 1531.54 1186.51 1536.18 Q1190.71 1540.8 1190.71 1548.76 M1184.86 1547.04 Q1184.79 1542.23 1182.15 1539.37 Q1179.54 1536.5 1175.21 1536.5 Q1170.31 1536.5 1167.35 1539.27 Q1164.42 1542.04 1163.98 1547.07 L1184.86 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1205.99 1562.7 L1205.99 1581.6 L1200.1 1581.6 L1200.1 1532.4 L1205.99 1532.4 L1205.99 1537.81 Q1207.84 1534.62 1210.64 1533.1 Q1213.47 1531.54 1217.39 1531.54 Q1223.88 1531.54 1227.92 1536.69 Q1232 1541.85 1232 1550.25 Q1232 1558.65 1227.92 1563.81 Q1223.88 1568.97 1217.39 1568.97 Q1213.47 1568.97 1210.64 1567.44 Q1207.84 1565.88 1205.99 1562.7 M1225.92 1550.25 Q1225.92 1543.79 1223.24 1540.13 Q1220.6 1536.44 1215.95 1536.44 Q1211.31 1536.44 1208.63 1540.13 Q1205.99 1543.79 1205.99 1550.25 Q1205.99 1556.71 1208.63 1560.4 Q1211.31 1564.07 1215.95 1564.07 Q1220.6 1564.07 1223.24 1560.4 Q1225.92 1556.71 1225.92 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1255.52 1536.5 Q1250.81 1536.5 1248.07 1540.19 Q1245.33 1543.85 1245.33 1550.25 Q1245.33 1556.65 1248.04 1560.34 Q1250.77 1564 1255.52 1564 Q1260.2 1564 1262.93 1560.31 Q1265.67 1556.62 1265.67 1550.25 Q1265.67 1543.92 1262.93 1540.23 Q1260.2 1536.5 1255.52 1536.5 M1255.52 1531.54 Q1263.16 1531.54 1267.52 1536.5 Q1271.88 1541.47 1271.88 1550.25 Q1271.88 1559 1267.52 1564 Q1263.16 1568.97 1255.52 1568.97 Q1247.85 1568.97 1243.49 1564 Q1239.16 1559 1239.16 1550.25 Q1239.16 1541.47 1243.49 1536.5 Q1247.85 1531.54 1255.52 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1307.24 1533.76 L1307.24 1539.24 Q1304.76 1537.87 1302.24 1537.2 Q1299.76 1536.5 1297.21 1536.5 Q1291.51 1536.5 1288.36 1540.13 Q1285.21 1543.73 1285.21 1550.25 Q1285.21 1556.78 1288.36 1560.4 Q1291.51 1564 1297.21 1564 Q1299.76 1564 1302.24 1563.33 Q1304.76 1562.63 1307.24 1561.26 L1307.24 1566.68 Q1304.79 1567.82 1302.15 1568.39 Q1299.54 1568.97 1296.58 1568.97 Q1288.52 1568.97 1283.78 1563.91 Q1279.04 1558.85 1279.04 1550.25 Q1279.04 1541.53 1283.81 1536.53 Q1288.62 1531.54 1296.96 1531.54 Q1299.66 1531.54 1302.24 1532.11 Q1304.82 1532.65 1307.24 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1347.06 1546.53 L1347.06 1568.04 L1341.2 1568.04 L1341.2 1546.72 Q1341.2 1541.66 1339.23 1539.14 Q1337.25 1536.63 1333.31 1536.63 Q1328.56 1536.63 1325.83 1539.65 Q1323.09 1542.68 1323.09 1547.9 L1323.09 1568.04 L1317.2 1568.04 L1317.2 1518.52 L1323.09 1518.52 L1323.09 1537.93 Q1325.19 1534.72 1328.02 1533.13 Q1330.89 1531.54 1334.61 1531.54 Q1340.75 1531.54 1343.9 1535.36 Q1347.06 1539.14 1347.06 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M1381.46 1533.45 L1381.46 1538.98 Q1378.98 1537.71 1376.31 1537.07 Q1373.63 1536.44 1370.77 1536.44 Q1366.41 1536.44 1364.21 1537.77 Q1362.05 1539.11 1362.05 1541.79 Q1362.05 1543.82 1363.61 1545 Q1365.17 1546.15 1369.88 1547.2 L1371.88 1547.64 Q1378.12 1548.98 1380.73 1551.43 Q1383.37 1553.85 1383.37 1558.21 Q1383.37 1563.17 1379.43 1566.07 Q1375.51 1568.97 1368.64 1568.97 Q1365.77 1568.97 1362.65 1568.39 Q1359.56 1567.85 1356.13 1566.74 L1356.13 1560.69 Q1359.37 1562.38 1362.52 1563.24 Q1365.68 1564.07 1368.76 1564.07 Q1372.9 1564.07 1375.13 1562.66 Q1377.36 1561.23 1377.36 1558.65 Q1377.36 1556.27 1375.73 1554.99 Q1374.14 1553.72 1368.7 1552.54 L1366.66 1552.07 Q1361.22 1550.92 1358.8 1548.56 Q1356.38 1546.18 1356.38 1542.04 Q1356.38 1537.01 1359.95 1534.27 Q1363.51 1531.54 1370.07 1531.54 Q1373.31 1531.54 1376.18 1532.01 Q1379.04 1532.49 1381.46 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.292,1423.18 188.292,47.2441 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.292,1010.37 207.19,1010.37 "/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.292,130.925 207.19,130.925 "/>
<path clip-path="url(#clip030)" d="M51.6634 1030.16 L59.3023 1030.16 L59.3023 1003.8 L50.9921 1005.46 L50.9921 1001.2 L59.256 999.536 L63.9319 999.536 L63.9319 1030.16 L71.5707 1030.16 L71.5707 1034.1 L51.6634 1034.1 L51.6634 1030.16 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M91.0151 1002.62 Q87.404 1002.62 85.5753 1006.18 Q83.7697 1009.72 83.7697 1016.85 Q83.7697 1023.96 85.5753 1027.52 Q87.404 1031.06 91.0151 1031.06 Q94.6493 1031.06 96.4548 1027.52 Q98.2835 1023.96 98.2835 1016.85 Q98.2835 1009.72 96.4548 1006.18 Q94.6493 1002.62 91.0151 1002.62 M91.0151 998.911 Q96.8252 998.911 99.8808 1003.52 Q102.959 1008.1 102.959 1016.85 Q102.959 1025.58 99.8808 1030.18 Q96.8252 1034.77 91.0151 1034.77 Q85.2049 1034.77 82.1262 1030.18 Q79.0707 1025.58 79.0707 1016.85 Q79.0707 1008.1 82.1262 1003.52 Q85.2049 998.911 91.0151 998.911 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M102.959 993.013 L127.071 993.013 L127.071 996.21 L102.959 996.21 L102.959 993.013 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M135.309 978.606 L150.223 978.606 L150.223 981.803 L138.788 981.803 L138.788 988.687 Q139.616 988.405 140.443 988.273 Q141.271 988.123 142.098 988.123 Q146.8 988.123 149.546 990.699 Q152.292 993.276 152.292 997.677 Q152.292 1002.21 149.471 1004.73 Q146.65 1007.23 141.515 1007.23 Q139.747 1007.23 137.904 1006.93 Q136.08 1006.63 134.124 1006.03 L134.124 1002.21 Q135.817 1003.13 137.622 1003.58 Q139.428 1004.03 141.44 1004.03 Q144.694 1004.03 146.593 1002.32 Q148.493 1000.61 148.493 997.677 Q148.493 994.743 146.593 993.032 Q144.694 991.32 141.44 991.32 Q139.917 991.32 138.393 991.659 Q136.889 991.997 135.309 992.712 L135.309 978.606 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M81.5866 150.718 L89.2254 150.718 L89.2254 124.352 L80.9153 126.019 L80.9153 121.76 L89.1791 120.093 L93.855 120.093 L93.855 150.718 L101.494 150.718 L101.494 154.653 L81.5866 154.653 L81.5866 150.718 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M120.938 123.172 Q117.327 123.172 115.498 126.736 Q113.693 130.278 113.693 137.408 Q113.693 144.514 115.498 148.079 Q117.327 151.621 120.938 151.621 Q124.572 151.621 126.378 148.079 Q128.207 144.514 128.207 137.408 Q128.207 130.278 126.378 126.736 Q124.572 123.172 120.938 123.172 M120.938 119.468 Q126.748 119.468 129.804 124.074 Q132.883 128.658 132.883 137.408 Q132.883 146.134 129.804 150.741 Q126.748 155.324 120.938 155.324 Q115.128 155.324 112.049 150.741 Q108.994 146.134 108.994 137.408 Q108.994 128.658 112.049 124.074 Q115.128 119.468 120.938 119.468 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M142.587 101.664 Q139.653 101.664 138.168 104.56 Q136.701 107.438 136.701 113.231 Q136.701 119.005 138.168 121.901 Q139.653 124.779 142.587 124.779 Q145.54 124.779 147.007 121.901 Q148.493 119.005 148.493 113.231 Q148.493 107.438 147.007 104.56 Q145.54 101.664 142.587 101.664 M142.587 98.6547 Q147.308 98.6547 149.791 102.397 Q152.292 106.121 152.292 113.231 Q152.292 120.321 149.791 124.064 Q147.308 127.788 142.587 127.788 Q137.867 127.788 135.365 124.064 Q132.883 120.321 132.883 113.231 Q132.883 106.121 135.365 102.397 Q137.867 98.6547 142.587 98.6547 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip032)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.551,305.113 476.434,487.243 703.317,623.559 930.2,778.664 1157.08,872.215 1383.97,980.113 1610.85,1064.08 1837.73,1180.33 2064.61,1269.74 2291.5,1384.24 "/>
<polyline clip-path="url(#clip032)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.551,86.1857 476.434,86.7778 703.317,86.9177 930.2,86.9562 1157.08,86.9967 1383.97,87.0014 1610.85,87.0119 1837.73,87.0159 2064.61,87.0171 2291.5,87.0198 "/>
<path clip-path="url(#clip030)" d="M1842.12 248.629 L2280.61 248.629 L2280.61 93.1086 L1842.12 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip030)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1842.12,248.629 2280.61,248.629 2280.61,93.1086 1842.12,93.1086 1842.12,248.629 "/>
<polyline clip-path="url(#clip030)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1866.17,144.949 2010.47,144.949 "/>
<path clip-path="url(#clip030)" d="M2041.93 128.942 L2041.93 136.303 L2050.7 136.303 L2050.7 139.613 L2041.93 139.613 L2041.93 153.687 Q2041.93 156.858 2042.79 157.761 Q2043.67 158.664 2046.33 158.664 L2050.7 158.664 L2050.7 162.229 L2046.33 162.229 Q2041.4 162.229 2039.52 160.4 Q2037.65 158.548 2037.65 153.687 L2037.65 139.613 L2034.52 139.613 L2034.52 136.303 L2037.65 136.303 L2037.65 128.942 L2041.93 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2071.33 140.284 Q2070.61 139.868 2069.75 139.682 Q2068.92 139.474 2067.9 139.474 Q2064.29 139.474 2062.35 141.835 Q2060.42 144.173 2060.42 148.571 L2060.42 162.229 L2056.14 162.229 L2056.14 136.303 L2060.42 136.303 L2060.42 140.331 Q2061.77 137.969 2063.92 136.835 Q2066.07 135.678 2069.15 135.678 Q2069.59 135.678 2070.12 135.747 Q2070.66 135.794 2071.3 135.909 L2071.33 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2087.58 149.196 Q2082.41 149.196 2080.42 150.377 Q2078.43 151.557 2078.43 154.405 Q2078.43 156.673 2079.91 158.016 Q2081.42 159.335 2083.99 159.335 Q2087.53 159.335 2089.66 156.835 Q2091.81 154.312 2091.81 150.145 L2091.81 149.196 L2087.58 149.196 M2096.07 147.437 L2096.07 162.229 L2091.81 162.229 L2091.81 158.293 Q2090.35 160.655 2088.18 161.789 Q2086 162.9 2082.85 162.9 Q2078.87 162.9 2076.51 160.678 Q2074.17 158.432 2074.17 154.682 Q2074.17 150.307 2077.09 148.085 Q2080.03 145.863 2085.84 145.863 L2091.81 145.863 L2091.81 145.446 Q2091.81 142.507 2089.87 140.909 Q2087.95 139.289 2084.45 139.289 Q2082.23 139.289 2080.12 139.821 Q2078.02 140.354 2076.07 141.419 L2076.07 137.483 Q2078.41 136.581 2080.61 136.141 Q2082.81 135.678 2084.89 135.678 Q2090.52 135.678 2093.29 138.594 Q2096.07 141.511 2096.07 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2104.85 136.303 L2109.1 136.303 L2109.1 162.229 L2104.85 162.229 L2104.85 136.303 M2104.85 126.21 L2109.1 126.21 L2109.1 131.604 L2104.85 131.604 L2104.85 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2139.57 146.581 L2139.57 162.229 L2135.31 162.229 L2135.31 146.719 Q2135.31 143.039 2133.87 141.21 Q2132.44 139.382 2129.57 139.382 Q2126.12 139.382 2124.13 141.581 Q2122.14 143.78 2122.14 147.576 L2122.14 162.229 L2117.85 162.229 L2117.85 136.303 L2122.14 136.303 L2122.14 140.331 Q2123.66 137.993 2125.72 136.835 Q2127.81 135.678 2130.52 135.678 Q2134.98 135.678 2137.28 138.456 Q2139.57 141.21 2139.57 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2167.76 170.099 L2167.76 173.409 L2143.13 173.409 L2143.13 170.099 L2167.76 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2171.77 126.21 L2176.03 126.21 L2176.03 162.229 L2171.77 162.229 L2171.77 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2194.98 139.289 Q2191.56 139.289 2189.57 141.974 Q2187.58 144.636 2187.58 149.289 Q2187.58 153.942 2189.54 156.627 Q2191.53 159.289 2194.98 159.289 Q2198.39 159.289 2200.38 156.604 Q2202.37 153.918 2202.37 149.289 Q2202.37 144.682 2200.38 141.997 Q2198.39 139.289 2194.98 139.289 M2194.98 135.678 Q2200.54 135.678 2203.71 139.289 Q2206.88 142.9 2206.88 149.289 Q2206.88 155.655 2203.71 159.289 Q2200.54 162.9 2194.98 162.9 Q2189.41 162.9 2186.23 159.289 Q2183.09 155.655 2183.09 149.289 Q2183.09 142.9 2186.23 139.289 Q2189.41 135.678 2194.98 135.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2230.47 137.067 L2230.47 141.094 Q2228.66 140.169 2226.72 139.706 Q2224.78 139.243 2222.69 139.243 Q2219.52 139.243 2217.92 140.215 Q2216.35 141.187 2216.35 143.131 Q2216.35 144.613 2217.48 145.469 Q2218.62 146.303 2222.04 147.067 L2223.5 147.391 Q2228.04 148.363 2229.94 150.145 Q2231.86 151.905 2231.86 155.076 Q2231.86 158.687 2228.99 160.793 Q2226.14 162.9 2221.14 162.9 Q2219.06 162.9 2216.79 162.483 Q2214.54 162.09 2212.04 161.28 L2212.04 156.881 Q2214.4 158.108 2216.7 158.733 Q2218.99 159.335 2221.23 159.335 Q2224.24 159.335 2225.86 158.317 Q2227.48 157.275 2227.48 155.4 Q2227.48 153.664 2226.3 152.738 Q2225.15 151.812 2221.19 150.956 L2219.71 150.608 Q2215.75 149.775 2213.99 148.062 Q2212.23 146.326 2212.23 143.317 Q2212.23 139.659 2214.82 137.669 Q2217.41 135.678 2222.18 135.678 Q2224.54 135.678 2226.63 136.025 Q2228.71 136.372 2230.47 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2255.17 137.067 L2255.17 141.094 Q2253.36 140.169 2251.42 139.706 Q2249.47 139.243 2247.39 139.243 Q2244.22 139.243 2242.62 140.215 Q2241.05 141.187 2241.05 143.131 Q2241.05 144.613 2242.18 145.469 Q2243.32 146.303 2246.74 147.067 L2248.2 147.391 Q2252.74 148.363 2254.64 150.145 Q2256.56 151.905 2256.56 155.076 Q2256.56 158.687 2253.69 160.793 Q2250.84 162.9 2245.84 162.9 Q2243.76 162.9 2241.49 162.483 Q2239.24 162.09 2236.74 161.28 L2236.74 156.881 Q2239.1 158.108 2241.4 158.733 Q2243.69 159.335 2245.93 159.335 Q2248.94 159.335 2250.56 158.317 Q2252.18 157.275 2252.18 155.4 Q2252.18 153.664 2251 152.738 Q2249.84 151.812 2245.89 150.956 L2244.4 150.608 Q2240.45 149.775 2238.69 148.062 Q2236.93 146.326 2236.93 143.317 Q2236.93 139.659 2239.52 137.669 Q2242.11 135.678 2246.88 135.678 Q2249.24 135.678 2251.33 136.025 Q2253.41 136.372 2255.17 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip030)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1866.17,196.789 2010.47,196.789 "/>
<path clip-path="url(#clip030)" d="M2034.52 188.143 L2039.04 188.143 L2047.14 209.902 L2055.24 188.143 L2059.75 188.143 L2050.03 214.069 L2044.24 214.069 L2034.52 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2077.41 201.036 Q2072.25 201.036 2070.26 202.217 Q2068.27 203.397 2068.27 206.245 Q2068.27 208.513 2069.75 209.856 Q2071.26 211.175 2073.83 211.175 Q2077.37 211.175 2079.5 208.675 Q2081.65 206.152 2081.65 201.985 L2081.65 201.036 L2077.41 201.036 M2085.91 199.277 L2085.91 214.069 L2081.65 214.069 L2081.65 210.133 Q2080.19 212.495 2078.02 213.629 Q2075.84 214.74 2072.69 214.74 Q2068.71 214.74 2066.35 212.518 Q2064.01 210.272 2064.01 206.522 Q2064.01 202.147 2066.93 199.925 Q2069.87 197.703 2075.68 197.703 L2081.65 197.703 L2081.65 197.286 Q2081.65 194.347 2079.71 192.749 Q2077.79 191.129 2074.29 191.129 Q2072.07 191.129 2069.96 191.661 Q2067.85 192.194 2065.91 193.259 L2065.91 189.323 Q2068.25 188.421 2070.45 187.981 Q2072.65 187.518 2074.73 187.518 Q2080.35 187.518 2083.13 190.434 Q2085.91 193.351 2085.91 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2094.68 178.05 L2098.94 178.05 L2098.94 214.069 L2094.68 214.069 L2094.68 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2127.55 221.939 L2127.55 225.249 L2102.92 225.249 L2102.92 221.939 L2127.55 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2131.56 178.05 L2135.82 178.05 L2135.82 214.069 L2131.56 214.069 L2131.56 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2154.78 191.129 Q2151.35 191.129 2149.36 193.814 Q2147.37 196.476 2147.37 201.129 Q2147.37 205.782 2149.34 208.467 Q2151.33 211.129 2154.78 211.129 Q2158.18 211.129 2160.17 208.444 Q2162.16 205.758 2162.16 201.129 Q2162.16 196.522 2160.17 193.837 Q2158.18 191.129 2154.78 191.129 M2154.78 187.518 Q2160.33 187.518 2163.5 191.129 Q2166.67 194.74 2166.67 201.129 Q2166.67 207.495 2163.5 211.129 Q2160.33 214.74 2154.78 214.74 Q2149.2 214.74 2146.03 211.129 Q2142.88 207.495 2142.88 201.129 Q2142.88 194.74 2146.03 191.129 Q2149.2 187.518 2154.78 187.518 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2190.26 188.907 L2190.26 192.934 Q2188.46 192.009 2186.51 191.546 Q2184.57 191.083 2182.48 191.083 Q2179.31 191.083 2177.72 192.055 Q2176.14 193.027 2176.14 194.971 Q2176.14 196.453 2177.28 197.309 Q2178.41 198.143 2181.84 198.907 L2183.29 199.231 Q2187.83 200.203 2189.73 201.985 Q2191.65 203.745 2191.65 206.916 Q2191.65 210.527 2188.78 212.633 Q2185.93 214.74 2180.93 214.74 Q2178.85 214.74 2176.58 214.323 Q2174.34 213.93 2171.84 213.12 L2171.84 208.721 Q2174.2 209.948 2176.49 210.573 Q2178.78 211.175 2181.03 211.175 Q2184.03 211.175 2185.66 210.157 Q2187.28 209.115 2187.28 207.24 Q2187.28 205.504 2186.09 204.578 Q2184.94 203.652 2180.98 202.796 L2179.5 202.448 Q2175.54 201.615 2173.78 199.902 Q2172.02 198.166 2172.02 195.157 Q2172.02 191.499 2174.61 189.509 Q2177.21 187.518 2181.97 187.518 Q2184.34 187.518 2186.42 187.865 Q2188.5 188.212 2190.26 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip030)" d="M2214.96 188.907 L2214.96 192.934 Q2213.15 192.009 2211.21 191.546 Q2209.27 191.083 2207.18 191.083 Q2204.01 191.083 2202.41 192.055 Q2200.84 193.027 2200.84 194.971 Q2200.84 196.453 2201.97 197.309 Q2203.11 198.143 2206.53 198.907 L2207.99 199.231 Q2212.53 200.203 2214.43 201.985 Q2216.35 203.745 2216.35 206.916 Q2216.35 210.527 2213.48 212.633 Q2210.63 214.74 2205.63 214.74 Q2203.55 214.74 2201.28 214.323 Q2199.03 213.93 2196.53 213.12 L2196.53 208.721 Q2198.9 209.948 2201.19 210.573 Q2203.48 211.175 2205.72 211.175 Q2208.73 211.175 2210.35 210.157 Q2211.97 209.115 2211.97 207.24 Q2211.97 205.504 2210.79 204.578 Q2209.64 203.652 2205.68 202.796 L2204.2 202.448 Q2200.24 201.615 2198.48 199.902 Q2196.72 198.166 2196.72 195.157 Q2196.72 191.499 2199.31 189.509 Q2201.91 187.518 2206.67 187.518 Q2209.03 187.518 2211.12 187.865 Q2213.2 188.212 2214.96 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="nohighlight hljs">(WeightDecayScratch{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, @NamedTuple{lambda::Float64}}(Dense(200 =&gt; 1), (lambda = 0.0,)), (val_loss = [0.6315150462155444, 1.7667208566284978, 1.0124669697653603, 0.34469795893243227, 1.0563052358000395, 2.566221664070826, 0.4486341039828149, 1.606172932980381, 2.1008154580934293, 2.67316597039848, 1.156265822363412, 1.0914530668393172, 4.342967840175924, 2.177160883786672, 1.0240883719609932, 0.30057182957817985, 6.080012696794702, 3.1974150170736544, 1.5931458666944185, 0.3647579105026305], val_acc = nothing))</code></pre><h3 id="Using-Weight-Decay"><a class="docs-heading-anchor" href="#Using-Weight-Decay">Using Weight Decay</a><a id="Using-Weight-Decay-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Weight-Decay" title="Permalink"></a></h3><p>Below, we run with substantial weight decay. Note that the training error increases but the validation error decreases. This is precisely the effect we expect from regularization.</p><pre><code class="language-julia hljs">train_scratch(3.)</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 3.2797984202712356, Val Loss: 4.062983477700901
    [ Info: Train Loss: 1.9804117886097512, Val Loss: 2.4610657281266337
    [ Info: Train Loss: 1.2077013343062588, Val Loss: 1.500083301680783
    [ Info: Train Loss: 0.7378329506568404, Val Loss: 0.9140557421194878
    [ Info: Train Loss: 0.4516074772083898, Val Loss: 0.5599806966984647
    [ Info: Train Loss: 0.2769862280470018, Val Loss: 0.3442206324581537
    [ Info: Train Loss: 0.17134064441566937, Val Loss: 0.2129407598175414
    [ Info: Train Loss: 0.10625916231477397, Val Loss: 0.13353039201811642
    [ Info: Train Loss: 0.06703511114350612, Val Loss: 0.08539619997682976
    [ Info: Train Loss: 0.0427132283461683, Val Loss: 0.05658541332204206</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip320">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip320)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip321">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip320)" d="M188.104 1423.18 L2352.76 1423.18 L2352.76 47.2441 L188.104 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip322">
    <rect x="188" y="47" width="2166" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="476.271,1423.18 476.271,47.2441 "/>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="930.076,1423.18 930.076,47.2441 "/>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1383.88,1423.18 1383.88,47.2441 "/>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1837.69,1423.18 1837.69,47.2441 "/>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2291.49,1423.18 2291.49,47.2441 "/>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="188.104,1185.1 2352.76,1185.1 "/>
<polyline clip-path="url(#clip322)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="188.104,515.424 2352.76,515.424 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.104,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="476.271,1423.18 476.271,1404.28 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="930.076,1423.18 930.076,1404.28 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1383.88,1423.18 1383.88,1404.28 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1837.69,1423.18 1837.69,1404.28 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2291.49,1423.18 2291.49,1404.28 "/>
<path clip-path="url(#clip320)" d="M470.923 1481.64 L487.243 1481.64 L487.243 1485.58 L465.298 1485.58 L465.298 1481.64 Q467.96 1478.89 472.544 1474.26 Q477.15 1469.61 478.331 1468.27 Q480.576 1465.74 481.456 1464.01 Q482.359 1462.25 482.359 1460.56 Q482.359 1457.8 480.414 1456.07 Q478.493 1454.33 475.391 1454.33 Q473.192 1454.33 470.738 1455.09 Q468.308 1455.86 465.53 1457.41 L465.53 1452.69 Q468.354 1451.55 470.808 1450.97 Q473.261 1450.39 475.298 1450.39 Q480.669 1450.39 483.863 1453.08 Q487.058 1455.77 487.058 1460.26 Q487.058 1462.39 486.247 1464.31 Q485.46 1466.2 483.354 1468.8 Q482.775 1469.47 479.673 1472.69 Q476.571 1475.88 470.923 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M933.085 1455.09 L921.28 1473.54 L933.085 1473.54 L933.085 1455.09 M931.858 1451.02 L937.738 1451.02 L937.738 1473.54 L942.668 1473.54 L942.668 1477.43 L937.738 1477.43 L937.738 1485.58 L933.085 1485.58 L933.085 1477.43 L917.483 1477.43 L917.483 1472.92 L931.858 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1384.29 1466.44 Q1381.14 1466.44 1379.29 1468.59 Q1377.46 1470.74 1377.46 1474.49 Q1377.46 1478.22 1379.29 1480.39 Q1381.14 1482.55 1384.29 1482.55 Q1387.43 1482.55 1389.26 1480.39 Q1391.12 1478.22 1391.12 1474.49 Q1391.12 1470.74 1389.26 1468.59 Q1387.43 1466.44 1384.29 1466.44 M1393.57 1451.78 L1393.57 1456.04 Q1391.81 1455.21 1390 1454.77 Q1388.22 1454.33 1386.46 1454.33 Q1381.83 1454.33 1379.38 1457.45 Q1376.95 1460.58 1376.6 1466.9 Q1377.97 1464.89 1380.03 1463.82 Q1382.09 1462.73 1384.56 1462.73 Q1389.77 1462.73 1392.78 1465.9 Q1395.81 1469.05 1395.81 1474.49 Q1395.81 1479.82 1392.67 1483.03 Q1389.52 1486.25 1384.29 1486.25 Q1378.29 1486.25 1375.12 1481.67 Q1371.95 1477.06 1371.95 1468.33 Q1371.95 1460.14 1375.84 1455.28 Q1379.73 1450.39 1386.28 1450.39 Q1388.04 1450.39 1389.82 1450.74 Q1391.62 1451.09 1393.57 1451.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1837.69 1469.17 Q1834.35 1469.17 1832.43 1470.95 Q1830.53 1472.73 1830.53 1475.86 Q1830.53 1478.98 1832.43 1480.77 Q1834.35 1482.55 1837.69 1482.55 Q1841.02 1482.55 1842.94 1480.77 Q1844.86 1478.96 1844.86 1475.86 Q1844.86 1472.73 1842.94 1470.95 Q1841.04 1469.17 1837.69 1469.17 M1833.01 1467.18 Q1830 1466.44 1828.31 1464.38 Q1826.65 1462.32 1826.65 1459.35 Q1826.65 1455.21 1829.58 1452.8 Q1832.55 1450.39 1837.69 1450.39 Q1842.85 1450.39 1845.79 1452.8 Q1848.73 1455.21 1848.73 1459.35 Q1848.73 1462.32 1847.04 1464.38 Q1845.37 1466.44 1842.39 1467.18 Q1845.77 1467.96 1847.64 1470.26 Q1849.54 1472.55 1849.54 1475.86 Q1849.54 1480.88 1846.46 1483.57 Q1843.4 1486.25 1837.69 1486.25 Q1831.97 1486.25 1828.89 1483.57 Q1825.83 1480.88 1825.83 1475.86 Q1825.83 1472.55 1827.73 1470.26 Q1829.63 1467.96 1833.01 1467.18 M1831.3 1459.79 Q1831.3 1462.48 1832.96 1463.98 Q1834.65 1465.49 1837.69 1465.49 Q1840.7 1465.49 1842.39 1463.98 Q1844.1 1462.48 1844.1 1459.79 Q1844.1 1457.11 1842.39 1455.6 Q1840.7 1454.1 1837.69 1454.1 Q1834.65 1454.1 1832.96 1455.6 Q1831.3 1457.11 1831.3 1459.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2266.18 1481.64 L2273.82 1481.64 L2273.82 1455.28 L2265.51 1456.95 L2265.51 1452.69 L2273.77 1451.02 L2278.45 1451.02 L2278.45 1481.64 L2286.09 1481.64 L2286.09 1485.58 L2266.18 1485.58 L2266.18 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2305.53 1454.1 Q2301.92 1454.1 2300.09 1457.66 Q2298.29 1461.2 2298.29 1468.33 Q2298.29 1475.44 2300.09 1479.01 Q2301.92 1482.55 2305.53 1482.55 Q2309.17 1482.55 2310.97 1479.01 Q2312.8 1475.44 2312.8 1468.33 Q2312.8 1461.2 2310.97 1457.66 Q2309.17 1454.1 2305.53 1454.1 M2305.53 1450.39 Q2311.34 1450.39 2314.4 1455 Q2317.48 1459.58 2317.48 1468.33 Q2317.48 1477.06 2314.4 1481.67 Q2311.34 1486.25 2305.53 1486.25 Q2299.72 1486.25 2296.64 1481.67 Q2293.59 1477.06 2293.59 1468.33 Q2293.59 1459.58 2296.64 1455 Q2299.72 1450.39 2305.53 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1190.62 1548.76 L1190.62 1551.62 L1163.69 1551.62 Q1164.07 1557.67 1167.32 1560.85 Q1170.6 1564 1176.42 1564 Q1179.8 1564 1182.95 1563.17 Q1186.13 1562.35 1189.25 1560.69 L1189.25 1566.23 Q1186.1 1567.57 1182.79 1568.27 Q1179.48 1568.97 1176.07 1568.97 Q1167.54 1568.97 1162.55 1564 Q1157.58 1559.04 1157.58 1550.57 Q1157.58 1541.82 1162.29 1536.69 Q1167.04 1531.54 1175.06 1531.54 Q1182.25 1531.54 1186.42 1536.18 Q1190.62 1540.8 1190.62 1548.76 M1184.76 1547.04 Q1184.7 1542.23 1182.06 1539.37 Q1179.45 1536.5 1175.12 1536.5 Q1170.22 1536.5 1167.26 1539.27 Q1164.33 1542.04 1163.88 1547.07 L1184.76 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1205.9 1562.7 L1205.9 1581.6 L1200.01 1581.6 L1200.01 1532.4 L1205.9 1532.4 L1205.9 1537.81 Q1207.74 1534.62 1210.54 1533.1 Q1213.38 1531.54 1217.29 1531.54 Q1223.79 1531.54 1227.83 1536.69 Q1231.9 1541.85 1231.9 1550.25 Q1231.9 1558.65 1227.83 1563.81 Q1223.79 1568.97 1217.29 1568.97 Q1213.38 1568.97 1210.54 1567.44 Q1207.74 1565.88 1205.9 1562.7 M1225.82 1550.25 Q1225.82 1543.79 1223.15 1540.13 Q1220.51 1536.44 1215.86 1536.44 Q1211.21 1536.44 1208.54 1540.13 Q1205.9 1543.79 1205.9 1550.25 Q1205.9 1556.71 1208.54 1560.4 Q1211.21 1564.07 1215.86 1564.07 Q1220.51 1564.07 1223.15 1560.4 Q1225.82 1556.71 1225.82 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1255.42 1536.5 Q1250.71 1536.5 1247.97 1540.19 Q1245.24 1543.85 1245.24 1550.25 Q1245.24 1556.65 1247.94 1560.34 Q1250.68 1564 1255.42 1564 Q1260.1 1564 1262.84 1560.31 Q1265.58 1556.62 1265.58 1550.25 Q1265.58 1543.92 1262.84 1540.23 Q1260.1 1536.5 1255.42 1536.5 M1255.42 1531.54 Q1263.06 1531.54 1267.42 1536.5 Q1271.78 1541.47 1271.78 1550.25 Q1271.78 1559 1267.42 1564 Q1263.06 1568.97 1255.42 1568.97 Q1247.75 1568.97 1243.39 1564 Q1239.06 1559 1239.06 1550.25 Q1239.06 1541.47 1243.39 1536.5 Q1247.75 1531.54 1255.42 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1307.14 1533.76 L1307.14 1539.24 Q1304.66 1537.87 1302.15 1537.2 Q1299.66 1536.5 1297.12 1536.5 Q1291.42 1536.5 1288.27 1540.13 Q1285.12 1543.73 1285.12 1550.25 Q1285.12 1556.78 1288.27 1560.4 Q1291.42 1564 1297.12 1564 Q1299.66 1564 1302.15 1563.33 Q1304.66 1562.63 1307.14 1561.26 L1307.14 1566.68 Q1304.69 1567.82 1302.05 1568.39 Q1299.44 1568.97 1296.48 1568.97 Q1288.43 1568.97 1283.69 1563.91 Q1278.94 1558.85 1278.94 1550.25 Q1278.94 1541.53 1283.72 1536.53 Q1288.52 1531.54 1296.86 1531.54 Q1299.57 1531.54 1302.15 1532.11 Q1304.73 1532.65 1307.14 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1346.96 1546.53 L1346.96 1568.04 L1341.11 1568.04 L1341.11 1546.72 Q1341.11 1541.66 1339.13 1539.14 Q1337.16 1536.63 1333.21 1536.63 Q1328.47 1536.63 1325.73 1539.65 Q1322.99 1542.68 1322.99 1547.9 L1322.99 1568.04 L1317.11 1568.04 L1317.11 1518.52 L1322.99 1518.52 L1322.99 1537.93 Q1325.1 1534.72 1327.93 1533.13 Q1330.79 1531.54 1334.52 1531.54 Q1340.66 1531.54 1343.81 1535.36 Q1346.96 1539.14 1346.96 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M1381.37 1533.45 L1381.37 1538.98 Q1378.89 1537.71 1376.21 1537.07 Q1373.54 1536.44 1370.67 1536.44 Q1366.31 1536.44 1364.12 1537.77 Q1361.95 1539.11 1361.95 1541.79 Q1361.95 1543.82 1363.51 1545 Q1365.07 1546.15 1369.78 1547.2 L1371.79 1547.64 Q1378.03 1548.98 1380.64 1551.43 Q1383.28 1553.85 1383.28 1558.21 Q1383.28 1563.17 1379.33 1566.07 Q1375.42 1568.97 1368.54 1568.97 Q1365.68 1568.97 1362.56 1568.39 Q1359.47 1567.85 1356.03 1566.74 L1356.03 1560.69 Q1359.28 1562.38 1362.43 1563.24 Q1365.58 1564.07 1368.67 1564.07 Q1372.81 1564.07 1375.03 1562.66 Q1377.26 1561.23 1377.26 1558.65 Q1377.26 1556.27 1375.64 1554.99 Q1374.05 1553.72 1368.61 1552.54 L1366.57 1552.07 Q1361.13 1550.92 1358.71 1548.56 Q1356.29 1546.18 1356.29 1542.04 Q1356.29 1537.01 1359.85 1534.27 Q1363.42 1531.54 1369.97 1531.54 Q1373.22 1531.54 1376.08 1532.01 Q1378.95 1532.49 1381.37 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.104,1423.18 188.104,47.2441 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.104,1185.1 207.002,1185.1 "/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="188.104,515.424 207.002,515.424 "/>
<path clip-path="url(#clip320)" d="M51.6634 1204.89 L59.3023 1204.89 L59.3023 1178.52 L50.9921 1180.19 L50.9921 1175.93 L59.256 1174.26 L63.9319 1174.26 L63.9319 1204.89 L71.5707 1204.89 L71.5707 1208.82 L51.6634 1208.82 L51.6634 1204.89 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M91.0151 1177.34 Q87.404 1177.34 85.5753 1180.91 Q83.7697 1184.45 83.7697 1191.58 Q83.7697 1198.69 85.5753 1202.25 Q87.404 1205.79 91.0151 1205.79 Q94.6493 1205.79 96.4548 1202.25 Q98.2835 1198.69 98.2835 1191.58 Q98.2835 1184.45 96.4548 1180.91 Q94.6493 1177.34 91.0151 1177.34 M91.0151 1173.64 Q96.8252 1173.64 99.8808 1178.25 Q102.959 1182.83 102.959 1191.58 Q102.959 1200.31 99.8808 1204.91 Q96.8252 1209.5 91.0151 1209.5 Q85.2049 1209.5 82.1262 1204.91 Q79.0707 1200.31 79.0707 1191.58 Q79.0707 1182.83 82.1262 1178.25 Q85.2049 1173.64 91.0151 1173.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M102.959 1167.74 L127.071 1167.74 L127.071 1170.94 L102.959 1170.94 L102.959 1167.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M135.929 1178.22 L142.136 1178.22 L142.136 1156.79 L135.384 1158.15 L135.384 1154.69 L142.098 1153.33 L145.898 1153.33 L145.898 1178.22 L152.104 1178.22 L152.104 1181.41 L135.929 1181.41 L135.929 1178.22 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M81.3985 535.216 L89.0374 535.216 L89.0374 508.851 L80.7272 510.518 L80.7272 506.258 L88.9911 504.592 L93.667 504.592 L93.667 535.216 L101.306 535.216 L101.306 539.152 L81.3985 539.152 L81.3985 535.216 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M120.75 507.67 Q117.139 507.67 115.31 511.235 Q113.505 514.777 113.505 521.906 Q113.505 529.013 115.31 532.578 Q117.139 536.119 120.75 536.119 Q124.384 536.119 126.19 532.578 Q128.019 529.013 128.019 521.906 Q128.019 514.777 126.19 511.235 Q124.384 507.67 120.75 507.67 M120.75 503.967 Q126.56 503.967 129.616 508.573 Q132.695 513.156 132.695 521.906 Q132.695 530.633 129.616 535.24 Q126.56 539.823 120.75 539.823 Q114.94 539.823 111.861 535.24 Q108.806 530.633 108.806 521.906 Q108.806 513.156 111.861 508.573 Q114.94 503.967 120.75 503.967 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M142.399 486.163 Q139.465 486.163 137.98 489.059 Q136.513 491.937 136.513 497.729 Q136.513 503.503 137.98 506.4 Q139.465 509.277 142.399 509.277 Q145.352 509.277 146.819 506.4 Q148.305 503.503 148.305 497.729 Q148.305 491.937 146.819 489.059 Q145.352 486.163 142.399 486.163 M142.399 483.153 Q147.12 483.153 149.603 486.896 Q152.104 490.62 152.104 497.729 Q152.104 504.82 149.603 508.563 Q147.12 512.287 142.399 512.287 Q137.679 512.287 135.177 508.563 Q132.695 504.82 132.695 497.729 Q132.695 490.62 135.177 486.896 Q137.679 483.153 142.399 483.153 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip322)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.368,102.312 476.271,259.005 703.173,403.817 930.076,547.301 1156.98,690.235 1383.88,832.604 1610.78,973.976 1837.69,1113.36 2064.59,1251.6 2291.49,1384.24 "/>
<polyline clip-path="url(#clip322)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.368,86.1857 476.271,230.354 703.173,372.531 930.076,513.688 1156.98,653.05 1383.88,790.251 1610.78,923.214 1837.69,1050.93 2064.59,1169.1 2291.49,1278.35 "/>
<path clip-path="url(#clip320)" d="M1842.1 248.629 L2280.6 248.629 L2280.6 93.1086 L1842.1 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip320)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1842.1,248.629 2280.6,248.629 2280.6,93.1086 1842.1,93.1086 1842.1,248.629 "/>
<polyline clip-path="url(#clip320)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1866.15,144.949 2010.46,144.949 "/>
<path clip-path="url(#clip320)" d="M2041.92 128.942 L2041.92 136.303 L2050.69 136.303 L2050.69 139.613 L2041.92 139.613 L2041.92 153.687 Q2041.92 156.858 2042.78 157.761 Q2043.66 158.664 2046.32 158.664 L2050.69 158.664 L2050.69 162.229 L2046.32 162.229 Q2041.39 162.229 2039.51 160.4 Q2037.64 158.548 2037.64 153.687 L2037.64 139.613 L2034.51 139.613 L2034.51 136.303 L2037.64 136.303 L2037.64 128.942 L2041.92 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2071.32 140.284 Q2070.6 139.868 2069.74 139.682 Q2068.91 139.474 2067.89 139.474 Q2064.28 139.474 2062.34 141.835 Q2060.42 144.173 2060.42 148.571 L2060.42 162.229 L2056.13 162.229 L2056.13 136.303 L2060.42 136.303 L2060.42 140.331 Q2061.76 137.969 2063.91 136.835 Q2066.06 135.678 2069.14 135.678 Q2069.58 135.678 2070.11 135.747 Q2070.65 135.794 2071.3 135.909 L2071.32 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2087.57 149.196 Q2082.41 149.196 2080.42 150.377 Q2078.43 151.557 2078.43 154.405 Q2078.43 156.673 2079.91 158.016 Q2081.41 159.335 2083.98 159.335 Q2087.52 159.335 2089.65 156.835 Q2091.8 154.312 2091.8 150.145 L2091.8 149.196 L2087.57 149.196 M2096.06 147.437 L2096.06 162.229 L2091.8 162.229 L2091.8 158.293 Q2090.35 160.655 2088.17 161.789 Q2085.99 162.9 2082.85 162.9 Q2078.86 162.9 2076.5 160.678 Q2074.17 158.432 2074.17 154.682 Q2074.17 150.307 2077.08 148.085 Q2080.02 145.863 2085.83 145.863 L2091.8 145.863 L2091.8 145.446 Q2091.8 142.507 2089.86 140.909 Q2087.94 139.289 2084.44 139.289 Q2082.22 139.289 2080.11 139.821 Q2078.01 140.354 2076.06 141.419 L2076.06 137.483 Q2078.4 136.581 2080.6 136.141 Q2082.8 135.678 2084.88 135.678 Q2090.51 135.678 2093.29 138.594 Q2096.06 141.511 2096.06 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2104.84 136.303 L2109.1 136.303 L2109.1 162.229 L2104.84 162.229 L2104.84 136.303 M2104.84 126.21 L2109.1 126.21 L2109.1 131.604 L2104.84 131.604 L2104.84 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2139.56 146.581 L2139.56 162.229 L2135.3 162.229 L2135.3 146.719 Q2135.3 143.039 2133.86 141.21 Q2132.43 139.382 2129.56 139.382 Q2126.11 139.382 2124.12 141.581 Q2122.13 143.78 2122.13 147.576 L2122.13 162.229 L2117.85 162.229 L2117.85 136.303 L2122.13 136.303 L2122.13 140.331 Q2123.66 137.993 2125.72 136.835 Q2127.8 135.678 2130.51 135.678 Q2134.98 135.678 2137.27 138.456 Q2139.56 141.21 2139.56 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2167.75 170.099 L2167.75 173.409 L2143.12 173.409 L2143.12 170.099 L2167.75 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2171.76 126.21 L2176.02 126.21 L2176.02 162.229 L2171.76 162.229 L2171.76 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2194.98 139.289 Q2191.55 139.289 2189.56 141.974 Q2187.57 144.636 2187.57 149.289 Q2187.57 153.942 2189.54 156.627 Q2191.53 159.289 2194.98 159.289 Q2198.38 159.289 2200.37 156.604 Q2202.36 153.918 2202.36 149.289 Q2202.36 144.682 2200.37 141.997 Q2198.38 139.289 2194.98 139.289 M2194.98 135.678 Q2200.53 135.678 2203.7 139.289 Q2206.87 142.9 2206.87 149.289 Q2206.87 155.655 2203.7 159.289 Q2200.53 162.9 2194.98 162.9 Q2189.4 162.9 2186.23 159.289 Q2183.08 155.655 2183.08 149.289 Q2183.08 142.9 2186.23 139.289 Q2189.4 135.678 2194.98 135.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2230.46 137.067 L2230.46 141.094 Q2228.66 140.169 2226.71 139.706 Q2224.77 139.243 2222.68 139.243 Q2219.51 139.243 2217.92 140.215 Q2216.34 141.187 2216.34 143.131 Q2216.34 144.613 2217.48 145.469 Q2218.61 146.303 2222.04 147.067 L2223.49 147.391 Q2228.03 148.363 2229.93 150.145 Q2231.85 151.905 2231.85 155.076 Q2231.85 158.687 2228.98 160.793 Q2226.13 162.9 2221.13 162.9 Q2219.05 162.9 2216.78 162.483 Q2214.54 162.09 2212.04 161.28 L2212.04 156.881 Q2214.4 158.108 2216.69 158.733 Q2218.98 159.335 2221.23 159.335 Q2224.23 159.335 2225.85 158.317 Q2227.48 157.275 2227.48 155.4 Q2227.48 153.664 2226.29 152.738 Q2225.14 151.812 2221.18 150.956 L2219.7 150.608 Q2215.74 149.775 2213.98 148.062 Q2212.22 146.326 2212.22 143.317 Q2212.22 139.659 2214.81 137.669 Q2217.41 135.678 2222.17 135.678 Q2224.54 135.678 2226.62 136.025 Q2228.7 136.372 2230.46 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2255.16 137.067 L2255.16 141.094 Q2253.35 140.169 2251.41 139.706 Q2249.47 139.243 2247.38 139.243 Q2244.21 139.243 2242.61 140.215 Q2241.04 141.187 2241.04 143.131 Q2241.04 144.613 2242.17 145.469 Q2243.31 146.303 2246.73 147.067 L2248.19 147.391 Q2252.73 148.363 2254.63 150.145 Q2256.55 151.905 2256.55 155.076 Q2256.55 158.687 2253.68 160.793 Q2250.83 162.9 2245.83 162.9 Q2243.75 162.9 2241.48 162.483 Q2239.23 162.09 2236.73 161.28 L2236.73 156.881 Q2239.1 158.108 2241.39 158.733 Q2243.68 159.335 2245.92 159.335 Q2248.93 159.335 2250.55 158.317 Q2252.17 157.275 2252.17 155.4 Q2252.17 153.664 2250.99 152.738 Q2249.84 151.812 2245.88 150.956 L2244.4 150.608 Q2240.44 149.775 2238.68 148.062 Q2236.92 146.326 2236.92 143.317 Q2236.92 139.659 2239.51 137.669 Q2242.1 135.678 2246.87 135.678 Q2249.23 135.678 2251.32 136.025 Q2253.4 136.372 2255.16 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip320)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1866.15,196.789 2010.46,196.789 "/>
<path clip-path="url(#clip320)" d="M2034.51 188.143 L2039.03 188.143 L2047.13 209.902 L2055.23 188.143 L2059.74 188.143 L2050.02 214.069 L2044.24 214.069 L2034.51 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2077.41 201.036 Q2072.24 201.036 2070.25 202.217 Q2068.26 203.397 2068.26 206.245 Q2068.26 208.513 2069.74 209.856 Q2071.25 211.175 2073.82 211.175 Q2077.36 211.175 2079.49 208.675 Q2081.64 206.152 2081.64 201.985 L2081.64 201.036 L2077.41 201.036 M2085.9 199.277 L2085.9 214.069 L2081.64 214.069 L2081.64 210.133 Q2080.18 212.495 2078.01 213.629 Q2075.83 214.74 2072.68 214.74 Q2068.7 214.74 2066.34 212.518 Q2064 210.272 2064 206.522 Q2064 202.147 2066.92 199.925 Q2069.86 197.703 2075.67 197.703 L2081.64 197.703 L2081.64 197.286 Q2081.64 194.347 2079.7 192.749 Q2077.78 191.129 2074.28 191.129 Q2072.06 191.129 2069.95 191.661 Q2067.85 192.194 2065.9 193.259 L2065.9 189.323 Q2068.24 188.421 2070.44 187.981 Q2072.64 187.518 2074.72 187.518 Q2080.35 187.518 2083.12 190.434 Q2085.9 193.351 2085.9 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2094.68 178.05 L2098.93 178.05 L2098.93 214.069 L2094.68 214.069 L2094.68 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2127.55 221.939 L2127.55 225.249 L2102.92 225.249 L2102.92 221.939 L2127.55 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2131.55 178.05 L2135.81 178.05 L2135.81 214.069 L2131.55 214.069 L2131.55 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2154.77 191.129 Q2151.34 191.129 2149.35 193.814 Q2147.36 196.476 2147.36 201.129 Q2147.36 205.782 2149.33 208.467 Q2151.32 211.129 2154.77 211.129 Q2158.17 211.129 2160.16 208.444 Q2162.15 205.758 2162.15 201.129 Q2162.15 196.522 2160.16 193.837 Q2158.17 191.129 2154.77 191.129 M2154.77 187.518 Q2160.32 187.518 2163.49 191.129 Q2166.67 194.74 2166.67 201.129 Q2166.67 207.495 2163.49 211.129 Q2160.32 214.74 2154.77 214.74 Q2149.19 214.74 2146.02 211.129 Q2142.87 207.495 2142.87 201.129 Q2142.87 194.74 2146.02 191.129 Q2149.19 187.518 2154.77 187.518 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2190.25 188.907 L2190.25 192.934 Q2188.45 192.009 2186.5 191.546 Q2184.56 191.083 2182.48 191.083 Q2179.3 191.083 2177.71 192.055 Q2176.13 193.027 2176.13 194.971 Q2176.13 196.453 2177.27 197.309 Q2178.4 198.143 2181.83 198.907 L2183.29 199.231 Q2187.82 200.203 2189.72 201.985 Q2191.64 203.745 2191.64 206.916 Q2191.64 210.527 2188.77 212.633 Q2185.92 214.74 2180.92 214.74 Q2178.84 214.74 2176.57 214.323 Q2174.33 213.93 2171.83 213.12 L2171.83 208.721 Q2174.19 209.948 2176.48 210.573 Q2178.77 211.175 2181.02 211.175 Q2184.03 211.175 2185.65 210.157 Q2187.27 209.115 2187.27 207.24 Q2187.27 205.504 2186.09 204.578 Q2184.93 203.652 2180.97 202.796 L2179.49 202.448 Q2175.53 201.615 2173.77 199.902 Q2172.01 198.166 2172.01 195.157 Q2172.01 191.499 2174.61 189.509 Q2177.2 187.518 2181.97 187.518 Q2184.33 187.518 2186.41 187.865 Q2188.49 188.212 2190.25 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip320)" d="M2214.95 188.907 L2214.95 192.934 Q2213.15 192.009 2211.2 191.546 Q2209.26 191.083 2207.17 191.083 Q2204 191.083 2202.41 192.055 Q2200.83 193.027 2200.83 194.971 Q2200.83 196.453 2201.97 197.309 Q2203.1 198.143 2206.53 198.907 L2207.98 199.231 Q2212.52 200.203 2214.42 201.985 Q2216.34 203.745 2216.34 206.916 Q2216.34 210.527 2213.47 212.633 Q2210.62 214.74 2205.62 214.74 Q2203.54 214.74 2201.27 214.323 Q2199.03 213.93 2196.53 213.12 L2196.53 208.721 Q2198.89 209.948 2201.18 210.573 Q2203.47 211.175 2205.72 211.175 Q2208.73 211.175 2210.35 210.157 Q2211.97 209.115 2211.97 207.24 Q2211.97 205.504 2210.79 204.578 Q2209.63 203.652 2205.67 202.796 L2204.19 202.448 Q2200.23 201.615 2198.47 199.902 Q2196.71 198.166 2196.71 195.157 Q2196.71 191.499 2199.3 189.509 Q2201.9 187.518 2206.67 187.518 Q2209.03 187.518 2211.11 187.865 Q2213.19 188.212 2214.95 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="nohighlight hljs">(WeightDecayScratch{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, @NamedTuple{lambda::Float64}}(Dense(200 =&gt; 1), (lambda = 3.0,)), (val_loss = [0.0720421476114861, 0.10422826972139035, 0.05659159672156999, 0.06927200854633188, 0.08334887807514543, 0.08471663080022433, 0.056548645586230295, 0.050229097932397465, 0.0728615335716399, 0.0886988647475379, 0.06967753364200492, 0.08053099017313697, 0.1301141913732636, 0.08189592795502351, 0.047899721839636644, 0.04776282464020387, 0.07244801286783671, 0.06684342421053806, 0.05908849218872609, 0.05658541332204206], val_acc = nothing))</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>Regularization is a common method for dealing with overfitting. Classical regularization techniques add a penalty term to the loss function (when training) to reduce the complexity of the learned model. One particular choice for keeping the model simple is using an <span>$\ell_2$</span> penalty. This leads to weight decay in the update steps of the minibatch stochastic gradient descent algorithm. In practice, the weight decay functionality is provided in optimizers from deep learning frameworks. Different sets of parameters can have different update behaviors within the same training loop.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Experiment with the value of <span>$\lambda$</span> in the estimation problem in this section. Plot training and validation accuracy as a function of <span>$\lambda$</span>. What do you observe?</li><li>Use a validation set to find the optimal value of <span>$\lambda$</span>. Is it really the optimal value? Does this matter?</li><li>What would the update equations look like if instead of <span>$\|\mathbf{w}\|^2$</span> we used <span>$\sum_i |w_i|$</span> as our penalty of choice (<span>$\ell_1$</span> regularization)?</li><li>We know that <span>$\|\mathbf{w}\|^2 = \mathbf{w}^\top \mathbf{w}$</span>. Can you find a similar equation for matrices (see the Frobenius norm in :numref:<code>subsec_lin-algebra-norms</code>)?</li><li>Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways might help us deal with overfitting?</li><li>In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via <span>$P(w \mid x) \propto P(x \mid w) P(w)$</span>. How can you identify <span>$P(w)$</span> with regularization?</li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../LNN_6/">« Generalization</a><a class="docs-footer-nextpage" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
