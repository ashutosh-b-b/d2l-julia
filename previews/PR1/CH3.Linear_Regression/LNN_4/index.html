<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear Regression Implementation from Scratch · d2l Julia</title><meta name="title" content="Linear Regression Implementation from Scratch · d2l Julia"/><meta property="og:title" content="Linear Regression Implementation from Scratch · d2l Julia"/><meta property="twitter:title" content="Linear Regression Implementation from Scratch · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../LNN_3/">Synthetic Regression Data</a></li><li class="is-active"><a class="tocitem" href>Linear Regression Implementation from Scratch</a><ul class="internal"><li><a class="tocitem" href="#Defining-the-Model"><span>Defining the Model</span></a></li><li><a class="tocitem" href="#Defining-the-Loss-Function"><span>Defining the Loss Function</span></a></li><li><a class="tocitem" href="#Defining-the-Optimization-Algorithm"><span>Defining the Optimization Algorithm</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../LNN_6/">Generalization</a></li><li><a class="tocitem" href="../LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Linear Neural Networks for Regression</a></li><li class="is-active"><a href>Linear Regression Implementation from Scratch</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear Regression Implementation from Scratch</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-Regression-Implementation-from-Scratch"><a class="docs-heading-anchor" href="#Linear-Regression-Implementation-from-Scratch">Linear Regression Implementation from Scratch</a><a id="Linear-Regression-Implementation-from-Scratch-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Regression-Implementation-from-Scratch" title="Permalink"></a></h1><p>:label:<code>sec_linear_scratch</code></p><p>We are now ready to work through  a fully functioning implementation  of linear regression.  In this section,  we will implement the entire method from scratch, including (i) the model; (ii) the loss function; (iii) a minibatch stochastic gradient descent optimizer; and (iv) the training function  that stitches all of these pieces together. Finally, we will run our synthetic data generator from :numref:<code>sec_synthetic-regression-data</code> and apply our model on the resulting dataset.  While modern deep learning frameworks  can automate nearly all of this work, implementing things from scratch is the only way to make sure that you really know what you are doing. Moreover, when it is time to customize models, defining our own layers or loss functions, understanding how things work under the hood will prove handy. In this section, we will rely only  on tensors and automatic differentiation. Later, we will introduce a more concise implementation, taking advantage of the bells and whistles of deep learning frameworks  while retaining the structure of what follows below.</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(&quot;../../d2lai&quot;)
using Flux, d2lai, Distributions</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/workspace/d2l-julia/d2lai`</code></pre><h2 id="Defining-the-Model"><a class="docs-heading-anchor" href="#Defining-the-Model">Defining the Model</a><a id="Defining-the-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-Model" title="Permalink"></a></h2><p>Before we can begin optimizing our model&#39;s parameters by minibatch SGD, we need to have some parameters in the first place. In the following we initialize weights by drawing random numbers from a normal distribution with mean 0 and a standard deviation of 0.01.  The magic number 0.01 often works well in practice,  but you can specify a different value  through the argument <code>sigma</code>. Moreover we set the bias to 0.</p><pre><code class="language-julia hljs">struct LinearRegressionScratch &lt;: d2lai.AbstractModel
    w::AbstractArray 
    b::AbstractArray 
    args::NamedTuple
end
function LinearRegressionScratch(num_inputs::Int64, lr, sigma = 0.01)
    w = rand(Normal(0, sigma), 1, num_inputs)
    b = zeros(1)
    args = (num_inputs = num_inputs, lr = lr, sigma = sigma)
    LinearRegressionScratch(w, b, args)
end
Flux.@layer LinearRegressionScratch trainable=(w, b)</code></pre><p>Next we must define our model, relating its input and parameters to its output. Using the same notation as :eqref:<code>eq_linreg-y-vec</code> for our linear model we simply take the matrix–vector product of the input features <span>$\mathbf{X}$</span>  and the model weights <span>$\mathbf{w}$</span>, and add the offset <span>$b$</span> to each example. The product <span>$\mathbf{Xw}$</span> is a vector and <span>$b$</span> is a scalar. Because of the broadcasting mechanism  (see :numref:<code>subsec_broadcasting</code>), when we add a vector and a scalar, the scalar is added to each component of the vector.</p><pre><code class="language-julia hljs">function d2lai.forward(lr::LinearRegressionScratch, x)
    return lr.w*x .+ lr.b
end</code></pre><h2 id="Defining-the-Loss-Function"><a class="docs-heading-anchor" href="#Defining-the-Loss-Function">Defining the Loss Function</a><a id="Defining-the-Loss-Function-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-Loss-Function" title="Permalink"></a></h2><p>Since updating our model requires taking the gradient of our loss function, we ought to (<strong>define the loss function first.</strong>) Here we use the squared loss function in :eqref:<code>eq_mse</code>. In the implementation, we need to transform the true value <code>y</code> into the predicted value&#39;s shape <code>y_hat</code>. The result returned by the following method will also have the same shape as <code>y_hat</code>.  We also return the averaged loss value among all examples in the minibatch.</p><pre><code class="language-julia hljs">function d2lai.loss(lr::LinearRegressionScratch, y_pred, y)
    l = 0.5*(y_pred - y).^2
    return mean(l)
end</code></pre><h2 id="Defining-the-Optimization-Algorithm"><a class="docs-heading-anchor" href="#Defining-the-Optimization-Algorithm">Defining the Optimization Algorithm</a><a id="Defining-the-Optimization-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-Optimization-Algorithm" title="Permalink"></a></h2><p>As discussed in :numref:<code>sec_linear_regression</code>, linear regression has a closed-form solution. However, our goal here is to illustrate  how to train more general neural networks, and that requires that we teach you  how to use minibatch SGD. Hence we will take this opportunity to introduce your first working example of SGD. At each step, using a minibatch  randomly drawn from our dataset, we estimate the gradient of the loss with respect to the parameters. Next, we update the parameters in the direction that may reduce the loss.</p><p>The following code applies the update,  given a set of parameters, a learning rate <code>lr</code>. Since our loss is computed as an average over the minibatch,  we do not need to adjust the learning rate against the batch size.  In later chapters we will investigate  how learning rates should be adjusted for very large minibatches as they arise  in distributed large-scale learning. For now, we can ignore this dependency.</p><pre><code class="language-julia hljs">struct SGD{P, L}
    params::P
    lr::L
end

function step!(sgd::SGD, model, grads)
    model.w .-= sgd.lr.*grads[model.w]
    model.b .-= sgd.lr.*grads[model.b]
end</code></pre><pre><code class="nohighlight hljs">step! (generic function with 1 method)</code></pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>Now that we have all of the parts in place (parameters, loss function, model, and optimizer), we are ready to implement the main training loop. It is crucial that you understand this code fully since you will employ similar training loops for every other deep learning model covered in this book. In each <em>epoch</em>, we iterate through  the entire training dataset,  passing once through every example (assuming that the number of examples  is divisible by the batch size).  In each <em>iteration</em>, we grab a minibatch of training examples, and compute its loss through the model&#39;s <code>training_step</code> method.  Then we compute the gradients with respect to each parameter.  Finally, we will call the optimization algorithm to update the model parameters.  In summary, we will execute the following loop:</p><ul><li>Initialize parameters <span>$(\mathbf{w}, b)$</span></li><li>Repeat until done<ul><li>Compute gradient <span>$\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$</span></li><li>Update parameters <span>$(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$</span></li></ul></li></ul><p>Recall that the synthetic regression dataset  that we generated in :numref:<span>$sec_synthetic-regression-data$</span>  does not provide a validation dataset.  In most cases, however,  we will want a validation dataset  to measure our model quality.  Here we pass the validation dataloader  once in each epoch to measure the model performance. Following our object-oriented design, the <code>prepare_batch</code> and <code>fit_epoch</code> methods are registered in the <code>d2l.Trainer</code> class (introduced in :numref:<code>oo-design-training</code>).</p><pre><code class="language-julia hljs">
function d2lai.fit_epoch(model::LinearRegressionScratch, opt; train_dataloader = nothing, val_dataloader = nothing, gradient_clip_val = 0.)
    losses = (train_losses = [], val_losses = [], val_acc = [])
    for batch in train_dataloader
        gs = gradient(Flux.Params([model.w, model.b])) do 
            training_step(trainer.model, batch)
        end 
        step!(opt, model, gs)
        train_loss = training_step(trainer.model, batch)
        push!(losses.train_losses, train_loss)
    end
    for batch in val_dataloader
        loss, _ = validation_step(trainer.model, batch)
        push!(losses.val_losses , loss)
    end
    return losses
end</code></pre><p>We are almost ready to train the model, but first we need some training data. Here we use the <code>SyntheticRegressionData</code> class  and pass in some ground truth parameters. Then we train our model with  the learning rate <code>lr=0.03</code>  and set <code>max_epochs=3</code>.  Note that in general, both the number of epochs  and the learning rate are hyperparameters. In general, setting hyperparameters is tricky and we will usually want to use a three-way split, one set for training,  a second for hyperparameter selection, and the third reserved for the final evaluation. We elide these details for now but will revise them later.</p><pre><code class="language-julia hljs">model = LinearRegressionScratch(2, 0.03)
sgd = SGD(nothing, 0.03)
data = SyntheticRegressionData([2 -3.4], 4.3)

trainer = Trainer(model, data, sgd; max_epochs = 5)

d2lai.fit(trainer)</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 4.159377763440317, Val Loss: 1.6136084418676728
    [ Info: Train Loss: 0.5253720630174942, Val Loss: 0.20050404086196566
    [ Info: Train Loss: 0.06684764937174377, Val Loss: 0.024274566118867594
    [ Info: Train Loss: 0.008670093066378354, Val Loss: 0.002744558889707349
    [ Info: Train Loss: 0.0011937151261237698, Val Loss: 0.0002836587166744213</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip600">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip600)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip601">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip600)" d="M187.803 1423.18 L2352.76 1423.18 L2352.76 47.2441 L187.803 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip602">
    <rect x="187" y="47" width="2166" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="249.075,1423.18 249.075,47.2441 "/>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="759.678,1423.18 759.678,47.2441 "/>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1270.28,1423.18 1270.28,47.2441 "/>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1780.88,1423.18 1780.88,47.2441 "/>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2291.48,1423.18 2291.48,47.2441 "/>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="187.803,1006.62 2352.76,1006.62 "/>
<polyline clip-path="url(#clip602)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="187.803,364.317 2352.76,364.317 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="187.803,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.075,1423.18 249.075,1404.28 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="759.678,1423.18 759.678,1404.28 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1270.28,1423.18 1270.28,1404.28 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1780.88,1423.18 1780.88,1404.28 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2291.48,1423.18 2291.48,1404.28 "/>
<path clip-path="url(#clip600)" d="M239.457 1481.64 L247.096 1481.64 L247.096 1455.28 L238.786 1456.95 L238.786 1452.69 L247.05 1451.02 L251.726 1451.02 L251.726 1481.64 L259.365 1481.64 L259.365 1485.58 L239.457 1485.58 L239.457 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M754.33 1481.64 L770.65 1481.64 L770.65 1485.58 L748.705 1485.58 L748.705 1481.64 Q751.367 1478.89 755.951 1474.26 Q760.557 1469.61 761.738 1468.27 Q763.983 1465.74 764.863 1464.01 Q765.765 1462.25 765.765 1460.56 Q765.765 1457.8 763.821 1456.07 Q761.9 1454.33 758.798 1454.33 Q756.599 1454.33 754.145 1455.09 Q751.715 1455.86 748.937 1457.41 L748.937 1452.69 Q751.761 1451.55 754.215 1450.97 Q756.668 1450.39 758.705 1450.39 Q764.076 1450.39 767.27 1453.08 Q770.464 1455.77 770.464 1460.26 Q770.464 1462.39 769.654 1464.31 Q768.867 1466.2 766.761 1468.8 Q766.182 1469.47 763.08 1472.69 Q759.978 1475.88 754.33 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1274.53 1466.95 Q1277.88 1467.66 1279.76 1469.93 Q1281.66 1472.2 1281.66 1475.53 Q1281.66 1480.65 1278.14 1483.45 Q1274.62 1486.25 1268.14 1486.25 Q1265.96 1486.25 1263.65 1485.81 Q1261.36 1485.39 1258.9 1484.54 L1258.9 1480.02 Q1260.85 1481.16 1263.16 1481.74 Q1265.48 1482.32 1268 1482.32 Q1272.4 1482.32 1274.69 1480.58 Q1277 1478.84 1277 1475.53 Q1277 1472.48 1274.85 1470.77 Q1272.72 1469.03 1268.9 1469.03 L1264.87 1469.03 L1264.87 1465.19 L1269.09 1465.19 Q1272.54 1465.19 1274.37 1463.82 Q1276.19 1462.43 1276.19 1459.84 Q1276.19 1457.18 1274.3 1455.77 Q1272.42 1454.33 1268.9 1454.33 Q1266.98 1454.33 1264.78 1454.75 Q1262.58 1455.16 1259.94 1456.04 L1259.94 1451.88 Q1262.61 1451.14 1264.92 1450.77 Q1267.26 1450.39 1269.32 1450.39 Q1274.64 1450.39 1277.74 1452.83 Q1280.85 1455.23 1280.85 1459.35 Q1280.85 1462.22 1279.2 1464.21 Q1277.56 1466.18 1274.53 1466.95 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1783.89 1455.09 L1772.09 1473.54 L1783.89 1473.54 L1783.89 1455.09 M1782.66 1451.02 L1788.54 1451.02 L1788.54 1473.54 L1793.47 1473.54 L1793.47 1477.43 L1788.54 1477.43 L1788.54 1485.58 L1783.89 1485.58 L1783.89 1477.43 L1768.29 1477.43 L1768.29 1472.92 L1782.66 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2281.76 1451.02 L2300.12 1451.02 L2300.12 1454.96 L2286.04 1454.96 L2286.04 1463.43 Q2287.06 1463.08 2288.08 1462.92 Q2289.1 1462.73 2290.12 1462.73 Q2295.9 1462.73 2299.28 1465.9 Q2302.66 1469.08 2302.66 1474.49 Q2302.66 1480.07 2299.19 1483.17 Q2295.72 1486.25 2289.4 1486.25 Q2287.22 1486.25 2284.96 1485.88 Q2282.71 1485.51 2280.3 1484.77 L2280.3 1480.07 Q2282.39 1481.2 2284.61 1481.76 Q2286.83 1482.32 2289.31 1482.32 Q2293.31 1482.32 2295.65 1480.21 Q2297.99 1478.1 2297.99 1474.49 Q2297.99 1470.88 2295.65 1468.77 Q2293.31 1466.67 2289.31 1466.67 Q2287.43 1466.67 2285.56 1467.08 Q2283.71 1467.5 2281.76 1468.38 L2281.76 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1190.47 1548.76 L1190.47 1551.62 L1163.54 1551.62 Q1163.92 1557.67 1167.17 1560.85 Q1170.45 1564 1176.27 1564 Q1179.65 1564 1182.8 1563.17 Q1185.98 1562.35 1189.1 1560.69 L1189.1 1566.23 Q1185.95 1567.57 1182.64 1568.27 Q1179.33 1568.97 1175.92 1568.97 Q1167.39 1568.97 1162.4 1564 Q1157.43 1559.04 1157.43 1550.57 Q1157.43 1541.82 1162.14 1536.69 Q1166.88 1531.54 1174.91 1531.54 Q1182.1 1531.54 1186.27 1536.18 Q1190.47 1540.8 1190.47 1548.76 M1184.61 1547.04 Q1184.55 1542.23 1181.91 1539.37 Q1179.3 1536.5 1174.97 1536.5 Q1170.07 1536.5 1167.11 1539.27 Q1164.18 1542.04 1163.73 1547.07 L1184.61 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1205.75 1562.7 L1205.75 1581.6 L1199.86 1581.6 L1199.86 1532.4 L1205.75 1532.4 L1205.75 1537.81 Q1207.59 1534.62 1210.39 1533.1 Q1213.23 1531.54 1217.14 1531.54 Q1223.63 1531.54 1227.68 1536.69 Q1231.75 1541.85 1231.75 1550.25 Q1231.75 1558.65 1227.68 1563.81 Q1223.63 1568.97 1217.14 1568.97 Q1213.23 1568.97 1210.39 1567.44 Q1207.59 1565.88 1205.75 1562.7 M1225.67 1550.25 Q1225.67 1543.79 1223 1540.13 Q1220.36 1536.44 1215.71 1536.44 Q1211.06 1536.44 1208.39 1540.13 Q1205.75 1543.79 1205.75 1550.25 Q1205.75 1556.71 1208.39 1560.4 Q1211.06 1564.07 1215.71 1564.07 Q1220.36 1564.07 1223 1560.4 Q1225.67 1556.71 1225.67 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1255.27 1536.5 Q1250.56 1536.5 1247.82 1540.19 Q1245.09 1543.85 1245.09 1550.25 Q1245.09 1556.65 1247.79 1560.34 Q1250.53 1564 1255.27 1564 Q1259.95 1564 1262.69 1560.31 Q1265.43 1556.62 1265.43 1550.25 Q1265.43 1543.92 1262.69 1540.23 Q1259.95 1536.5 1255.27 1536.5 M1255.27 1531.54 Q1262.91 1531.54 1267.27 1536.5 Q1271.63 1541.47 1271.63 1550.25 Q1271.63 1559 1267.27 1564 Q1262.91 1568.97 1255.27 1568.97 Q1247.6 1568.97 1243.24 1564 Q1238.91 1559 1238.91 1550.25 Q1238.91 1541.47 1243.24 1536.5 Q1247.6 1531.54 1255.27 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1306.99 1533.76 L1306.99 1539.24 Q1304.51 1537.87 1302 1537.2 Q1299.51 1536.5 1296.97 1536.5 Q1291.27 1536.5 1288.12 1540.13 Q1284.97 1543.73 1284.97 1550.25 Q1284.97 1556.78 1288.12 1560.4 Q1291.27 1564 1296.97 1564 Q1299.51 1564 1302 1563.33 Q1304.51 1562.63 1306.99 1561.26 L1306.99 1566.68 Q1304.54 1567.82 1301.9 1568.39 Q1299.29 1568.97 1296.33 1568.97 Q1288.28 1568.97 1283.54 1563.91 Q1278.79 1558.85 1278.79 1550.25 Q1278.79 1541.53 1283.57 1536.53 Q1288.37 1531.54 1296.71 1531.54 Q1299.42 1531.54 1302 1532.11 Q1304.57 1532.65 1306.99 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1346.81 1546.53 L1346.81 1568.04 L1340.95 1568.04 L1340.95 1546.72 Q1340.95 1541.66 1338.98 1539.14 Q1337.01 1536.63 1333.06 1536.63 Q1328.32 1536.63 1325.58 1539.65 Q1322.84 1542.68 1322.84 1547.9 L1322.84 1568.04 L1316.96 1568.04 L1316.96 1518.52 L1322.84 1518.52 L1322.84 1537.93 Q1324.95 1534.72 1327.78 1533.13 Q1330.64 1531.54 1334.37 1531.54 Q1340.51 1531.54 1343.66 1535.36 Q1346.81 1539.14 1346.81 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M1381.22 1533.45 L1381.22 1538.98 Q1378.74 1537.71 1376.06 1537.07 Q1373.39 1536.44 1370.52 1536.44 Q1366.16 1536.44 1363.97 1537.77 Q1361.8 1539.11 1361.8 1541.79 Q1361.8 1543.82 1363.36 1545 Q1364.92 1546.15 1369.63 1547.2 L1371.64 1547.64 Q1377.88 1548.98 1380.49 1551.43 Q1383.13 1553.85 1383.13 1558.21 Q1383.13 1563.17 1379.18 1566.07 Q1375.27 1568.97 1368.39 1568.97 Q1365.53 1568.97 1362.41 1568.39 Q1359.32 1567.85 1355.88 1566.74 L1355.88 1560.69 Q1359.13 1562.38 1362.28 1563.24 Q1365.43 1564.07 1368.52 1564.07 Q1372.66 1564.07 1374.88 1562.66 Q1377.11 1561.23 1377.11 1558.65 Q1377.11 1556.27 1375.49 1554.99 Q1373.9 1553.72 1368.45 1552.54 L1366.42 1552.07 Q1360.97 1550.92 1358.56 1548.56 Q1356.14 1546.18 1356.14 1542.04 Q1356.14 1537.01 1359.7 1534.27 Q1363.27 1531.54 1369.82 1531.54 Q1373.07 1531.54 1375.93 1532.01 Q1378.8 1532.49 1381.22 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="187.803,1423.18 187.803,47.2441 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="187.803,1006.62 206.701,1006.62 "/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="187.803,364.317 206.701,364.317 "/>
<path clip-path="url(#clip600)" d="M51.6634 1026.41 L59.3023 1026.41 L59.3023 1000.04 L50.9921 1001.71 L50.9921 997.45 L59.256 995.783 L63.9319 995.783 L63.9319 1026.41 L71.5707 1026.41 L71.5707 1030.34 L51.6634 1030.34 L51.6634 1026.41 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M91.0151 998.862 Q87.404 998.862 85.5753 1002.43 Q83.7697 1005.97 83.7697 1013.1 Q83.7697 1020.2 85.5753 1023.77 Q87.404 1027.31 91.0151 1027.31 Q94.6493 1027.31 96.4548 1023.77 Q98.2835 1020.2 98.2835 1013.1 Q98.2835 1005.97 96.4548 1002.43 Q94.6493 998.862 91.0151 998.862 M91.0151 995.158 Q96.8252 995.158 99.8808 999.765 Q102.959 1004.35 102.959 1013.1 Q102.959 1021.82 99.8808 1026.43 Q96.8252 1031.01 91.0151 1031.01 Q85.2049 1031.01 82.1262 1026.43 Q79.0707 1021.82 79.0707 1013.1 Q79.0707 1004.35 82.1262 999.765 Q85.2049 995.158 91.0151 995.158 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M102.959 989.259 L127.071 989.259 L127.071 992.457 L102.959 992.457 L102.959 989.259 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M138.544 999.735 L151.803 999.735 L151.803 1002.93 L133.973 1002.93 L133.973 999.735 Q136.136 997.497 139.86 993.736 Q143.603 989.955 144.562 988.865 Q146.387 986.814 147.101 985.404 Q147.835 983.974 147.835 982.602 Q147.835 980.363 146.255 978.953 Q144.694 977.542 142.174 977.542 Q140.387 977.542 138.393 978.163 Q136.418 978.784 134.162 980.044 L134.162 976.207 Q136.456 975.285 138.45 974.815 Q140.443 974.345 142.098 974.345 Q146.462 974.345 149.057 976.527 Q151.653 978.708 151.653 982.357 Q151.653 984.087 150.994 985.648 Q150.355 987.191 148.644 989.297 Q148.173 989.843 145.653 992.457 Q143.133 995.052 138.544 999.735 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M81.0976 384.11 L88.7364 384.11 L88.7364 357.744 L80.4263 359.411 L80.4263 355.152 L88.6901 353.485 L93.366 353.485 L93.366 384.11 L101.005 384.11 L101.005 388.045 L81.0976 388.045 L81.0976 384.11 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M120.449 356.564 Q116.838 356.564 115.009 360.128 Q113.204 363.67 113.204 370.8 Q113.204 377.906 115.009 381.471 Q116.838 385.013 120.449 385.013 Q124.083 385.013 125.889 381.471 Q127.718 377.906 127.718 370.8 Q127.718 363.67 125.889 360.128 Q124.083 356.564 120.449 356.564 M120.449 352.86 Q126.259 352.86 129.315 357.466 Q132.394 362.05 132.394 370.8 Q132.394 379.527 129.315 384.133 Q126.259 388.716 120.449 388.716 Q114.639 388.716 111.56 384.133 Q108.505 379.527 108.505 370.8 Q108.505 362.05 111.56 357.466 Q114.639 352.86 120.449 352.86 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M142.098 335.056 Q139.164 335.056 137.679 337.952 Q136.212 340.83 136.212 346.623 Q136.212 352.397 137.679 355.293 Q139.164 358.171 142.098 358.171 Q145.051 358.171 146.518 355.293 Q148.004 352.397 148.004 346.623 Q148.004 340.83 146.518 337.952 Q145.051 335.056 142.098 335.056 M142.098 332.047 Q146.819 332.047 149.302 335.79 Q151.803 339.513 151.803 346.623 Q151.803 353.713 149.302 357.456 Q146.819 361.18 142.098 361.18 Q137.378 361.18 134.876 357.456 Q132.394 353.713 132.394 346.623 Q132.394 339.513 134.876 335.79 Q137.378 332.047 142.098 332.047 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip602)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.075,86.1857 759.678,371.324 1270.28,655.851 1780.88,939.466 2291.48,1220.14 "/>
<polyline clip-path="url(#clip602)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="249.075,258.499 759.678,543.109 1270.28,827.021 1780.88,1109.47 2291.48,1384.24 "/>
<path clip-path="url(#clip600)" d="M1842.06 248.629 L2280.59 248.629 L2280.59 93.1086 L1842.06 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip600)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1842.06,248.629 2280.59,248.629 2280.59,93.1086 1842.06,93.1086 1842.06,248.629 "/>
<polyline clip-path="url(#clip600)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1866.11,144.949 2010.44,144.949 "/>
<path clip-path="url(#clip600)" d="M2041.91 128.942 L2041.91 136.303 L2050.68 136.303 L2050.68 139.613 L2041.91 139.613 L2041.91 153.687 Q2041.91 156.858 2042.76 157.761 Q2043.64 158.664 2046.31 158.664 L2050.68 158.664 L2050.68 162.229 L2046.31 162.229 Q2041.37 162.229 2039.5 160.4 Q2037.62 158.548 2037.62 153.687 L2037.62 139.613 L2034.5 139.613 L2034.5 136.303 L2037.62 136.303 L2037.62 128.942 L2041.91 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2071.31 140.284 Q2070.59 139.868 2069.73 139.682 Q2068.9 139.474 2067.88 139.474 Q2064.27 139.474 2062.32 141.835 Q2060.4 144.173 2060.4 148.571 L2060.4 162.229 L2056.12 162.229 L2056.12 136.303 L2060.4 136.303 L2060.4 140.331 Q2061.75 137.969 2063.9 136.835 Q2066.05 135.678 2069.13 135.678 Q2069.57 135.678 2070.1 135.747 Q2070.63 135.794 2071.28 135.909 L2071.31 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2087.56 149.196 Q2082.39 149.196 2080.4 150.377 Q2078.41 151.557 2078.41 154.405 Q2078.41 156.673 2079.89 158.016 Q2081.4 159.335 2083.97 159.335 Q2087.51 159.335 2089.64 156.835 Q2091.79 154.312 2091.79 150.145 L2091.79 149.196 L2087.56 149.196 M2096.05 147.437 L2096.05 162.229 L2091.79 162.229 L2091.79 158.293 Q2090.33 160.655 2088.16 161.789 Q2085.98 162.9 2082.83 162.9 Q2078.85 162.9 2076.49 160.678 Q2074.15 158.432 2074.15 154.682 Q2074.15 150.307 2077.07 148.085 Q2080.01 145.863 2085.82 145.863 L2091.79 145.863 L2091.79 145.446 Q2091.79 142.507 2089.85 140.909 Q2087.93 139.289 2084.43 139.289 Q2082.21 139.289 2080.1 139.821 Q2078 140.354 2076.05 141.419 L2076.05 137.483 Q2078.39 136.581 2080.59 136.141 Q2082.79 135.678 2084.87 135.678 Q2090.5 135.678 2093.27 138.594 Q2096.05 141.511 2096.05 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2104.82 136.303 L2109.08 136.303 L2109.08 162.229 L2104.82 162.229 L2104.82 136.303 M2104.82 126.21 L2109.08 126.21 L2109.08 131.604 L2104.82 131.604 L2104.82 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2139.55 146.581 L2139.55 162.229 L2135.29 162.229 L2135.29 146.719 Q2135.29 143.039 2133.85 141.21 Q2132.42 139.382 2129.55 139.382 Q2126.1 139.382 2124.11 141.581 Q2122.12 143.78 2122.12 147.576 L2122.12 162.229 L2117.83 162.229 L2117.83 136.303 L2122.12 136.303 L2122.12 140.331 Q2123.64 137.993 2125.7 136.835 Q2127.79 135.678 2130.49 135.678 Q2134.96 135.678 2137.25 138.456 Q2139.55 141.21 2139.55 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2167.74 170.099 L2167.74 173.409 L2143.11 173.409 L2143.11 170.099 L2167.74 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2171.74 126.21 L2176 126.21 L2176 162.229 L2171.74 162.229 L2171.74 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2194.96 139.289 Q2191.54 139.289 2189.55 141.974 Q2187.55 144.636 2187.55 149.289 Q2187.55 153.942 2189.52 156.627 Q2191.51 159.289 2194.96 159.289 Q2198.36 159.289 2200.36 156.604 Q2202.35 153.918 2202.35 149.289 Q2202.35 144.682 2200.36 141.997 Q2198.36 139.289 2194.96 139.289 M2194.96 135.678 Q2200.52 135.678 2203.69 139.289 Q2206.86 142.9 2206.86 149.289 Q2206.86 155.655 2203.69 159.289 Q2200.52 162.9 2194.96 162.9 Q2189.38 162.9 2186.21 159.289 Q2183.06 155.655 2183.06 149.289 Q2183.06 142.9 2186.21 139.289 Q2189.38 135.678 2194.96 135.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2230.45 137.067 L2230.45 141.094 Q2228.64 140.169 2226.7 139.706 Q2224.75 139.243 2222.67 139.243 Q2219.5 139.243 2217.9 140.215 Q2216.33 141.187 2216.33 143.131 Q2216.33 144.613 2217.46 145.469 Q2218.6 146.303 2222.02 147.067 L2223.48 147.391 Q2228.02 148.363 2229.92 150.145 Q2231.84 151.905 2231.84 155.076 Q2231.84 158.687 2228.97 160.793 Q2226.12 162.9 2221.12 162.9 Q2219.04 162.9 2216.77 162.483 Q2214.52 162.09 2212.02 161.28 L2212.02 156.881 Q2214.38 158.108 2216.67 158.733 Q2218.97 159.335 2221.21 159.335 Q2224.22 159.335 2225.84 158.317 Q2227.46 157.275 2227.46 155.4 Q2227.46 153.664 2226.28 152.738 Q2225.12 151.812 2221.17 150.956 L2219.68 150.608 Q2215.73 149.775 2213.97 148.062 Q2212.21 146.326 2212.21 143.317 Q2212.21 139.659 2214.8 137.669 Q2217.39 135.678 2222.16 135.678 Q2224.52 135.678 2226.61 136.025 Q2228.69 136.372 2230.45 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2255.15 137.067 L2255.15 141.094 Q2253.34 140.169 2251.4 139.706 Q2249.45 139.243 2247.37 139.243 Q2244.2 139.243 2242.6 140.215 Q2241.03 141.187 2241.03 143.131 Q2241.03 144.613 2242.16 145.469 Q2243.3 146.303 2246.72 147.067 L2248.18 147.391 Q2252.72 148.363 2254.61 150.145 Q2256.54 151.905 2256.54 155.076 Q2256.54 158.687 2253.67 160.793 Q2250.82 162.9 2245.82 162.9 Q2243.73 162.9 2241.47 162.483 Q2239.22 162.09 2236.72 161.28 L2236.72 156.881 Q2239.08 158.108 2241.37 158.733 Q2243.67 159.335 2245.91 159.335 Q2248.92 159.335 2250.54 158.317 Q2252.16 157.275 2252.16 155.4 Q2252.16 153.664 2250.98 152.738 Q2249.82 151.812 2245.86 150.956 L2244.38 150.608 Q2240.42 149.775 2238.67 148.062 Q2236.91 146.326 2236.91 143.317 Q2236.91 139.659 2239.5 137.669 Q2242.09 135.678 2246.86 135.678 Q2249.22 135.678 2251.3 136.025 Q2253.39 136.372 2255.15 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip600)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1866.11,196.789 2010.44,196.789 "/>
<path clip-path="url(#clip600)" d="M2034.5 188.143 L2039.01 188.143 L2047.12 209.902 L2055.22 188.143 L2059.73 188.143 L2050.01 214.069 L2044.22 214.069 L2034.5 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2077.39 201.036 Q2072.23 201.036 2070.24 202.217 Q2068.25 203.397 2068.25 206.245 Q2068.25 208.513 2069.73 209.856 Q2071.24 211.175 2073.81 211.175 Q2077.35 211.175 2079.48 208.675 Q2081.63 206.152 2081.63 201.985 L2081.63 201.036 L2077.39 201.036 M2085.89 199.277 L2085.89 214.069 L2081.63 214.069 L2081.63 210.133 Q2080.17 212.495 2078 213.629 Q2075.82 214.74 2072.67 214.74 Q2068.69 214.74 2066.33 212.518 Q2063.99 210.272 2063.99 206.522 Q2063.99 202.147 2066.91 199.925 Q2069.85 197.703 2075.66 197.703 L2081.63 197.703 L2081.63 197.286 Q2081.63 194.347 2079.68 192.749 Q2077.76 191.129 2074.27 191.129 Q2072.05 191.129 2069.94 191.661 Q2067.83 192.194 2065.89 193.259 L2065.89 189.323 Q2068.23 188.421 2070.43 187.981 Q2072.62 187.518 2074.71 187.518 Q2080.33 187.518 2083.11 190.434 Q2085.89 193.351 2085.89 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2094.66 178.05 L2098.92 178.05 L2098.92 214.069 L2094.66 214.069 L2094.66 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2127.53 221.939 L2127.53 225.249 L2102.9 225.249 L2102.9 221.939 L2127.53 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2131.54 178.05 L2135.8 178.05 L2135.8 214.069 L2131.54 214.069 L2131.54 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2154.75 191.129 Q2151.33 191.129 2149.34 193.814 Q2147.35 196.476 2147.35 201.129 Q2147.35 205.782 2149.31 208.467 Q2151.3 211.129 2154.75 211.129 Q2158.16 211.129 2160.15 208.444 Q2162.14 205.758 2162.14 201.129 Q2162.14 196.522 2160.15 193.837 Q2158.16 191.129 2154.75 191.129 M2154.75 187.518 Q2160.31 187.518 2163.48 191.129 Q2166.65 194.74 2166.65 201.129 Q2166.65 207.495 2163.48 211.129 Q2160.31 214.74 2154.75 214.74 Q2149.18 214.74 2146 211.129 Q2142.86 207.495 2142.86 201.129 Q2142.86 194.74 2146 191.129 Q2149.18 187.518 2154.75 187.518 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2190.24 188.907 L2190.24 192.934 Q2188.43 192.009 2186.49 191.546 Q2184.55 191.083 2182.46 191.083 Q2179.29 191.083 2177.69 192.055 Q2176.12 193.027 2176.12 194.971 Q2176.12 196.453 2177.25 197.309 Q2178.39 198.143 2181.81 198.907 L2183.27 199.231 Q2187.81 200.203 2189.71 201.985 Q2191.63 203.745 2191.63 206.916 Q2191.63 210.527 2188.76 212.633 Q2185.91 214.74 2180.91 214.74 Q2178.83 214.74 2176.56 214.323 Q2174.31 213.93 2171.81 213.12 L2171.81 208.721 Q2174.18 209.948 2176.47 210.573 Q2178.76 211.175 2181 211.175 Q2184.01 211.175 2185.63 210.157 Q2187.25 209.115 2187.25 207.24 Q2187.25 205.504 2186.07 204.578 Q2184.92 203.652 2180.96 202.796 L2179.48 202.448 Q2175.52 201.615 2173.76 199.902 Q2172 198.166 2172 195.157 Q2172 191.499 2174.59 189.509 Q2177.18 187.518 2181.95 187.518 Q2184.31 187.518 2186.4 187.865 Q2188.48 188.212 2190.24 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip600)" d="M2214.94 188.907 L2214.94 192.934 Q2213.13 192.009 2211.19 191.546 Q2209.24 191.083 2207.16 191.083 Q2203.99 191.083 2202.39 192.055 Q2200.82 193.027 2200.82 194.971 Q2200.82 196.453 2201.95 197.309 Q2203.09 198.143 2206.51 198.907 L2207.97 199.231 Q2212.51 200.203 2214.41 201.985 Q2216.33 203.745 2216.33 206.916 Q2216.33 210.527 2213.46 212.633 Q2210.61 214.74 2205.61 214.74 Q2203.53 214.74 2201.26 214.323 Q2199.01 213.93 2196.51 213.12 L2196.51 208.721 Q2198.87 209.948 2201.17 210.573 Q2203.46 211.175 2205.7 211.175 Q2208.71 211.175 2210.33 210.157 Q2211.95 209.115 2211.95 207.24 Q2211.95 205.504 2210.77 204.578 Q2209.61 203.652 2205.66 202.796 L2204.17 202.448 Q2200.22 201.615 2198.46 199.902 Q2196.7 198.166 2196.7 195.157 Q2196.7 191.499 2199.29 189.509 Q2201.88 187.518 2206.65 187.518 Q2209.01 187.518 2211.1 187.865 Q2213.18 188.212 2214.94 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="nohighlight hljs">(LinearRegressionScratch([1.992836720414413 -3.37799751615318], [4.2724079808329645], (num_inputs = 2, lr = 0.03, sigma = 0.01)), (val_loss = [0.0008190482770071592, 0.0004729000362407759, 0.0008228406226611303, 0.0009655684573209392, 0.0003667007325329076, 0.000566660471668708, 0.0007715233407286309, 0.0005612130406624064, 0.0005501054302935709, 0.0007714316856376472  …  0.0008044788837960324, 0.0007816813786489018, 0.0005494918163793845, 0.0006066466659859343, 0.0007979676460930835, 0.0007077506486962472, 0.0007604577022060139, 0.0006439598375716324, 0.0005397228479022556, 0.0002836587166744213], val_acc = nothing))</code></pre><p>Because we synthesized the dataset ourselves, we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other.</p><pre><code class="language-julia hljs">@show model.w
@show model.b </code></pre><pre><code class="nohighlight hljs">model.w = [1.992836720414413 -3.37799751615318]
model.b = [4.2724079808329645]





1-element Vector{Float64}:
 4.2724079808329645</code></pre><p>We should not take the ability to exactly recover  the ground truth parameters for granted. In general, for deep models unique solutions for the parameters do not exist, and even for linear models, exactly recovering the parameters is only possible when no feature  is linearly dependent on the others. However, in machine learning,  we are often less concerned with recovering true underlying parameters, but rather with parameters  that lead to highly accurate prediction :cite:<code>Vapnik.1992</code>. Fortunately, even on difficult optimization problems, stochastic gradient descent can often find remarkably good solutions, owing partly to the fact that, for deep networks, there exist many configurations of the parameters that lead to highly accurate prediction.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>In this section, we took a significant step  towards designing deep learning systems  by implementing a fully functional  neural network model and training loop. In this process, we built a data loader,  a model, a loss function, an optimization procedure, and a visualization and monitoring tool.  We did this by composing a Python object  that contains all relevant components for training a model.  While this is not yet a professional-grade implementation it is perfectly functional and code like this  could already help you to solve small problems quickly. In the coming sections, we will see how to do this both <em>more concisely</em> (avoiding boilerplate code) and <em>more efficiently</em> (using our GPUs to their full potential).</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>What would happen if we were to initialize the weights to zero. Would the algorithm still work? What if we initialized the parameters with variance <span>$1000$</span> rather than <span>$0.01$</span>?</li><li>Assume that you are <a href="https://en.wikipedia.org/wiki/Georg_Ohm">Georg Simon Ohm</a> trying to come up with a model for resistance that relates voltage and current. Can you use automatic differentiation to learn the parameters of your model?</li><li>Can you use <a href="https://en.wikipedia.org/wiki/Planck%27s_law">Planck&#39;s Law</a> to determine the temperature of an object using spectral energy density? For reference, the spectral density <span>$B$</span> of radiation emanating from a black body is <span>$B(\lambda, T) = \frac{2 hc^2}{\lambda^5} \cdot \left(\exp \frac{h c}{\lambda k T} - 1\right)^{-1}$</span>. Here <span>$\lambda$</span> is the wavelength, <span>$T$</span> is the temperature, <span>$c$</span> is the speed of light, <span>$h$</span> is Planck&#39;s constant, and <span>$k$</span> is the Boltzmann constant. You measure the energy for different wavelengths <span>$\lambda$</span> and you now need to fit the spectral density curve to Planck&#39;s law.</li><li>What are the problems you might encounter if you wanted to compute the second derivatives of the loss? How would you fix them?</li><li>Why is the <code>reshape</code> method needed in the <code>loss</code> function?</li><li>Experiment using different learning rates to find out how quickly the loss function value drops. Can you reduce the error by increasing the number of epochs of training?</li><li>If the number of examples cannot be divided by the batch size, what happens to <code>data_iter</code> at the end of an epoch?</li><li>Try implementing a different loss function, such as the absolute value loss <code>(y_hat - d2l.reshape(y, y_hat.shape)).abs().sum()</code>.<ol><li>Check what happens for regular data.</li><li>Check whether there is a difference in behavior if you actively perturb some entries, such as <span>$y_5 = 10000$</span>, of <span>$\mathbf{y}$</span>.</li><li>Can you think of a cheap solution for combining the best aspects of squared loss and absolute value loss? Hint: how can you avoid really large gradient values?</li></ol></li><li>Why do we need to reshuffle the dataset? Can you design a case where a maliciously constructed dataset would break the optimization algorithm otherwise?</li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../LNN_3/">« Synthetic Regression Data</a><a class="docs-footer-nextpage" href="../LNN_5/">Concise Implementation of Linear Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
