<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Forward Propagation, Backward Propagation, and Computational Graphs · d2l Julia</title><meta name="title" content="Forward Propagation, Backward Propagation, and Computational Graphs · d2l Julia"/><meta property="og:title" content="Forward Propagation, Backward Propagation, and Computational Graphs · d2l Julia"/><meta property="twitter:title" content="Forward Propagation, Backward Propagation, and Computational Graphs · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../MLP_2/">Implementation of Multilayer Perceptrons</a></li><li class="is-active"><a class="tocitem" href>Forward Propagation, Backward Propagation, and Computational Graphs</a><ul class="internal"><li><a class="tocitem" href="#Forward-Propagation"><span>Forward Propagation</span></a></li><li><a class="tocitem" href="#Computational-Graph-of-Forward-Propagation"><span>Computational Graph of Forward Propagation</span></a></li><li><a class="tocitem" href="#Backpropagation"><span>Backpropagation</span></a></li><li><a class="tocitem" href="#Training-Neural-Networks"><span>Training Neural Networks</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Multilayer Perceptron</a></li><li class="is-active"><a href>Forward Propagation, Backward Propagation, and Computational Graphs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Forward Propagation, Backward Propagation, and Computational Graphs</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Forward-Propagation,-Backward-Propagation,-and-Computational-Graphs"><a class="docs-heading-anchor" href="#Forward-Propagation,-Backward-Propagation,-and-Computational-Graphs">Forward Propagation, Backward Propagation, and Computational Graphs</a><a id="Forward-Propagation,-Backward-Propagation,-and-Computational-Graphs-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-Propagation,-Backward-Propagation,-and-Computational-Graphs" title="Permalink"></a></h1><p>:label:<code>sec_backprop</code></p><p>So far, we have trained our models with minibatch stochastic gradient descent. However, when we implemented the algorithm, we only worried about the calculations involved in <em>forward propagation</em> through the model. When it came time to calculate the gradients, we just invoked the backpropagation function provided by the deep learning framework.</p><p>The automatic calculation of gradients profoundly simplifies the implementation of deep learning algorithms. Before automatic differentiation, even small changes to complicated models required recalculating complicated derivatives by hand. Surprisingly often, academic papers had to allocate numerous pages to deriving update rules. While we must continue to rely on automatic differentiation so we can focus on the interesting parts, you ought to know how these gradients are calculated under the hood if you want to go beyond a shallow understanding of deep learning.</p><p>In this section, we take a deep dive into the details of <em>backward propagation</em> (more commonly called <em>backpropagation</em>). To convey some insight for both the techniques and their implementations, we rely on some basic mathematics and computational graphs. To start, we focus our exposition on a one-hidden-layer MLP with weight decay (<span>$\ell_2$</span> regularization, to be described in subsequent chapters).</p><h2 id="Forward-Propagation"><a class="docs-heading-anchor" href="#Forward-Propagation">Forward Propagation</a><a id="Forward-Propagation-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-Propagation" title="Permalink"></a></h2><p><em>Forward propagation</em> (or <em>forward pass</em>) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer. We now work step-by-step through the mechanics of a neural network with one hidden layer. This may seem tedious but in the eternal words of funk virtuoso James Brown, you must &quot;pay the cost to be the boss&quot;.</p><p>For the sake of simplicity, let&#39;s assume that the input example is <span>$\mathbf{x}\in \mathbb{R}^d$</span> and that our hidden layer does not include a bias term. Here the intermediate variable is:</p><p class="math-container">\[\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x},\]</p><p>where <span>$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$</span> is the weight parameter of the hidden layer. After running the intermediate variable <span>$\mathbf{z}\in \mathbb{R}^h$</span> through the activation function <span>$\phi$</span> we obtain our hidden activation vector of length <span>$h$</span>:</p><p class="math-container">\[\mathbf{h}= \phi (\mathbf{z}).\]</p><p>The hidden layer output <span>$\mathbf{h}$</span> is also an intermediate variable. Assuming that the parameters of the output layer possess only a weight of <span>$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$</span>, we can obtain an output layer variable with a vector of length <span>$q$</span>:</p><p class="math-container">\[\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.\]</p><p>Assuming that the loss function is <span>$l$</span> and the example label is <span>$y$</span>, we can then calculate the loss term for a single data example,</p><p class="math-container">\[L = l(\mathbf{o}, y).\]</p><p>As we will see the definition of <span>$\ell_2$</span> regularization to be introduced later, given the hyperparameter <span>$\lambda$</span>, the regularization term is</p><p class="math-container">\[s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_\textrm{F}^2 + \|\mathbf{W}^{(2)}\|_\textrm{F}^2\right),\]</p><p>:eqlabel:<code>eq_forward-s</code></p><p>where the Frobenius norm of the matrix is simply the <span>$\ell_2$</span> norm applied after flattening the matrix into a vector. Finally, the model&#39;s regularized loss on a given data example is:</p><p class="math-container">\[J = L + s.\]</p><p>We refer to <span>$J$</span> as the <em>objective function</em> in the following discussion.</p><h2 id="Computational-Graph-of-Forward-Propagation"><a class="docs-heading-anchor" href="#Computational-Graph-of-Forward-Propagation">Computational Graph of Forward Propagation</a><a id="Computational-Graph-of-Forward-Propagation-1"></a><a class="docs-heading-anchor-permalink" href="#Computational-Graph-of-Forward-Propagation" title="Permalink"></a></h2><p>Plotting <em>computational graphs</em> helps us visualize the dependencies of operators and variables within the calculation. :numref:<code>fig_forward</code> contains the graph associated with the simple network described above, where squares denote variables and circles denote operators. The lower-left corner signifies the input and the upper-right corner is the output. Notice that the directions of the arrows (which illustrate data flow) are primarily rightward and upward.</p><p><img src="../../img/forward.svg" alt="Computational graph of forward propagation."/> :label:<code>fig_forward</code></p><h2 id="Backpropagation"><a class="docs-heading-anchor" href="#Backpropagation">Backpropagation</a><a id="Backpropagation-1"></a><a class="docs-heading-anchor-permalink" href="#Backpropagation" title="Permalink"></a></h2><p><em>Backpropagation</em> refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the <em>chain rule</em> from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters. Assume that we have functions <span>$\mathsf{Y}=f(\mathsf{X})$</span> and <span>$\mathsf{Z}=g(\mathsf{Y})$</span>, in which the input and the output <span>$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$</span> are tensors of arbitrary shapes. By using the chain rule, we can compute the derivative of <span>$\mathsf{Z}$</span> with respect to <span>$\mathsf{X}$</span> via</p><p class="math-container">\[\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \textrm{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right).\]</p><p>Here we use the <span>$\textrm{prod}$</span> operator to multiply its arguments after the necessary operations, such as transposition and swapping input positions, have been carried out. For vectors, this is straightforward: it is simply matrix–matrix multiplication. For higher dimensional tensors, we use the appropriate counterpart. The operator <span>$\textrm{prod}$</span> hides all the notational overhead.</p><p>Recall that the parameters of the simple network with one hidden layer, whose computational graph is in :numref:<code>fig_forward</code>, are <span>$\mathbf{W}^{(1)}$</span> and <span>$\mathbf{W}^{(2)}$</span>. The objective of backpropagation is to calculate the gradients <span>$\partial J/\partial \mathbf{W}^{(1)}$</span> and <span>$\partial J/\partial \mathbf{W}^{(2)}$</span>. To accomplish this, we apply the chain rule and calculate, in turn, the gradient of each intermediate variable and parameter. The order of calculations are reversed relative to those performed in forward propagation, since we need to start with the outcome of the computational graph and work our way towards the parameters. The first step is to calculate the gradients of the objective function <span>$J=L+s$</span> with respect to the loss term <span>$L$</span> and the regularization term <span>$s$</span>:</p><p class="math-container">\[\frac{\partial J}{\partial L} = 1 \; \textrm{and} \; \frac{\partial J}{\partial s} = 1.\]</p><p>Next, we compute the gradient of the objective function with respect to variable of the output layer <span>$\mathbf{o}$</span> according to the chain rule:</p><p class="math-container">\[
\frac{\partial J}{\partial \mathbf{o}}
= \textrm{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \mathbf{o}}\right)
= \frac{\partial L}{\partial \mathbf{o}}
\in \mathbb{R}^q.
$$

Next, we calculate the gradients
of the regularization term
with respect to both parameters:

$$\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}
\; \textrm{and} \;
\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.\]</p><p>Now we are able to calculate the gradient <span>$\partial J/\partial \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$</span> of the model parameters closest to the output layer. Using the chain rule yields:</p><p class="math-container">\[\frac{\partial J}{\partial \mathbf{W}^{(2)}}= \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) + \textrm{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(2)}}\right)= \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.\]</p><p>:eqlabel:<code>eq_backprop-J-h</code></p><p>To obtain the gradient with respect to <span>$\mathbf{W}^{(1)}$</span> we need to continue backpropagation along the output layer to the hidden layer. The gradient with respect to the hidden layer output <span>$\partial J/\partial \mathbf{h} \in \mathbb{R}^h$</span> is given by</p><p>$</p><p>\frac{\partial J}{\partial \mathbf{h}} = \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{h}}\right) = {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}. $</p><p>Since the activation function <span>$\phi$</span> applies elementwise, calculating the gradient <span>$\partial J/\partial \mathbf{z} \in \mathbb{R}^h$</span> of the intermediate variable <span>$\mathbf{z}$</span> requires that we use the elementwise multiplication operator, which we denote by <span>$\odot$</span>:</p><p>$</p><p>\frac{\partial J}{\partial \mathbf{z}} = \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{h}}, \frac{\partial \mathbf{h}}{\partial \mathbf{z}}\right) = \frac{\partial J}{\partial \mathbf{h}} \odot \phi&#39;\left(\mathbf{z}\right). $</p><p>Finally, we can obtain the gradient <span>$\partial J/\partial \mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$</span> of the model parameters closest to the input layer. According to the chain rule, we get</p><p>$</p><p>\frac{\partial J}{\partial \mathbf{W}^{(1)}} = \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{z}}, \frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) + \textrm{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(1)}}\right) = \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}. $</p><h2 id="Training-Neural-Networks"><a class="docs-heading-anchor" href="#Training-Neural-Networks">Training Neural Networks</a><a id="Training-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Training-Neural-Networks" title="Permalink"></a></h2><p>When training neural networks, forward and backward propagation depend on each other. In particular, for forward propagation, we traverse the computational graph in the direction of dependencies and compute all the variables on its path. These are then used for backpropagation where the compute order on the graph is reversed.</p><p>Take the aforementioned simple network as an illustrative example. On the one hand, computing the regularization term :eqref:<code>eq_forward-s</code> during forward propagation depends on the current values of model parameters <span>$\mathbf{W}^{(1)}$</span> and <span>$\mathbf{W}^{(2)}$</span>. They are given by the optimization algorithm according to backpropagation in the most recent iteration. On the other hand, the gradient calculation for the parameter :eqref:<code>eq_backprop-J-h</code> during backpropagation depends on the current value of the hidden layer output <span>$\mathbf{h}$</span>, which is given by forward propagation.</p><p>Therefore when training neural networks, once model parameters are initialized, we alternate forward propagation with backpropagation, updating model parameters using gradients given by backpropagation. Note that backpropagation reuses the stored intermediate values from forward propagation to avoid duplicate calculations. One of the consequences is that we need to retain the intermediate values until backpropagation is complete. This is also one of the reasons why training requires significantly more memory than plain prediction. Besides, the size of such intermediate values is roughly proportional to the number of network layers and the batch size. Thus, training deeper networks using larger batch sizes more easily leads to <em>out-of-memory</em> errors.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer. Backpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order. When training deep learning models, forward propagation and backpropagation are interdependent, and training requires significantly more memory than prediction.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Assume that the inputs <span>$\mathbf{X}$</span> to some scalar function <span>$f$</span> are <span>$n \times m$</span> matrices. What is the dimensionality of the gradient of <span>$f$</span> with respect to <span>$\mathbf{X}$</span>?</li><li>Add a bias to the hidden layer of the model described in this section (you do not need to include bias in the regularization term).<ol><li>Draw the corresponding computational graph.</li><li>Derive the forward and backward propagation equations.</li></ol></li><li>Compute the memory footprint for training and prediction in the model described in this section.</li><li>Assume that you want to compute second derivatives. What happens to the computational graph? How long do you expect the calculation to take?</li><li>Assume that the computational graph is too large for your GPU.<ol><li>Can you partition it over more than one GPU?</li><li>What are the advantages and disadvantages over training on a smaller minibatch?</li></ol></li></ol><p><a href="https://discuss.d2l.ai/t/102">Discussions</a></p><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MLP_2/">« Implementation of Multilayer Perceptrons</a><a class="docs-footer-nextpage" href="../MLP_4/">Numerical Stability and Initialization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
