<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Dropout · d2l Julia</title><meta name="title" content="Dropout · d2l Julia"/><meta property="og:title" content="Dropout · d2l Julia"/><meta property="twitter:title" content="Dropout · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../MLP_5/">Generalization in Deep Learning</a></li><li class="is-active"><a class="tocitem" href>Dropout</a><ul class="internal"><li><a class="tocitem" href="#Dropout-in-Practice"><span>Dropout in Practice</span></a></li><li><a class="tocitem" href="#Implementation-from-Scratch"><span>Implementation from Scratch</span></a></li></ul></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Multilayer Perceptron</a></li><li class="is-active"><a href>Dropout</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Dropout</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Dropout"><a class="docs-heading-anchor" href="#Dropout">Dropout</a><a id="Dropout-1"></a><a class="docs-heading-anchor-permalink" href="#Dropout" title="Permalink"></a></h1><p>:label:<code>sec_dropout</code></p><p>Let&#39;s think briefly about what we expect from a good predictive model. We want it to peform well on unseen data. Classical generalization theory suggests that to close the gap between train and test performance, we should aim for a simple model. Simplicity can come in the form of a small number of dimensions. We explored this when discussing the monomial basis functions of linear models in :numref:<code>sec_generalization_basics</code>. Additionally, as we saw when discussing weight decay (<span>$\ell_2$</span> regularization) in :numref:<code>sec_weight_decay</code>, the (inverse) norm of the parameters also represents a useful measure of simplicity. Another useful notion of simplicity is smoothness, i.e., that the function should not be sensitive to small changes to its inputs. For instance, when we classify images, we would expect that adding some random noise to the pixels should be mostly harmless.</p><p>:citet:<code>Bishop.1995</code> formalized this idea when he proved that training with input noise is equivalent to Tikhonov regularization. This work drew a clear mathematical connection between the requirement that a function be smooth (and thus simple), and the requirement that it be resilient to perturbations in the input.</p><p>Then, :citet:<code>Srivastava.Hinton.Krizhevsky.ea.2014</code> developed a clever idea for how to apply Bishop&#39;s idea to the internal layers of a network, too. Their idea, called <em>dropout</em>, involves injecting noise while computing each internal layer during forward propagation, and it has become a standard technique for training neural networks. The method is called <em>dropout</em> because we literally <em>drop out</em> some neurons during training. Throughout training, on each iteration, standard dropout consists of zeroing out some fraction of the nodes in each layer before calculating the subsequent layer.</p><p>To be clear, we are imposing our own narrative with the link to Bishop. The original paper on dropout offers intuition through a surprising analogy to sexual reproduction. The authors argue that neural network overfitting is characterized by a state in which each layer relies on a specific pattern of activations in the previous layer, calling this condition <em>co-adaptation</em>. Dropout, they claim, breaks up co-adaptation just as sexual reproduction is argued to break up co-adapted genes. While such an justification of this theory is certainly up for debate, the dropout technique itself has proved enduring, and various forms of dropout are implemented in most deep learning libraries. </p><p>The key challenge is how to inject this noise. One idea is to inject it in an <em>unbiased</em> manner so that the expected value of each layer–-while fixing the others–-equals the value it would have taken absent noise. In Bishop&#39;s work, he added Gaussian noise to the inputs to a linear model. At each training iteration, he added noise sampled from a distribution with mean zero <span>$\epsilon \sim \mathcal{N}(0,\sigma^2)$</span> to the input <span>$\mathbf{x}$</span>, yielding a perturbed point <span>$\mathbf{x}&#39; = \mathbf{x} + \epsilon$</span>. In expectation, <span>$E[\mathbf{x}&#39;] = \mathbf{x}$</span>.</p><p>In standard dropout regularization, one zeros out some fraction of the nodes in each layer and then <em>debiases</em> each layer by normalizing by the fraction of nodes that were retained (not dropped out). In other words, with <em>dropout probability</em> <span>$p$</span>, each intermediate activation <span>$h$</span> is replaced by a random variable <span>$h&#39;$</span> as follows:</p><p>$</p><p>\begin{aligned} h&#39; = \begin{cases}     0 &amp; \textrm{ with probability } p \
    \frac{h}{1-p} &amp; \textrm{ otherwise} \end{cases} \end{aligned} $</p><p>By design, the expectation remains unchanged, i.e., <span>$E[h&#39;] = h$</span>.</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(&quot;../../d2lai&quot;)
using d2lai, Flux, Distributions</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/workspace/d2l-julia/d2lai`</code></pre><h2 id="Dropout-in-Practice"><a class="docs-heading-anchor" href="#Dropout-in-Practice">Dropout in Practice</a><a id="Dropout-in-Practice-1"></a><a class="docs-heading-anchor-permalink" href="#Dropout-in-Practice" title="Permalink"></a></h2><p>Recall the MLP with a hidden layer and five hidden units from :numref:<code>fig_mlp</code>. When we apply dropout to a hidden layer, zeroing out each hidden unit with probability <span>$p$</span>, the result can be viewed as a network containing only a subset of the original neurons. In :numref:<code>fig_dropout2</code>, <span>$h_2$</span> and <span>$h_5$</span> are removed. Consequently, the calculation of the outputs no longer depends on <span>$h_2$</span> or <span>$h_5$</span> and their respective gradient also vanishes when performing backpropagation. In this way, the calculation of the output layer cannot be overly dependent on any one element of <span>$h_1, \ldots, h_5$</span>.</p><p><img src="../../img/dropout2.svg" alt="MLP before and after dropout."/> :label:<code>fig_dropout2</code></p><p>Typically, we disable dropout at test time. Given a trained model and a new example, we do not drop out any nodes and thus do not need to normalize. However, there are some exceptions: some researchers use dropout at test time as a heuristic for estimating the <em>uncertainty</em> of neural network predictions: if the predictions agree across many different dropout outputs, then we might say that the network is more confident.</p><h2 id="Implementation-from-Scratch"><a class="docs-heading-anchor" href="#Implementation-from-Scratch">Implementation from Scratch</a><a id="Implementation-from-Scratch-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-from-Scratch" title="Permalink"></a></h2><p>To implement the dropout function for a single layer, we must draw as many samples from a Bernoulli (binary) random variable as our layer has dimensions, where the random variable takes value <span>$1$</span> (keep) with probability <span>$1-p$</span> and <span>$0$</span> (drop) with probability <span>$p$</span>. One easy way to implement this is to first draw samples from the uniform distribution <span>$U[0, 1]$</span>. Then we can keep those nodes for which the corresponding sample is greater than <span>$p$</span>, dropping the rest.</p><p>In the following code, we (<strong>implement a <code>dropout_layer</code> function that drops out the elements in the tensor input <code>X</code> with probability <code>dropout</code></strong>), rescaling the remainder as described above: dividing the survivors by <code>1.0-dropout</code>.</p><pre><code class="language-julia hljs">function dropout_layer(X::AbstractArray, dropout)
    probs = rand(Bernoulli(dropout), size(X, 1))
    return probs .* X
end
</code></pre><pre><code class="nohighlight hljs">dropout_layer (generic function with 1 method)</code></pre><p>We can test out the dropout_layer function on a few examples. In the following lines of code, we pass our input X through the dropout operation, with probabilities 0, 0.5, and 1, respectively.</p><pre><code class="language-julia hljs">X = reshape(1:16, 2, 8)
println(&quot;dropout with p = 0&quot;, dropout(X, 0.))
println(&quot;dropout with p = 0.5&quot;, dropout(X, 0.5))
println(&quot;dropout with p = 1.0&quot;, dropout(X, 1.))</code></pre><pre><code class="nohighlight hljs">dropout with p = 0[1.0 3.0 5.0 7.0 9.0 11.0 13.0 15.0; 2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0]
dropout with p = 0.5[2.0 0.0 0.0 14.0 18.0 0.0 0.0 0.0; 0.0 8.0 12.0 0.0 0.0 24.0 28.0 32.0]
dropout with p = 1.0[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]</code></pre><h3 id="Defining-the-Model"><a class="docs-heading-anchor" href="#Defining-the-Model">Defining the Model</a><a id="Defining-the-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-Model" title="Permalink"></a></h3><p>The model below applies dropout to the output of each hidden layer (following the activation function). We can set dropout probabilities for each layer separately. A common choice is to set a lower dropout probability closer to the input layer. We ensure that dropout is only active during training.</p><pre><code class="language-julia hljs">mutable struct DropoutScratchMLP{N, A} &lt;: AbstractClassifier
    net::N 
    args::A 
    train::Bool
end
Flux.@layer DropoutScratchMLP trainable=(net,)
function DropoutScratchMLP(; args...)
    net = Chain(Dense(args[:num_inputs], args[:num_hidden_1]), Dense(args[:num_hidden_1], args[:num_hidden_2]), Dense(args[:num_hidden_2], args[:num_outputs]), Flux.softmax)
    DropoutScratchMLP(net, NamedTuple(args), true)
end

function d2lai.forward(mlp::DropoutScratchMLP, x)
    lin1, lin2, lin3, softmax = mlp.net.layers
    h1 = model.train ? dropout_layer(lin1(x), mlp.args.dropout_1) : lin1(x)
    h2 = model.train ? dropout_layer(lin2(h1), mlp.args.dropout_2) : lin2(h1)
    h3 = lin3(h2)
    return softmax(h3)
end
</code></pre><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><p>The following is similar to the training of MLPs described previously.</p><pre><code class="language-julia hljs">hparams = (num_inputs = 28*28, num_outputs = 10, num_hidden_1 = 256, num_hidden_2 = 256,
           dropout_1 = 0.5, dropout_2 = 0.5, lr = 0.1)
model = DropoutScratchMLP(; hparams...)

opt = Descent(0.1)
data = d2lai.FashionMNISTData(; batchsize = 256, flatten = true)
trainer = Trainer(model, data, opt; max_epochs = 10)
d2lai.fit(trainer)</code></pre><pre><code class="language-julia hljs"></code></pre><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MLP_5/">« Generalization in Deep Learning</a><a class="docs-footer-nextpage" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
