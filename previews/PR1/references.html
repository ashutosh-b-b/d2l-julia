<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>References | d2l Julia</title>
    <meta name="description" content="Documentation for d2l-julia">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/d2l-julia/previews/PR1/assets/style.BUOi7SFr.css" as="style">
    <link rel="preload stylesheet" href="/d2l-julia/previews/PR1/vp-icons.css" as="style">
    
    <script type="module" src="/d2l-julia/previews/PR1/assets/app.DyCk50An.js"></script>
    <link rel="preload" href="/d2l-julia/previews/PR1/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/chunks/theme.CszcN3qP.js">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/chunks/framework.DjA5121Y.js">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/references.md.Dwf5fRK0.lean.js">
    <script src="/d2l-julia/versions.js"></script>
    <script src="/d2l-julia/previews/PR1/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/d2l-julia/previews/PR1/" data-v-0f4f798b><!--[--><!--]--><!--[--><img class="VPImage logo" src="/d2l-julia/previews/PR1/logo.png" width="24" height="24" alt data-v-35a7d0b8><!--]--><span data-v-0f4f798b>d2l Julia</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/d2l-julia/previews/PR1/index" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Chapters</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/chapters" data-v-acbfed09><!--[--><span data-v-acbfed09>ðŸ“˜ Chapters Overview</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Linear Neural Networks for Regression</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Linear Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Multiple Dispatch Design for Implementation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Synthetic Regression Data</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Linear Regression Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Linear Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Weight Decay</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Linear Neural Networks for Classification</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Softmax Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>The Image Classification Dataset</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Softmax Regression Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Softmax Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization in Classification</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Environment and Distribution Shift</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Multilayer Perceptron</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Multilayer Perceptrons</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Implementation of Multilayer Perceptrons</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Forward Propagation, Backward Propagation, and Computational Graphs</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Numerical Stability and Initialization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization in Deep Learning</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Dropout</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Convolutional Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Convolutions for Images</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Padding and Stride</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Multiple Input and Multiple Output Channels</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Pooling</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Convolutional Neural Networks (LeNet)</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Modern Convolutional Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_0" data-v-acbfed09><!--[--><span data-v-acbfed09>Modern Convolutional Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Deep Convolutional Neural Networks (AlexNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Networks Using Blocks (VGG)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Network in Network (NiN)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Branch Networks  (GoogLeNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Batch Normalization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Residual Networks (ResNet) and ResNeXt</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Densely Connected Networks (DenseNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-acbfed09><!--[--><span data-v-acbfed09>Designing Convolution Network Architectures</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Recurrent Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_0" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Working with Sequences</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Converting Raw Text into Sequence Data</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Language Models</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Network Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Backpropagation Through Time</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Modern Recurrent Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Long Short-Term Memory (LSTM)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Gated Recurrent Units (GRU)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Deep Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Bidirectional Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Machine Translation and the Dataset</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>The Encoderâ€“Decoder Architecture</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Sequence-to-Sequence Learning for Machine Translation</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Attention Mechanisms and Transformers</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Queries, Keys, and Values</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Attention Pooling by Similarity</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Attention Scoring Functions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>The Bahdanau Attention Mechanism</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Head Attention</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Self-Attention and Positional Encoding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-acbfed09><!--[--><span data-v-acbfed09>CH10.Attention_Mechanisms_and_Transformers/Untitled</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/d2l-julia/previews/PR1/references" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>References</span><!--]--></a><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ashutosh-b-b/d2l-julia" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ashutosh-b-b/d2l-julia" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/index" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Home</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Chapters</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/chapters" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>ðŸ“˜ Chapters Overview</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Linear Neural Networks for Regression</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Linear Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multiple Dispatch Design for Implementation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Synthetic Regression Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Linear Regression Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Linear Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Weight Decay</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Linear Neural Networks for Classification</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Softmax Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Image Classification Dataset</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Softmax Regression Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Softmax Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization in Classification</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Environment and Distribution Shift</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Multilayer Perceptron</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multilayer Perceptrons</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Implementation of Multilayer Perceptrons</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Forward Propagation, Backward Propagation, and Computational Graphs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Numerical Stability and Initialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization in Deep Learning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Dropout</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Convolutional Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Convolutions for Images</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Padding and Stride</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multiple Input and Multiple Output Channels</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Pooling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Convolutional Neural Networks (LeNet)</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Modern Convolutional Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_0" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Modern Convolutional Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Deep Convolutional Neural Networks (AlexNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Networks Using Blocks (VGG)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Network in Network (NiN)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multi-Branch Networks  (GoogLeNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Batch Normalization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Residual Networks (ResNet) and ResNeXt</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Densely Connected Networks (DenseNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Designing Convolution Network Architectures</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Recurrent Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_0" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Working with Sequences</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Converting Raw Text into Sequence Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Language Models</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Network Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Backpropagation Through Time</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Modern Recurrent Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Long Short-Term Memory (LSTM)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Gated Recurrent Units (GRU)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Deep Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Bidirectional Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Machine Translation and the Dataset</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Encoderâ€“Decoder Architecture</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Sequence-to-Sequence Learning for Machine Translation</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Attention Mechanisms and Transformers</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Queries, Keys, and Values</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Attention Pooling by Similarity</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Attention Scoring Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Bahdanau Attention Mechanism</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multi-Head Attention</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Self-Attention and Positional Encoding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>CH10.Attention_Mechanisms_and_Transformers/Untitled</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 has-active" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/references" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>References</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _d2l-julia_previews_PR1_references" data-v-83890dd9><div><h1 id="References" tabindex="-1">References <a class="header-anchor" href="#References" aria-label="Permalink to &quot;References {#References}&quot;">â€‹</a></h1><ol><li><p><a id="Legendre_1805"></a> A.Â M.Â Legendre. <em>MÃ©moire sur les OpÃ©rations TrigonomÃ©triques: dont les RÃ©sultats DÃ©pendent de la Figure de la Terre</em> (F. Didot, 1805).</p></li><li><p><a id="Gauss_1809"></a> C.Â F.Â Gauss. <em>Theoria motus corporum coelestum</em>. In: <em>Werke</em> (KÃ¶niglich Preussische Akademie der Wissenschaften, 1809).</p></li><li><p><a id="Golub_Van-Loan_1996"></a> G.Â H.Â Golub and C.Â F.Â Van Loan. <em>Matrix Computations</em> (Johns Hopkins University Press, 1996).</p></li><li><p><a id="Liu_Nocedal_1989"></a> D.Â C.Â Liu and J.Â Nocedal. <em>On the limited memory BFGS method for large scale optimization</em>. MathematicalÂ Programming <strong>45</strong>, 503â€“528 (1989).</p></li><li><p><a id="Bottou_2010"></a> L.Â Bottou. <em>Large-scale machine learning with stochastic gradient descent</em>. In: <em>Proceedings of COMPSTAT&#39;2010</em> (Springer, 2010); pp.Â 177â€“186.</p></li><li><p><a id="Li_Zhang_Chen_ea_2014"></a> M.Â Li, T.Â Zhang, Y.Â Chen and A.Â J.Â Smola. <em>Efficient mini-batch training for stochastic optimization</em>. In: <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2014); pp.Â 661â€“670.</p></li><li><p><a id="Frazier_2018"></a> P.Â I.Â Frazier. <em>A tutorial on Bayesian optimization</em>. ArXiv:1807.02811 (2018).</p></li><li><p><a id="Izmailov_Podoprikhin_Garipov_ea_2018"></a> P.Â Izmailov, D.Â Podoprikhin, T.Â Garipov, D.Â Vetrov and A.Â G.Â Wilson. <em>Averaging weights leads to wider optima and better generalization</em>. ArXiv:1803.05407 (2018).</p></li><li><p><a id="Frankle_Carbin_2018"></a> J.Â Frankle and M.Â Carbin. <em>The lottery ticket hypothesis: Finding sparse, trainable neural networks</em>. ArXiv:1803.03635 (2018).</p></li><li><p><a id="Russell_Norvig_2016"></a> S.Â J.Â Russell and P.Â Norvig. <em>Artificial Intelligence: A Modern Approach</em> (Pearson Education Limited, 2016).</p></li><li><p><a id="Black_Scholes_1973"></a> F.Â Black and M.Â Scholes. <em>The pricing of options and corporate liabilities</em>. JournalÂ ofÂ PoliticalÂ Economy <strong>81</strong>, 637â€“654 (1973).</p></li><li><p><a id="Naor_Reingold_1999"></a> M.Â Naor and O.Â Reingold. <em>On the construction of pseudorandom permutations: Lubyâ€“Rackoff revisited</em>. JournalÂ ofÂ Cryptology <strong>12</strong>, 29â€“66 (1999).</p></li><li><p><a id="Vapnik_1992"></a> V.Â Vapnik. <em>Principles of risk minimization for learning theory</em>. In: <em>Advances in Neural Information Processing Systems</em> (1992); pp.Â 831â€“838.</p></li><li><p><a id="Bergstra_Breuleux_Bastien_ea_2010"></a> J.Â Bergstra, O.Â Breuleux, F.Â Bastien, P.Â Lamblin, R.Â Pascanu, G.Â Desjardins, J.Â Turian, D.Â Warde-Farley and Y.Â Bengio. <em>Theano: A CPU and GPU math compiler in Python</em>. In: <em>Proc. 9th Python in Science Conference</em>, Vol.Â 1 (2010); pp.Â 3â€“10.</p></li><li><p><a id="Dean_Corrado_Monga_ea_2012"></a> J.Â Dean, G.Â S.Â Corrado, R.Â Monga, K.Â Chen, M.Â Devin, Q.Â V.Â Le, M.Â Z.Â Mao, M.Â Ranzato, A.Â Senior, P.Â Tucker and al. <em>Large scale distributed deep networks</em>. In: <em>Proceedings of the 25th International Conference on Neural Information Processing Systems, Volume 1</em> (2012); pp.Â 1223â€“1231.</p></li><li><p><a id="Jia_Shelhamer_Donahue_ea_2014"></a> Y.Â Jia, E.Â Shelhamer, J.Â Donahue, S.Â Karayev, J.Â Long, R.Â Girshick, S.Â Guadarrama and T.Â Darrell. <em>Caffe: Convolutional architecture for fast feature embedding</em>. In: <em>Proceedings of the 22nd ACM International Conference on Multimedia</em> (2014); pp.Â 675â€“678.</p></li><li><p><a id="Bottou_Le-Cun_1988"></a> L.Â Bottou and Y.Â Le Cun. <a href="http://leon.bottou.org/papers/bottou-lecun-88" target="_blank" rel="noreferrer"><em>SN: A simulator for connectionist models</em></a>. In: <em>Proceedings of NeuroNimes 88</em> (Nimes, France, 1988); pp.Â 371â€“382.</p></li><li><p><a id="Chen_Li_Li_ea_2015"></a> T.Â Chen, M.Â Li, Y.Â Li, M.Â Lin, N.Â Wang, M.Â Wang, T.Â Xiao, B.Â Xu, C.Â Zhang and Z.Â Zhang. <em>MXNET: A flexible and efficient machine learning library for heterogeneous distributed systems</em>. ArXiv:1512.01274 (2015).</p></li><li><p><a id="Frostig_Johnson_Leary_2018"></a> R.Â Frostig, M.Â J.Â Johnson and C.Â Leary. <em>Compiling machine learning programs via high-level tracing</em>. In: <em>Proceedings of Systems for Machine Learning</em> (GoogleResearch, 2018).</p></li><li><p><a id="Paszke_Gross_Massa_ea_2019"></a> A.Â Paszke, S.Â Gross, F.Â Massa, A.Â Lerer, J.Â Bradbury, G.Â Chanan, T.Â Killeen, Z.Â Lin, N.Â Gimelshein, L.Â Antiga and al. <em>PyTorch: An imperative style, high-performance deep learning library</em>. AdvancesÂ inÂ NeuralÂ InformationÂ ProcessingÂ Systems <strong>32</strong>, 8026â€“8037 (2019).</p></li><li><p><a id="Abadi_Barham_Chen_ea_2016"></a> M.Â Abadi, P.Â Barham, J.Â Chen, Z.Â Chen, A.Â Davis, J.Â Dean, M.Â Devin, S.Â Ghemawat, G.Â Irving, M.Â Isard and al. <em>TensorFlow: A system for large-scale machine learning</em>. In: <em>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</em> (2016); pp.Â 265â€“283.</p></li><li><p><a id="Deng_Dong_Socher_ea_2009"></a> J.Â Deng, W.Â Dong, R.Â Socher, L.-J.Â Li, K.Â Li and L.Â Fei-Fei. <em>Imagenet: A large-scale hierarchical image database</em>. In: <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> (IEEE, 2009); pp.Â 248â€“255.</p></li><li><p><a id="thomee2016yfcc100m"></a> B.Â Thomee, D.Â A.Â Shamma, G.Â Friedland, B.Â Elizalde, K.Â Ni, D.Â Poland, D.Â Borth and L.-J.Â Li. <em>YFCC100M: The new data in multimedia research</em>. CommunicationsÂ ofÂ theÂ ACM <strong>59</strong>, 64â€“73 (2016).</p></li><li><p><a id="Vapnik98"></a> V.Â Vapnik. <em>Statistical Learning Theory</em> (John Wiley and Sons, New York, 1998).</p></li><li><p><a id="boucheron2005theory"></a> S.Â Boucheron, O.Â Bousquet and G.Â Lugosi. <em>Theory of classification: A survey of some recent advances</em>. ESAIM:Â ProbabilityÂ andÂ Statistics <strong>9</strong>, 323â€“375 (2005).</p></li><li><p><a id="vapnik1994measuring"></a> V.Â Vapnik, E.Â Levin and Y.Â Le Cun. <em>Measuring the VC-dimension of a learning machine</em>. NeuralÂ Computation <strong>6</strong>, 851â€“876 (1994).</p></li><li><p><a id="Scholkopf_Smola_2002"></a> B.Â SchÃ¶lkopf and A.Â J.Â Smola. <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</em> (MIT Press, 2002).</p></li><li><p><a id="ong2005learning"></a> C.Â S.Â Ong, A.Â Smola and R.Â Williamson. <em>Learning the kernel with hyperkernels</em>. JournalÂ ofÂ MachineÂ LearningÂ Research <strong>6</strong>, 1043â€“1071 (2005).</p></li><li><p><a id="Tsoumakas_Katakis_2007"></a> G.Â Tsoumakas and I.Â Katakis. <em>Multi-label classification: An overview</em>. InternationalÂ JournalÂ ofÂ DataÂ WarehousingÂ andÂ Mining <strong>3</strong>, 1â€“13 (2007).</p></li><li><p><a id="Huang_Xu_Yu_2015"></a> Z.Â Huang, W.Â Xu and K.Â Yu. <em>Bidirectional LSTMâ€“CRF models for sequence tagging</em>. ArXiv:1508.01991 (2015).</p></li><li><p><a id="Moon_Smola_Chang_ea_2010"></a> T.Â Moon, A.Â Smola, Y.Â Chang and Z.Â Zheng. <em>Intervalrank: isotonic regression with listwise and pairwise constraints</em>. In: <em>Proceedings of the 3rd ACM International Conference on Web Search and Data Mining</em> (2010); pp.Â 151â€“160.</p></li><li><p><a id="Beutel_Murray_Faloutsos_ea_2014"></a> A.Â Beutel, K.Â Murray, C.Â Faloutsos and A.Â J.Â Smola. <em>CoBaFi: collaborative Bayesian filtering</em>. In: <em>Proceedings of the 23rd International Conference on World Wide Web</em> (2014); pp.Â 97â€“108.</p></li><li><p><a id="Shannon_1948"></a> C.Â E.Â Shannon. <em>A Mathematical Theory of Communication</em>. TheÂ BellÂ SystemÂ TechnicalÂ Journal <strong>27</strong>, 379â€“423 (1948).</p></li><li><p><a id="Yang_Moczulski_Denil_ea_2015"></a> Z.Â Yang, M.Â Moczulski, M.Â Denil, N.Â De Freitas, A.Â Smola, L.Â Song and Z.Â Wang. <em>Deep fried convnets</em>. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em> (2015); pp.Â 1476â€“1483.</p></li><li><p><a id="sindhwani2015structured"></a> V.Â Sindhwani, T.Â N.Â Sainath and S.Â Kumar. <em>Structured transforms for small-footprint deep learning</em>. ArXiv:1510.01722 (2015).</p></li><li><p><a id="Zhang_Tay_Zhang_ea_2021"></a> A.Â Zhang, Y.Â Tay, S.Â Zhang, A.Â Chan, A.Â T.Â Luu, S.Â C.Â Hui and J.Â Fu. <em>Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</em>. In: <em>International Conference on Learning Representations</em> (2021).</p></li><li><p><a id="Bradley_Terry_1952"></a> R.Â A.Â Bradley and M.Â E.Â Terry. <em>Rank analysis of incomplete block designs: I. The method of paired comparisons</em>. Biometrika <strong>39</strong>, 324â€“345 (1952).</p></li><li><p><a id="LeCun_Bottou_Bengio_ea_1998"></a> Y.Â LeCun, L.Â Bottou, Y.Â Bengio and P.Â Haffner. <em>Gradient-based learning applied to document recognition</em>. ProceedingsÂ ofÂ theÂ IEEE <strong>86</strong>, 2278â€“2324 (1998).</p></li><li><p><a id="LeCun_Jackel_Bottou_ea_1995"></a> Y.Â LeCun, L.Â Jackel, L.Â Bottou, A.Â Brunot, C.Â Cortes, J.Â Denker, H.Â Drucker, I.Â Guyon, U.Â Muller, E.Â Sackinger and al. <em>Comparison of learning algorithms for handwritten digit recognition</em>. In: <em>International Conference on Artificial Neural Networks</em> (1995); pp.Â 53â€“60.</p></li><li><p><a id="Scholkopf_Burges_Vapnik_1996"></a> B.Â SchÃ¶lkopf, C.Â Burges and V.Â Vapnik. <em>Incorporating invariances in support vector learning machines</em>. In: <em>International Conference on Artificial Neural Networks</em> (Springer, 1996); pp.Â 47â€“52.</p></li><li><p><a id="Simard_LeCun_Denker_ea_1998"></a> P.Â Y.Â Simard, Y.Â A.Â LeCun, J.Â S.Â Denker and B.Â Victorri. <em>Transformation invariance in pattern recognition â€“ tangent distance and tangent propagation</em>. In: <em>Neural Networks: Tricks of the Trade</em>, Vol.Â 529 no.Â 7587 (Springer, 1998); pp.Â 239â€“274.</p></li><li><p><a id="Xiao_Rasul_Vollgraf_2017"></a> H.Â Xiao, K.Â Rasul and R.Â Vollgraf. <em>Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</em>. ArXiv:1708.07747 (2017).</p></li><li><p><a id="dwork2015preserving"></a> C.Â Dwork, V.Â Feldman, M.Â Hardt, T.Â Pitassi, O.Â Reingold and A.Â L.Â Roth. <em>Preserving statistical validity in adaptive data analysis</em>. In: <em>Proceedings of the 47th Annual ACM Symposium on Theory of Computing</em> (2015); pp.Â 117â€“126.</p></li><li><p><a id="VapChe64"></a> V.Â Vapnik and A.Â Chervonenkis. <em>A note on one class of perceptrons</em>. AutomationÂ andÂ RemoteÂ Control <strong>25</strong> (1964).</p></li><li><p><a id="VapChe68"></a> V.Â Vapnik and A.Â Chervonenkis. <em>Uniform convergence of frequencies of occurence of events to their probabilities</em>. Dokl.Ãƒkad.Ã‘aukÂ SSSR <strong>181</strong>, 915â€“918 (1968).</p></li><li><p><a id="VapChe71"></a> V.Â Vapnik and A.Â Chervonenkis. <em>On the uniform convergence of relative frequencies of events to their probabilities</em>. TheoryÂ Probab.Â Appl. <strong>16</strong>, 264â€“281 (1971).</p></li><li><p><a id="VapChe74b"></a> V.Â Vapnik and A.Â Chervonenkis. <em>Ordered risk minimization</em>. AutomationÂ andÂ RemoteÂ Control <strong>35</strong>, 1226â€“1235, 1403â€“1412 (1974).</p></li><li><p><a id="VapChe81"></a> V.Â Vapnik and A.Â Chervonenkis. <em>The necessary and sufficient conditions for the uniform convergence of averages to their expected values</em>. TeoriyaÂ VeroyatnosteiÂ iÂ EeÂ Primeneniya <strong>26</strong>, 543â€“564 (1981).</p></li><li><p><a id="VapChe91"></a> V.Â Vapnik and A.Â Chervonenkis. <em>The necessary and sufficient conditions for consistency in the empirical risk minimization method</em>. PatternÂ RecognitionÂ andÂ ImageÂ Analysis <strong>1</strong>, 283â€“305 (1991).</p></li><li><p><a id="Shao_Yao_Sun_ea_2020"></a> H.Â Shao, S.Â Yao, D.Â Sun, A.Â Zhang, S.Â Liu, D.Â Liu, J.Â Wang and T.Â Abdelzaher. <em>ControlVAE: Controllable variational autoencoder</em>. In: <em>Proceedings of the 37th International Conference on Machine Learning</em> (JMLR. org, 2020).</p></li><li><p><a id="Fisher_1928"></a> R.Â A.Â Fisher. <em>Statistical Methods for Research Workers.</em> (Oliver &amp; Boyd, 1925).</p></li><li><p><a id="quinlan2014c4"></a> J.Â R.Â Quinlan. <em>C4.5: Programs for Machine Learning</em>. Vol.Â 51 no.Â 4 (Elsevier, 1993); p.Â 66.</p></li><li><p><a id="Aronszajn_1950"></a> N.Â Aronszajn. <em>Theory of reproducing kernels</em>. TransactionsÂ ofÂ theÂ AmericanÂ MathematicalÂ Society <strong>68</strong>, 337â€“404 (1950).</p></li><li><p><a id="Wahba_1990"></a> G.Â Wahba. <em>Spline Models for Observational Data</em> (SIAM, 1990).</p></li><li><p><a id="Cajal_Azoulay_1894"></a> S.Â RamÃ³n yÂ Cajal and L.Â Azoulay. <em>Les Nouvelles IdÃ©es sur la Structure du SystÃ¨me Nerveux chez l&#39;Homme et chez les VertÃ©brÃ©s</em>. Vol.Â 11 no.Â 2 (Paris, C. Reinwald &amp; Cie, 1894); p.Â 125.</p></li><li><p><a id="McCulloch_Pitts_1943"></a> W.Â S.Â McCulloch and W.Â Pitts. <em>A logical calculus of the ideas immanent in nervous activity</em>. BulletinÂ ofÂ MathematicalÂ Biophysics <strong>5</strong>, 115â€“133 (1943).</p></li><li><p><a id="LeCun_Bottou_Orr_ea_1998"></a> Y.Â LeCun, L.Â Bottou, G.Â Orr and K.-R.Â Muller. <em>Efficient backprop</em>. In: <em>Neural Networks: Tricks of the Trade</em> (Springer, 1998).</p></li><li><p><a id="Kalman_Kwasny_1992"></a> B.Â L.Â Kalman and S.Â C.Â Kwasny. <em>Why tanh: choosing a sigmoidal function</em>. In: <em>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</em> (IEEE, 1992); pp.Â 578â€“581.</p></li><li><p><a id="Hendrycks_Gimpel_2016"></a> D.Â Hendrycks and K.Â Gimpel. <em>Gaussian error linear units (GELUs)</em>. ArXiv:1606.08415 (2016).</p></li><li><p><a id="Ramachandran_Zoph_Le_2017"></a> P.Â Ramachandran, B.Â Zoph and Q.Â V.Â Le. <em>Searching for activation functions</em>. ArXiv:1710.05941 (2017).</p></li><li><p><a id="Ioffe_Szegedy_2015"></a> S.Â Ioffe and C.Â Szegedy. <em>Batch normalization: Accelerating deep network training by reducing internal covariate shift</em>. ArXiv:1502.03167 (2015).</p></li><li><p><a id="Xiao_Bahri_Sohl-Dickstein_ea_2018"></a> L.Â Xiao, Y.Â Bahri, J.Â Sohl-Dickstein, S.Â Schoenholz and J.Â Pennington. <em>Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks</em>. In: <em>International Conference on Machine Learning</em> (2018); pp.Â 5393â€“5402.</p></li><li><p><a id="You_Gitman_Ginsburg_2017"></a> Y.Â You, I.Â Gitman and B.Â Ginsburg. <em>Large batch training of convolutional networks</em>. ArXiv:1708.03888 (2017).</p></li><li><p><a id="wolpert1995no"></a> D.Â H.Â Wolpert and W.Â G.Â Macready. <em>No free lunch theorems for search</em> (Technical Report SFI-TR-95-02-010, Santa Fe Institute, 1995).</p></li><li><p><a id="zhang2021understanding"></a> C.Â Zhang, S.Â Bengio, M.Â Hardt, B.Â Recht and O.Â Vinyals. <em>Understanding deep learning (still) requires rethinking generalization</em>. CommunicationsÂ ofÂ theÂ ACM <strong>64</strong>, 107â€“115 (2021).</p></li><li><p><a id="nakkiran2021deep"></a> P.Â Nakkiran, G.Â Kaplun, Y.Â Bansal, T.Â Yang, B.Â Barak and I.Â Sutskever. <em>Deep double descent: Where bigger models and more data hurt</em>. JournalÂ ofÂ StatisticalÂ Mechanics:Â TheoryÂ andÂ Experiment <strong>2021</strong>, 124003 (2021).</p></li><li><p><a id="Jacot_Grabriel_Hongler_2018"></a> A.Â Jacot, F.Â Gabriel and C.Â Hongler. <em>Neural tangent kernel: Convergence and generalization in neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em>, Vol.Â 31 (2018).</p></li><li><p><a id="Rolnick_Veit_Belongie_Shavit_2017"></a> D.Â Rolnick, A.Â Veit, S.Â Belongie and N.Â Shavit. <em>Deep learning is robust to massive label noise</em>. ArXiv:1705.10694 (2017).</p></li><li><p><a id="Garg_Balakrishnan_Kolter_Lipton_2021"></a> S.Â Garg, S.Â Balakrishnan, Z.Â Kolter and Z.Â Lipton. <em>RATT: Leveraging unlabeled data to guarantee generalization</em>. In: <em>International Conference on Machine Learning</em>, Vol.Â 31 (PMLR, 2021); pp.Â 3598â€“3609.</p></li><li><p><a id="Srivastava_Hinton_Krizhevsky_ea_2014"></a> N.Â Srivastava, G.Â Hinton, A.Â Krizhevsky, I.Â Sutskever and R.Â Salakhutdinov. <em>Dropout: a simple way to prevent neural networks from overfitting</em>. JournalÂ ofÂ MachineÂ LearningÂ Research <strong>15</strong>, 1929â€“1958 (2014).</p></li><li><p><a id="Bishop_1995"></a> C.Â M.Â Bishop. <em>Training with noise is equivalent to Tikhonov regularization</em>. NeuralÂ Computation <strong>7</strong>, 108â€“116 (1995).</p></li><li><p><a id="Hubel_Wiesel_1959"></a> D.Â H.Â Hubel and T.Â N.Â Wiesel. <em>Receptive fields of single neurones in the cat&#39;s striate cortex</em>. JournalÂ ofÂ Physiology <strong>148</strong>, 574â€“591 (1959).</p></li><li><p><a id="Hubel_Wiesel_1962"></a> D.Â H.Â Hubel and T.Â N.Â Wiesel. <em>Receptive fields, binocular interaction and functional architecture in the cat&#39;s visual cortex</em>. JournalÂ ofÂ Physiology <strong>160</strong>, 106â€“154 (1962).</p></li><li><p><a id="Hubel_Wiesel_1968"></a> D.Â H.Â Hubel and T.Â N.Â Wiesel. <em>Receptive fields and functional architecture of monkey striate cortex</em>. JournalÂ ofÂ Physiology <strong>195</strong>, 215â€“243 (1968).</p></li><li><p><a id="Field_1987"></a> D.Â J.Â Field. <em>Relations between the statistics of natural images and the response properties of cortical cells</em>. JOSAÂ A <strong>4</strong>, 2379â€“2394 (1987).</p></li><li><p><a id="Kuzovkin_Vicente_Petton_ea_2018"></a> I.Â Kuzovkin, R.Â Vicente, M.Â Petton, J.-P.Â Lachaux, M.Â Baciu, P.Â Kahane, S.Â Rheims, J.Â R.Â Vidal and J.Â Aru. <em>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex</em>. CommunicationsÂ Biology <strong>1</strong>, 1â€“12 (2018).</p></li><li><p><a id="Alsallakh_Kokhlikyan_Miglani_ea_2020"></a> B.Â Alsallakh, N.Â Kokhlikyan, V.Â Miglani, J.Â Yuan and O.Â Reblitz-Richardson. <em>Mind the PAD â€“ CNNs can develop blind spots</em>. ArXiv:2010.02178 (2020).</p></li><li><p><a id="Lin_Chen_Yan_2013"></a> M.Â Lin, Q.Â Chen and S.Â Yan. <em>Network in network</em>. ArXiv:1312.4400 (2013).</p></li><li><p><a id="Szegedy_Ioffe_Vanhoucke_ea_2017"></a> C.Â Szegedy, S.Â Ioffe, V.Â Vanhoucke and A.Â A.Â Alemi. <em>Inception-v4, Inception-ResNet and the impact of residual connections on learning</em>. In: <em>31st AAAI Conference on Artificial Intelligence</em> (2017).</p></li><li><p><a id="Xie_Girshick_Dollar_ea_2017"></a> S.Â Xie, R.Â Girshick, P.Â DollÃ¡r, Z.Â Tu and K.Â He. <em>Aggregated residual transformations for deep neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2017); pp.Â 1492â€“1500.</p></li><li><p><a id="Riesenhuber_Poggio_1999"></a> M.Â Riesenhuber and T.Â Poggio. <em>Hierarchical models of object recognition in cortex</em>. NatureÂ Neuroscience <strong>2</strong>, 1019â€“1025 (1999).</p></li><li><p><a id="Yamaguchi_Sakamoto_Akabane_ea_1990"></a> K.Â Yamaguchi, K.Â Sakamoto, T.Â Akabane and Y.Â Fujimoto. <em>A neural network for speaker-independent isolated word recognition</em>. In: <em>First International Conference on Spoken Language Processing</em> (1990).</p></li><li><p><a id="LeCun_Boser_Denker_ea_1989"></a> Y.Â LeCun, B.Â Boser, J.Â S.Â Denker, D.Â Henderson, R.Â E.Â Howard, W.Â Hubbard and L.Â D.Â Jackel. <em>Backpropagation applied to handwritten zip code recognition</em>. NeuralÂ Computation <strong>1</strong>, 541â€“551 (1989).</p></li><li><p><a id="Zhang_Sun_Jiang_ea_2021"></a> Y.Â Zhang, P.Â Sun, Y.Â Jiang, D.Â Yu, Z.Â Yuan, P.Â Luo, W.Â Liu and X.Â Wang. <em>ByteTrack: Multi-object tracking by associating every detection box</em>. ArXiv:2110.06864 (2021).</p></li><li><p><a id="Long_Shelhamer_Darrell_2015"></a> J.Â Long, E.Â Shelhamer and T.Â Darrell. <em>Fully convolutional networks for semantic segmentation</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015); pp.Â 3431â€“3440.</p></li><li><p><a id="Redmon_Farhadi_2018"></a> J.Â Redmon and A.Â Farhadi. <em>YOLOv3: An incremental improvement</em>. ArXiv:1804.02767 (2018).</p></li><li><p><a id="Gatys_Ecker_Bethge_2016"></a> L.Â A.Â Gatys, A.Â S.Â Ecker and M.Â Bethge. <em>Image style transfer using convolutional neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp.Â 2414â€“2423.</p></li><li><p><a id="Dosovitskiy_Beyer_Kolesnikov_ea_2021"></a> A.Â Dosovitskiy, L.Â Beyer, A.Â Kolesnikov, D.Â Weissenborn, X.Â Zhai, T.Â Unterthiner, M.Â Dehghani, M.Â Minderer, G.Â Heigold, S.Â Gelly and al. <em>An image is worth 16 x 16 words: Transformers for image recognition at scale</em>. In: <em>International Conference on Learning Representations</em> (2021).</p></li><li><p><a id="liu2021swin"></a> Z.Â Liu, Y.Â Lin, Y.Â Cao, H.Â Hu, Y.Â Wei, Z.Â Zhang, S.Â Lin and B.Â Guo. <em>Swin transformer: Hierarchical vision transformer using shifted windows</em>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, Vol.Â 34 (2021); pp.Â 10012â€“10022.</p></li><li><p><a id="Krizhevsky_Sutskever_Hinton_2012"></a> A.Â Krizhevsky, I.Â Sutskever and G.Â E.Â Hinton. <em>ImageNet classification with deep convolutional neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2012); pp.Â 1097â€“1105.</p></li><li><p><a id="Simonyan_Zisserman_2014"></a> K.Â Simonyan and A.Â Zisserman. <em>Very deep convolutional networks for large-scale image recognition</em>. ArXiv:1409.1556 (2014).</p></li><li><p><a id="Szegedy_Liu_Jia_ea_2015"></a> C.Â Szegedy, W.Â Liu, Y.Â Jia, P.Â Sermanet, S.Â Reed, D.Â Anguelov, D.Â Erhan, V.Â Vanhoucke and A.Â Rabinovich. <em>Going deeper with convolutions</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015); pp.Â 1â€“9.</p></li><li><p><a id="He_Zhang_Ren_ea_2016"></a> K.Â He, X.Â Zhang, S.Â Ren and J.Â Sun. <em>Deep residual learning for image recognition</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp.Â 770â€“778.</p></li><li><p><a id="Huang_Liu_Van-Der-Maaten_ea_2017"></a> G.Â Huang, Z.Â Liu, L.Â Van Der Maaten and K.Â Q.Â Weinberger. <em>Densely connected convolutional networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2017); pp.Â 4700â€“4708.</p></li><li><p><a id="wu2018shift"></a> B.Â Wu, A.Â Wan, X.Â Yue, P.Â Jin, S.Â Zhao, N.Â Golmant, A.Â Gholaminejad, J.Â Gonzalez and K.Â Keutzer. <em>Shift: A zero flop, zero parameter alternative to spatial convolutions</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018); pp.Â 9127â€“9135.</p></li><li><p><a id="Howard_Sandler_Chu_ea_2019"></a> A.Â Howard, M.Â Sandler, G.Â Chu, L.-C.Â Chen, B.Â Chen, M.Â Tan, W.Â Wang, Y.Â Zhu, R.Â Pang, V.Â Vasudevan, Q.Â V.Â Le and H.Â Adam. <em>Searching for MobileNetV3</em>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (2019); pp.Â 1314â€“1324.</p></li><li><p><a id="Radosavovic_Kosaraju_Girshick_ea_2020"></a> I.Â Radosavovic, R.Â P.Â Kosaraju, R.Â Girshick, K.Â He and P.Â DollÃ¡r. <em>Designing network design spaces</em>. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (2020); pp.Â 10428â€“10436.</p></li><li><p><a id="liu2022convnet"></a> Z.Â Liu, H.Â Mao, C.-Y.Â Wu, C.Â Feichtenhofer, T.Â Darrell and S.Â Xie. <em>A ConvNet for the 2020s</em>. ArXiv:2201.03545 (2022).</p></li><li><p><a id="Freund_Schapire_ea_1996"></a> Y.Â Freund and R.Â E.Â Schapire. <em>Experiments with a new boosting algorithm</em>. In: <em>Proceedings of the International Conference on Machine Learning</em>, Vol.Â 96 (Citeseer, 1996); pp.Â 148â€“156.</p></li><li><p><a id="Taskar_Guestrin_Koller_2004"></a> B.Â Taskar, C.Â Guestrin and D.Â Koller. <em>Max-margin Markov networks</em>. AdvancesÂ inÂ NeuralÂ InformationÂ ProcessingÂ Systems <strong>16</strong>, 25 (2004).</p></li><li><p><a id="Lowe_2004"></a> D.Â G.Â Lowe. <em>Distinctive image features from scale-invariant keypoints</em>. InternationalÂ JournalÂ ofÂ ComputerÂ Vision <strong>60</strong>, 91â€“110 (2004).</p></li><li><p><a id="Bay_Tuytelaars_Van-Gool_2006"></a> H.Â Bay, T.Â Tuytelaars and L.Â Van Gool. <em>SURF: Speeded up robust features</em>. In: <em>European Conference on Computer Vision</em> (Springer, 2006); pp.Â 404â€“417.</p></li><li><p><a id="Sivic_Zisserman_2003"></a> J.Â Sivic and A.Â Zisserman. <em>Video Google: A text retrieval approach to object matching in videos</em>. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em>, Vol.Â 3 (IEEE Computer Society, 2003); pp.Â 1470â€“1470.</p></li><li><p><a id="Hartley_Zisserman_2000"></a> R.Â Hartley and A.Â Zisserman. <em>Multiple View Geometry in Computer Vision</em> (Cambridge University Press, 2000).</p></li><li><p><a id="Glorot_Bengio_2010"></a> X.Â Glorot and Y.Â Bengio. <em>Understanding the difficulty of training deep feedforward neural networks</em>. In: <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em>, Vol.Â 4 (2010); pp.Â 249â€“256.</p></li><li><p><a id="Kingma_Ba_2014"></a> D.Â P.Â Kingma and J.Â Ba. <em>Adam: A method for stochastic optimization</em>. ArXiv:1412.6980 (2014).</p></li><li><p><a id="Nair_Hinton_2010"></a> V.Â Nair and G.Â E.Â Hinton. <em>Rectified linear units improve restricted Boltzmann machines</em>. In: <em>ICML</em> (2010).</p></li><li><p><a id="Boyd_Vandenberghe_2004"></a> S.Â Boyd and L.Â Vandenberghe. <em>Convex Optimization</em> (Cambridge University Press, Cambridge, England, 2004).</p></li><li><p><a id="hartley2009global"></a> R.Â I.Â Hartley and F.Â Kahl. <em>Global optimization through rotation space search</em>. InternationalÂ JournalÂ ofÂ ComputerÂ Vision <strong>82</strong>, 64â€“79 (2009).</p></li><li><p><a id="Dalal_Triggs_2005"></a> N.Â Dalal and B.Â Triggs. <em>Histograms of oriented gradients for human detection</em>. In: <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#39;05)</em>, Vol.Â 1 (IEEE, 2005); pp.Â 886â€“893.</p></li><li><p><a id="olshausen1996emergence"></a> B.Â A.Â Olshausen and D.Â J.Â Field. <em>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</em>. Nature <strong>381</strong>, 607â€“609 (1996).</p></li><li><p><a id="le2013building"></a> Q.Â V.Â Le. <em>Building high-level features using large scale unsupervised learning</em>. In: <em>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</em> (IEEE, 2013); pp.Â 8595â€“8598.</p></li><li><p><a id="Miller_1995"></a> G.Â A.Â Miller. <em>WordNet: a lexical database for English</em>. CommunicationsÂ ofÂ theÂ ACM <strong>38</strong>, 39â€“41 (1995).</p></li><li><p><a id="Torralba_Fergus_Freeman_2008"></a> A.Â Torralba, R.Â Fergus and W.Â T.Â Freeman. <em>80 million tiny images: A large data set for nonparametric object and scene recognition</em>. IEEEÂ TransactionsÂ onÂ PatternÂ AnalysisÂ andÂ MachineÂ Intelligence <strong>30</strong>, 1958â€“1970 (2008).</p></li><li><p><a id="russakovsky2015imagenet"></a> O.Â Russakovsky, J.Â Deng, H.Â Su, J.Â Krause, S.Â Satheesh, S.Â Ma, Z.Â Huang, A.Â Karpathy, A.Â Khosla, M.Â Bernstein and al. <em>ImageNet large scale visual recognition challenge</em>. InternationalÂ JournalÂ ofÂ ComputerÂ Vision <strong>115</strong>, 211â€“252 (2015).</p></li><li><p><a id="schuhmann2022laion"></a> C.Â Schuhmann, R.Â Beaumont, R.Â Vencu, C.Â Gordon, R.Â Wightman, M.Â Cherti, T.Â Coombes, A.Â Katta, C.Â Mullis, M.Â Wortsman and al. <em>LAION-5B: An open large-scale dataset for training next generation image-text models</em>. ArXiv:2210.08402 (2022).</p></li><li><p><a id="Fernando_2004"></a> R.Â Fernando. <em>GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics</em>. Vol.Â 2 (Addison-Wesley, 2004).</p></li><li><p><a id="Russakovsky_Deng_Huang_ea_2013"></a> O.Â Russakovsky, J.Â Deng, Z.Â Huang, A.Â C.Â Berg and L.Â Fei-Fei. <em>Detecting avocados to zucchinis: what have we done, and where are we going?</em> In: <em>International Conference on Computer Vision (ICCV)</em>, Vol.Â 5 no.Â 3 (2013); p.Â 1.</p></li><li><p><a id="Buslaev_Iglovikov_Khvedchenya_ea_2020"></a> A.Â Buslaev, V.Â I.Â Iglovikov, E.Â Khvedchenya, A.Â Parinov, M.Â Druzhinin and A.Â A.Â Kalinin. <em>Albumentations: Fast and flexible image augmentations</em>. Information <strong>11</strong>, 125 (2020).</p></li><li><p><a id="Mead_1980"></a> C.Â Mead. <em>Introduction to VLSI systems</em>. IEEÂ ProceedingsÂ I-Solid-StateÂ andÂ ElectronÂ Devices <strong>128</strong>, 18 (1980).</p></li><li><p><a id="bommasani2021opportunities"></a> R.Â Bommasani, D.Â A.Â Hudson, E.Â Adeli, R.Â Altman, S.Â Arora, S.Â vonÂ Arx, M.Â S.Â Bernstein, J.Â Bohg, A.Â Bosselut, E.Â Brunskill and al. <em>On the opportunities and risks of foundation models</em>. ArXiv:2108.07258 (2021).</p></li><li><p><a id="lavin2016fast"></a> A.Â Lavin and S.Â Gray. <em>Fast algorithms for convolutional neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, No.Â 3 (MIT Press, 2016); pp.Â 4013â€“4021.</p></li><li><p><a id="Goyal_Bochkovskiy_Deng_ea_2021"></a> A.Â Goyal, A.Â Bochkovskiy, J.Â Deng and V.Â Koltun. <em>Non-deep networks</em>. ArXiv:2110.07641 (2021).</p></li><li><p><a id="Szegedy_Vanhoucke_Ioffe_ea_2016"></a> C.Â Szegedy, V.Â Vanhoucke, S.Â Ioffe, J.Â Shlens and Z.Â Wojna. <em>Rethinking the Inception architecture for computer vision</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp.Â 2818â€“2826.</p></li><li><p><a id="friedman1987exploratory"></a> J.Â H.Â Friedman. <em>Exploratory projection pursuit</em>. JournalÂ ofÂ theÂ AmericanÂ StatisticalÂ Association <strong>82</strong>, 249â€“266 (1987).</p></li><li><p><a id="guyon2008feature"></a> I.Â Guyon, S.Â Gunn, M.Â Nikravesh and L.Â A.Â Zadeh. <em>Feature Extraction: Foundations and Applications</em> (Springer, 2008).</p></li><li><p><a id="Vapnik95"></a> V.Â Vapnik. <em>The Nature of Statistical Learning Theory</em> (Springer, New York, 1995).</p></li><li><p><a id="Novikoff62"></a> A.Â B.Â J.Â Novikoff. <em>On convergence proofs on perceptrons</em>. In: <em>Proceedings of the Symposium on the Mathematical Theory of Automata</em> (Polytechnic Institute of Brooklyn, 1962); pp.Â 615â€“622.</p></li><li><p><a id="Ba_Kiros_Hinton_2016"></a> J.Â L.Â Ba, J.Â R.Â Kiros and G.Â E.Â Hinton. <em>Layer normalization</em>. ArXiv:1607.06450 (2016).</p></li><li><p><a id="Duchi_Hazan_Singer_2011"></a> J.Â Duchi, E.Â Hazan and Y.Â Singer. <em>Adaptive subgradient methods for online learning and stochastic optimization</em>. JournalÂ ofÂ MachineÂ LearningÂ Research <strong>12</strong>, 2121â€“2159 (2011).</p></li><li><p><a id="Zaheer_Reddi_Sachan_ea_2018"></a> M.Â Zaheer, S.Â Reddi, D.Â Sachan, S.Â Kale and S.Â Kumar. <em>Adaptive methods for nonconvex optimization</em>. In: <em>Advances in Neural Information Processing Systems</em> (2018); pp.Â 9793â€“9803.</p></li><li><p><a id="anil2020scalable"></a> R.Â Anil, V.Â Gupta, T.Â Koren, K.Â Regan and Y.Â Singer. <em>Scalable second-order optimization for deep learning</em>. ArXiv:2002.09018 (2020).</p></li><li><p><a id="Teye_Azizpour_Smith_2018"></a> M.Â Teye, H.Â Azizpour and K.Â Smith. <em>Bayesian uncertainty estimation for batch normalized deep networks</em>. ArXiv:1802.06455 (2018).</p></li><li><p><a id="Luo_Wang_Shao_ea_2018"></a> P.Â Luo, X.Â Wang, W.Â Shao and Z.Â Peng. <em>Towards understanding regularization in batch normalization</em>. ArXiv:1809.00846 (2018).</p></li><li><p><a id="Lipton_Steinhardt_2018"></a> Z.Â C.Â Lipton and J.Â Steinhardt. <em>Troubling trends in machine learning scholarship</em>. CommunicationsÂ ofÂ theÂ ACM <strong>17</strong>, 45â€“77 (2018).</p></li><li><p><a id="Santurkar_Tsipras_Ilyas_ea_2018"></a> S.Â Santurkar, D.Â Tsipras, A.Â Ilyas and A.Â Madry. <em>How does batch normalization help optimization?</em> In: <em>Advances in Neural Information Processing Systems</em> (2018); pp.Â 2483â€“2493.</p></li><li><p><a id="wang2022removing"></a> H.Â Wang, A.Â Zhang, S.Â Zheng, X.Â Shi, M.Â Li and Z.Â Wang. <em>Removing batch normalization boosts adversarial training</em>. In: <em>International Conference on Machine Learning</em> (PMLR, 2022); pp.Â 23433â€“23445.</p></li><li><p><a id="tikhonov1977solutions"></a> A.Â N.Â Tikhonov and V.Â Y.Â Arsenin. <em>Solutions of Ill-Posed Problems</em>. Vol.Â 2021 no.Â 12 (W.H.Â Winston, 1977); p.Â 124003.</p></li><li><p><a id="morozov2012methods"></a> V.Â A.Â Morozov. <em>Methods for Solving Incorrectly Posed Problems</em> (Springer, 1984).</p></li><li><p><a id="prakash2016neural"></a> A.Â Prakash, S.Â A.Â Hasan, K.Â Lee, V.Â Datla, A.Â Qadir, J.Â Liu and O.Â Farri. <em>Neural paraphrase generation with stacked residual LSTM networks</em>. ArXiv:1610.03098 (2016).</p></li><li><p><a id="kim2017residual"></a> J.Â Kim, M.Â El-Khamy and J.Â Lee. <em>Residual LSTM: Design of a deep recurrent architecture for distant speech recognition</em>. ArXiv:1701.03360 (2017).</p></li><li><p><a id="Vaswani_Shazeer_Parmar_ea_2017"></a> A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N.Â Gomez, Å.Â Kaiser and I.Â Polosukhin. <em>Attention is all you need</em>. In: <em>Advances in Neural Information Processing Systems</em> (2017); pp.Â 5998â€“6008.</p></li><li><p><a id="Kipf_Welling_2016"></a> T.Â N.Â Kipf and M.Â Welling. <em>Semi-supervised classification with graph convolutional networks</em>. ArXiv:1609.02907 (2016).</p></li><li><p><a id="Ren_He_Girshick_ea_2015"></a> S.Â Ren, K.Â He, R.Â Girshick and J.Â Sun. <em>Faster R-CNN: Towards real-time object detection with region proposal networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2015); pp.Â 91â€“99.</p></li><li><p><a id="srivastava2015highway"></a> R.Â K.Â Srivastava, K.Â Greff and J.Â Schmidhuber. <em>Highway networks</em>. ArXiv:1505.00387 (2015).</p></li><li><p><a id="He_Zhang_Ren_ea_2016_1"></a> K.Â He, X.Â Zhang, S.Â Ren and J.Â Sun. <em>Identity mappings in deep residual networks</em>. In: <em>European Conference on Computer Vision</em> (Springer, 2016); pp.Â 630â€“645.</p></li><li><p><a id="pleiss2017memory"></a> G.Â Pleiss, D.Â Chen, G.Â Huang, T.Â Li, L.Â Van Der Maaten and K.Â Q.Â Weinberger. <em>Memory-efficient implementation of densenets</em>. ArXiv:1707.06990 (2017).</p></li><li><p><a id="Hu_Shen_Sun_2018"></a> J.Â Hu, L.Â Shen and G.Â Sun. <em>Squeeze-and-excitation networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018); pp.Â 7132â€“7141.</p></li><li><p><a id="zoph2016neural"></a> B.Â Zoph and Q.Â V.Â Le. <em>Neural architecture search with reinforcement learning</em>. ArXiv:1611.01578 (2016).</p></li><li><p><a id="liu2018darts"></a> H.Â Liu, K.Â Simonyan and Y.Â Yang. <em>DARTS: Differentiable architecture search</em>. ArXiv:1806.09055 (2018).</p></li><li><p><a id="tan2019efficientnet"></a> M.Â Tan and Q.Â Le. <em>EfficientNet: Rethinking model scaling for convolutional neural networks</em>. In: <em>International Conference on Machine Learning</em> (PMLR, 2019); pp.Â 6105â€“6114.</p></li><li><p><a id="graves2008novel"></a> A.Â Graves, M.Â Liwicki, S.Â FernÃ¡ndez, R.Â Bertolami, H.Â Bunke and J.Â Schmidhuber. <em>A novel connectionist system for unconstrained handwriting recognition</em>. IEEEÂ TransactionsÂ onÂ PatternÂ AnalysisÂ andÂ MachineÂ Intelligence <strong>31</strong>, 855â€“868 (2008).</p></li><li><p><a id="Sutskever_Vinyals_Le_2014"></a> I.Â Sutskever, O.Â Vinyals and Q.Â V.Â Le. <em>Sequence to sequence learning with neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2014); pp.Â 3104â€“3112.</p></li><li><p><a id="Lipton_Kale_2016"></a> Z.Â C.Â Lipton, D.Â C.Â Kale, C.Â Elkan and R.Â Wetzel. <em>Learning to diagnose with LSTM recurrent neural networks</em>. In: <em>International Conference on Learning Representations (ICLR)</em> (2016).</p></li><li><p><a id="Lipton_Berkowitz_Elkan_2015"></a> Z.Â C.Â Lipton, J.Â Berkowitz and C.Â Elkan. <em>A critical review of recurrent neural networks for sequence learning</em>. ArXiv:1506.00019 <strong>17</strong>, 45â€“77 (2015).</p></li><li><p><a id="Hoyer_Janzing_Mooij_ea_2009"></a> P.Â O.Â Hoyer, D.Â Janzing, J.Â M.Â Mooij, J.Â Peters and B.Â SchÃ¶lkopf. <em>Nonlinear causal discovery with additive noise models</em>. In: <em>Advances in Neural Information Processing Systems</em> (2009); pp.Â 689â€“696.</p></li><li><p><a id="Peters_Janzing_Scholkopf_2017"></a> J.Â Peters, D.Â Janzing and B.Â SchÃ¶lkopf. <em>Elements of Causal Inference: Foundations and Learning Algorithms</em> (MIT Press, 2017).</p></li><li><p><a id="Wood_Gasthaus_Archambeau_ea_2011"></a> F.Â Wood, J.Â Gasthaus, C.Â Archambeau, L.Â James and Y.Â W.Â Teh. <em>The sequence memoizer</em>. CommunicationsÂ ofÂ theÂ ACM <strong>54</strong>, 91â€“98 (2011).</p></li><li><p><a id="Bengio_Ducharme_Vincent_ea_2003"></a> Y.Â Bengio, R.Â Ducharme, P.Â Vincent and C.Â Jauvin. <em>A neural probabilistic language model</em>. JournalÂ ofÂ MachineÂ LearningÂ Research <strong>3</strong>, 1137â€“1155 (2003).</p></li><li><p><a id="Werbos_1990"></a> P.Â J.Â Werbos. <em>Backpropagation through time: what it does and how to do it</em>. ProceedingsÂ ofÂ theÂ IEEE <strong>78</strong>, 1550â€“1560 (1990).</p></li><li><p><a id="Jaeger_2002"></a> H.Â Jaeger. <em>Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the ``echo state network&#39;&#39; approach</em>. Vol.Â 31 (GMD-Forschungszentrum Informationstechnik Bonn, 2002).</p></li><li><p><a id="Tallec_Ollivier_2017"></a> C.Â Tallec and Y.Â Ollivier. <em>Unbiasing truncated backpropagation through time</em>. ArXiv:1705.08209 (2017).</p></li><li><p><a id="elman1990finding"></a> J.Â L.Â Elman. <em>Finding structure in time</em>. CognitiveÂ Science <strong>14</strong>, 179â€“211 (1990).</p></li><li><p><a id="bengio1994learning"></a> Y.Â Bengio, P.Â Simard and P.Â Frasconi. <em>Learning long-term dependencies with gradient descent is difficult</em>. IEEEÂ TransactionsÂ onÂ NeuralÂ Networks <strong>5</strong>, 157â€“166 (1994).</p></li><li><p><a id="Hochreiter_Bengio_Frasconi_ea_2001"></a> S.Â Hochreiter, Y.Â Bengio, P.Â Frasconi and J.Â Schmidhuber. <em>Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</em>. In: <em>A Field Guide to Dynamical Recurrent Neural Networks</em> (IEEE Press, 2001).</p></li><li><p><a id="Hochreiter_Schmidhuber_1997"></a> S.Â Hochreiter and J.Â Schmidhuber. <em>Long short-term memory</em>. NeuralÂ Computation <strong>9</strong>, 1735â€“1780 (1997).</p></li><li><p><a id="Cho_Van-Merrienboer_Bahdanau_ea_2014"></a> K.Â Cho, B.Â Van MerriÃ«nboer, D.Â Bahdanau and Y.Â Bengio. <em>On the properties of neural machine translation: Encoderâ€“decoder approaches</em>. ArXiv:1409.1259 (2014).</p></li><li><p><a id="Chung_Gulcehre_Cho_ea_2014"></a> J.Â Chung, C.Â Gulcehre, K.Â Cho and Y.Â Bengio. <em>Empirical evaluation of gated recurrent neural networks on sequence modeling</em>. ArXiv:1412.3555 (2014).</p></li><li><p><a id="Schuster_Paliwal_1997"></a> M.Â Schuster and K.Â K.Â Paliwal. <em>Bidirectional recurrent neural networks</em>. IEEEÂ TransactionsÂ onÂ SignalÂ Processing <strong>45</strong>, 2673â€“2681 (1997).</p></li><li><p><a id="Brown_Cocke_Della-Pietra_ea_1988"></a> P.Â F.Â Brown, J.Â Cocke, S.Â A.Â Della Pietra, V.Â J.Â Della Pietra, F.Â Jelinek, R.Â L.Â Mercer and P.Â Roossin. <em>A statistical approach to language translation</em>. In: <em>COLING Budapest 1988 Volume 1: International Conference on Computational Linguistics</em> (1988).</p></li><li><p><a id="Brown_Cocke_Della-Pietra_ea_1990"></a> P.Â F.Â Brown, J.Â Cocke, S.Â A.Â Della Pietra, V.Â J.Â Della Pietra, F.Â Jelinek, J.Â Lafferty, R.Â L.Â Mercer and P.Â S.Â Roossin. <em>A statistical approach to machine translation</em>. ComputationalÂ Linguistics <strong>16</strong>, 79â€“85 (1990).</p></li><li><p><a id="Cho_Van-Merrienboer_Gulcehre_ea_2014"></a> K.Â Cho, B.Â Van MerriÃ«nboer, C.Â Gulcehre, D.Â Bahdanau, F.Â Bougares, H.Â Schwenk and Y.Â Bengio. <em>Learning phrase representations using RNN encoderâ€“decoder for statistical machine translation</em>. ArXiv:1406.1078 (2014).</p></li><li><p><a id="Papineni_Roukos_Ward_ea_2002"></a> K.Â Papineni, S.Â Roukos, T.Â Ward and W.-J.Â Zhu. <em>BLEU: a method for automatic evaluation of machine translation</em>. In: <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em> (2002); pp.Â 311â€“318.</p></li><li><p><a id="Kalchbrenner_Grefenstette_Blunsom_2014"></a> N.Â Kalchbrenner, E.Â Grefenstette and P.Â Blunsom. <em>A convolutional neural network for modelling sentences</em>. ArXiv:1404.2188 (2014).</p></li><li><p><a id="yang2016neural"></a> Z.Â Yang, Z.Â Hu, Y.Â Deng, C.Â Dyer and A.Â Smola. <em>Neural machine translation with recurrent attention modeling</em>. ArXiv:1607.05108 (2016).</p></li><li><p><a id="Bahdanau_Cho_Bengio_2014"></a> D.Â Bahdanau, K.Â Cho and Y.Â Bengio. <em>Neural machine translation by jointly learning to align and translate</em>. ArXiv:1409.0473 (2014).</p></li><li><p><a id="Mnih_Heess_Graves_ea_2014"></a> V.Â Mnih, N.Â Heess, A.Â Graves and others. <em>Recurrent models of visual attention</em>. In: <em>Advances in Neural Information Processing Systems</em> (2014); pp.Â 2204â€“2212.</p></li><li><p><a id="Nadaraya_1964"></a> E.Â A.Â Nadaraya. <em>On estimating regression</em>. TheoryÂ ofÂ ProbabilityÂ &amp;Â itsÂ Applications <strong>9</strong>, 141â€“142 (1964).</p></li><li><p><a id="Watson_1964"></a> G.Â S.Â Watson. <em>Smooth regression analysis</em>. SankhyÄ:Â TheÂ IndianÂ JournalÂ ofÂ Statistics,Â SeriesÂ A, 359â€“372 (1964).</p></li><li><p><a id="mack1982weak"></a> Y.-P.Â Mack and B.Â W.Â Silverman. <em>Weak and strong uniform consistency of kernel regression estimates</em>. ZeitschriftÂ fÃ¼rÂ WahrscheinlichkeitstheorieÂ undÂ verwandteÂ Gebiete <strong>61</strong>, 405â€“415 (1982).</p></li><li><p><a id="Silverman86"></a> B.Â Silverman. <em>Density Estimation for Statistical and Data Analysis</em> (Chapman and Hall, 1986).</p></li><li><p><a id="norelli2022asif"></a> A.Â Norelli, M.Â Fumero, V.Â Maiorca, L.Â Moschella, E.Â RodolÃ  and F.Â Locatello. <em>ASIF: Coupled data turns unimodal models to multimodal without training</em>. ArXiv:2210.01738 (2022).</p></li><li><p><a id="shoeybi2019megatron"></a> M.Â Shoeybi, M.Â Patwary, R.Â Puri, P.Â LeGresley, J.Â Casper and B.Â Catanzaro. <em>Megatron-LM: Training multi-billion parameter language models using model parallelism</em>. ArXiv:1909.08053 (2019).</p></li><li><p><a id="Graves_2013"></a> A.Â Graves. <em>Generating sequences with recurrent neural networks</em>. ArXiv:1308.0850 (2013).</p></li><li><p><a id="rabiner1993fundamentals"></a> L.Â Rabiner and B.-H.Â Juang. <em>Fundamentals of Speech Recognition</em> (Prentice-Hall., 1993).</p></li><li><p><a id="chan2015listen"></a> W.Â Chan, N.Â Jaitly, Q.Â V.Â Le and O.Â Vinyals. <em>Listen, attend and spell</em>. ArXiv:1508.01211 (2015).</p></li><li><p><a id="Lin_Feng_Santos_ea_2017"></a> Z.Â Lin, M.Â Feng, C.Â N.Â Santos, M.Â Yu, B.Â Xiang, B.Â Zhou and Y.Â Bengio. <em>A structured self-attentive sentence embedding</em>. ArXiv:1703.03130 (2017).</p></li><li><p><a id="Cheng_Dong_Lapata_2016"></a> J.Â Cheng, L.Â Dong and M.Â Lapata. <em>Long short-term memory-networks for machine reading</em>. In: <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em> (2016); pp.Â 551â€“561.</p></li><li><p><a id="Parikh_Tackstrom_Das_ea_2016"></a> A.Â P.Â Parikh, O.Â TÃ¤ckstrÃ¶m, D.Â Das and J.Â Uszkoreit. <em>A decomposable attention model for natural language inference</em>. ArXiv:1606.01933 (2016).</p></li><li><p><a id="Paulus_Xiong_Socher_2017"></a> R.Â Paulus, C.Â Xiong and R.Â Socher. <em>A deep reinforced model for abstractive summarization</em>. ArXiv:1705.04304 (2017).</p></li><li><p><a id="shaw2018self"></a> P.Â Shaw, J.Â Uszkoreit and A.Â Vaswani. <em>Self-attention with relative position representations</em>. ArXiv:1803.02155 (2018).</p></li><li><p><a id="huang2018music"></a> C.-Z.Â A.Â Huang, A.Â Vaswani, J.Â Uszkoreit, I.Â Simon, C.Â Hawthorne, N.Â Shazeer, A.Â M.Â Dai, M.Â D.Â Hoffman, M.Â Dinculescu and D.Â Eck. <em>Music transformer: generating music with long-term structure</em>. In: <em>International Conference on Learning Representations</em> (2018).</p></li></ol></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://https://github.com/ashutosh-b-b/d2l-julia/edit/master/docs/src/references.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>CH10.Attention_Mechanisms_and_Transformers/Untitled</span><!--]--></a></div><div class="pager" data-v-4f9813fa><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://luxdl.github.io/DocumenterVitepress.jl/dev/" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br></p><p class="copyright" data-v-c970a860>Â© Copyright 2025.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"ch10.attention_mechanisms_and_transformers_attn_1.md\":\"BPTHOId7\",\"ch10.attention_mechanisms_and_transformers_attn_2.md\":\"CSvHAO07\",\"ch10.attention_mechanisms_and_transformers_attn_3.md\":\"B6P-XMmW\",\"ch10.attention_mechanisms_and_transformers_attn_4.md\":\"CB5DEaB1\",\"ch10.attention_mechanisms_and_transformers_attn_5.md\":\"DQHNoxkK\",\"ch10.attention_mechanisms_and_transformers_attn_6.md\":\"DJp_Cd3H\",\"ch10.attention_mechanisms_and_transformers_untitled.md\":\"AZxRWAaB\",\"ch3.linear_regression_lnn_1.md\":\"DTK1fX6I\",\"ch3.linear_regression_lnn_2.md\":\"BD8w6uR8\",\"ch3.linear_regression_lnn_3.md\":\"C4MB6zPT\",\"ch3.linear_regression_lnn_4.md\":\"B7J5MW04\",\"ch3.linear_regression_lnn_5.md\":\"CfF2CfNK\",\"ch3.linear_regression_lnn_6.md\":\"uj6xm8fY\",\"ch3.linear_regression_lnn_7.md\":\"B_zpSpiN\",\"ch4.linear_classification_lcn_1.md\":\"Boi1cDsh\",\"ch4.linear_classification_lcn_2.md\":\"CkOR2Iia\",\"ch4.linear_classification_lcn_3.md\":\"V-0rtib8\",\"ch4.linear_classification_lcn_4.md\":\"BX5UP2HZ\",\"ch4.linear_classification_lcn_5.md\":\"C-7KZE9h\",\"ch4.linear_classification_lcn_6.md\":\"DNDvKaZ3\",\"ch5.mlp_mlp_1.md\":\"DNcZmrDZ\",\"ch5.mlp_mlp_2.md\":\"MI21_tyz\",\"ch5.mlp_mlp_3.md\":\"DVB63m8H\",\"ch5.mlp_mlp_4.md\":\"BYjKXVG9\",\"ch5.mlp_mlp_5.md\":\"CxEzVy5G\",\"ch5.mlp_mlp_6.md\":\"CpygNHoD\",\"ch6.convolutional_neural_networks_cnn_2.md\":\"pAndzyT8\",\"ch6.convolutional_neural_networks_cnn_3.md\":\"CyHC0BXV\",\"ch6.convolutional_neural_networks_cnn_4.md\":\"-fYwA9iM\",\"ch6.convolutional_neural_networks_cnn_5.md\":\"DKCH7I6h\",\"ch6.convolutional_neural_networks_cnn_6.md\":\"BxTNLj1_\",\"ch7.modernconvolutionalneuralnetworks_mcnn_0.md\":\"Do42Pcb-\",\"ch7.modernconvolutionalneuralnetworks_mcnn_1.md\":\"DyrfEGE0\",\"ch7.modernconvolutionalneuralnetworks_mcnn_2.md\":\"BdK6e3J7\",\"ch7.modernconvolutionalneuralnetworks_mcnn_3.md\":\"CvJU8raL\",\"ch7.modernconvolutionalneuralnetworks_mcnn_4.md\":\"DfMUvHOO\",\"ch7.modernconvolutionalneuralnetworks_mcnn_5.md\":\"CVkY2ACS\",\"ch7.modernconvolutionalneuralnetworks_mcnn_6.md\":\"Cin-j3ht\",\"ch7.modernconvolutionalneuralnetworks_mcnn_7.md\":\"BY0qhW6e\",\"ch7.modernconvolutionalneuralnetworks_mcnn_8.md\":\"CfFt59lS\",\"ch8.recurrent_neural_networks_rnn_0.md\":\"CIW34d_V\",\"ch8.recurrent_neural_networks_rnn_1.md\":\"Dqwc86d0\",\"ch8.recurrent_neural_networks_rnn_2.md\":\"DVOnaLjw\",\"ch8.recurrent_neural_networks_rnn_3.md\":\"BzfHLTyo\",\"ch8.recurrent_neural_networks_rnn_4.md\":\"DnQjCEE3\",\"ch8.recurrent_neural_networks_rnn_5.md\":\"6IPEYW7M\",\"ch8.recurrent_neural_networks_rnn_6.md\":\"GS6Ynndo\",\"ch8.recurrent_neural_networks_rnn_7.md\":\"B3soXDbW\",\"ch9.modern_recurrent_neural_networks_mrnn_1.md\":\"zGzQsIzl\",\"ch9.modern_recurrent_neural_networks_mrnn_2.md\":\"CV4lwlnE\",\"ch9.modern_recurrent_neural_networks_mrnn_3.md\":\"DI9bj1mT\",\"ch9.modern_recurrent_neural_networks_mrnn_4.md\":\"CvMg8EjP\",\"ch9.modern_recurrent_neural_networks_mrnn_5.md\":\"CuOfLj02\",\"ch9.modern_recurrent_neural_networks_mrnn_6.md\":\"BrLASjSi\",\"ch9.modern_recurrent_neural_networks_mrnn_7.md\":\"D5R7Lv49\",\"chapters.md\":\"BCfSINH3\",\"index.md\":\"C9fKtYo7\",\"references.md\":\"Dwf5fRK0\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"d2l Julia\",\"description\":\"Documentation for d2l-julia\",\"base\":\"/d2l-julia/previews/PR1/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"src\":\"/logo.png\",\"width\":24,\"height\":24},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Chapters\",\"collapsed\":false,\"items\":[{\"text\":\"ðŸ“˜ Chapters Overview\",\"link\":\"/chapters\"},{\"text\":\"Linear Neural Networks for Regression\",\"collapsed\":false,\"items\":[{\"text\":\"Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_1\"},{\"text\":\"Multiple Dispatch Design for Implementation\",\"link\":\"/CH3.Linear_Regression/LNN_2\"},{\"text\":\"Synthetic Regression Data\",\"link\":\"/CH3.Linear_Regression/LNN_3\"},{\"text\":\"Linear Regression Implementation from Scratch\",\"link\":\"/CH3.Linear_Regression/LNN_4\"},{\"text\":\"Concise Implementation of Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_5\"},{\"text\":\"Generalization\",\"link\":\"/CH3.Linear_Regression/LNN_6\"},{\"text\":\"Weight Decay\",\"link\":\"/CH3.Linear_Regression/LNN_7\"}]},{\"text\":\"Linear Neural Networks for Classification\",\"collapsed\":false,\"items\":[{\"text\":\"Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_1\"},{\"text\":\"The Image Classification Dataset\",\"link\":\"/CH4.Linear_Classification/LCN_2\"},{\"text\":\"Softmax Regression Implementation from Scratch\",\"link\":\"/CH4.Linear_Classification/LCN_3\"},{\"text\":\"Concise Implementation of Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_4\"},{\"text\":\"Generalization in Classification\",\"link\":\"/CH4.Linear_Classification/LCN_5\"},{\"text\":\"Environment and Distribution Shift\",\"link\":\"/CH4.Linear_Classification/LCN_6\"}]},{\"text\":\"Multilayer Perceptron\",\"collapsed\":false,\"items\":[{\"text\":\"Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_1\"},{\"text\":\"Implementation of Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_2\"},{\"text\":\"Forward Propagation, Backward Propagation, and Computational Graphs\",\"link\":\"/CH5.MLP/MLP_3\"},{\"text\":\"Numerical Stability and Initialization\",\"link\":\"/CH5.MLP/MLP_4\"},{\"text\":\"Generalization in Deep Learning\",\"link\":\"/CH5.MLP/MLP_5\"},{\"text\":\"Dropout\",\"link\":\"/CH5.MLP/MLP_6\"}]},{\"text\":\"Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Convolutions for Images\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_2\"},{\"text\":\"Padding and Stride\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_3\"},{\"text\":\"Multiple Input and Multiple Output Channels\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_4\"},{\"text\":\"Pooling\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_5\"},{\"text\":\"Convolutional Neural Networks (LeNet)\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_6\"}]},{\"text\":\"Modern Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Modern Convolutional Neural Networks\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_0\"},{\"text\":\"Deep Convolutional Neural Networks (AlexNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_1\"},{\"text\":\"Networks Using Blocks (VGG)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_2\"},{\"text\":\"Network in Network (NiN)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_3\"},{\"text\":\"Multi-Branch Networks  (GoogLeNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_4\"},{\"text\":\"Batch Normalization\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_5\"},{\"text\":\"Residual Networks (ResNet) and ResNeXt\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_6\"},{\"text\":\"Densely Connected Networks (DenseNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_7\"},{\"text\":\"Designing Convolution Network Architectures\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_8\"}]},{\"text\":\"Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_0\"},{\"text\":\"Working with Sequences\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_1\"},{\"text\":\"Converting Raw Text into Sequence Data\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_2\"},{\"text\":\"Language Models\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_3\"},{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_4\"},{\"text\":\"Recurrent Neural Network Implementation from Scratch\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_5\"},{\"text\":\"Concise Implementation of Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_6\"},{\"text\":\"Backpropagation Through Time\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_7\"}]},{\"text\":\"Modern Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Long Short-Term Memory (LSTM)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_1\"},{\"text\":\"Gated Recurrent Units (GRU)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_2\"},{\"text\":\"Deep Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_3\"},{\"text\":\"Bidirectional Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_4\"},{\"text\":\"Machine Translation and the Dataset\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_5\"},{\"text\":\"The Encoderâ€“Decoder Architecture\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_6\"},{\"text\":\"Sequence-to-Sequence Learning for Machine Translation\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_7\"}]},{\"text\":\"Attention Mechanisms and Transformers\",\"collapsed\":false,\"items\":[{\"text\":\"Queries, Keys, and Values\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_1\"},{\"text\":\"Attention Pooling by Similarity\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_2\"},{\"text\":\"Attention Scoring Functions\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_3\"},{\"text\":\"The Bahdanau Attention Mechanism\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_4\"},{\"text\":\"Multi-Head Attention\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_5\"},{\"text\":\"Self-Attention and Positional Encoding\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_6\"},{\"text\":\"CH10.Attention_Mechanisms_and_Transformers/Untitled\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/Untitled\"}]}]},{\"text\":\"References\",\"link\":\"/references\"},{\"component\":\"VersionPicker\"}],\"sidebar\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Chapters\",\"collapsed\":false,\"items\":[{\"text\":\"ðŸ“˜ Chapters Overview\",\"link\":\"/chapters\"},{\"text\":\"Linear Neural Networks for Regression\",\"collapsed\":false,\"items\":[{\"text\":\"Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_1\"},{\"text\":\"Multiple Dispatch Design for Implementation\",\"link\":\"/CH3.Linear_Regression/LNN_2\"},{\"text\":\"Synthetic Regression Data\",\"link\":\"/CH3.Linear_Regression/LNN_3\"},{\"text\":\"Linear Regression Implementation from Scratch\",\"link\":\"/CH3.Linear_Regression/LNN_4\"},{\"text\":\"Concise Implementation of Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_5\"},{\"text\":\"Generalization\",\"link\":\"/CH3.Linear_Regression/LNN_6\"},{\"text\":\"Weight Decay\",\"link\":\"/CH3.Linear_Regression/LNN_7\"}]},{\"text\":\"Linear Neural Networks for Classification\",\"collapsed\":false,\"items\":[{\"text\":\"Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_1\"},{\"text\":\"The Image Classification Dataset\",\"link\":\"/CH4.Linear_Classification/LCN_2\"},{\"text\":\"Softmax Regression Implementation from Scratch\",\"link\":\"/CH4.Linear_Classification/LCN_3\"},{\"text\":\"Concise Implementation of Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_4\"},{\"text\":\"Generalization in Classification\",\"link\":\"/CH4.Linear_Classification/LCN_5\"},{\"text\":\"Environment and Distribution Shift\",\"link\":\"/CH4.Linear_Classification/LCN_6\"}]},{\"text\":\"Multilayer Perceptron\",\"collapsed\":false,\"items\":[{\"text\":\"Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_1\"},{\"text\":\"Implementation of Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_2\"},{\"text\":\"Forward Propagation, Backward Propagation, and Computational Graphs\",\"link\":\"/CH5.MLP/MLP_3\"},{\"text\":\"Numerical Stability and Initialization\",\"link\":\"/CH5.MLP/MLP_4\"},{\"text\":\"Generalization in Deep Learning\",\"link\":\"/CH5.MLP/MLP_5\"},{\"text\":\"Dropout\",\"link\":\"/CH5.MLP/MLP_6\"}]},{\"text\":\"Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Convolutions for Images\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_2\"},{\"text\":\"Padding and Stride\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_3\"},{\"text\":\"Multiple Input and Multiple Output Channels\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_4\"},{\"text\":\"Pooling\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_5\"},{\"text\":\"Convolutional Neural Networks (LeNet)\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_6\"}]},{\"text\":\"Modern Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Modern Convolutional Neural Networks\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_0\"},{\"text\":\"Deep Convolutional Neural Networks (AlexNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_1\"},{\"text\":\"Networks Using Blocks (VGG)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_2\"},{\"text\":\"Network in Network (NiN)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_3\"},{\"text\":\"Multi-Branch Networks  (GoogLeNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_4\"},{\"text\":\"Batch Normalization\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_5\"},{\"text\":\"Residual Networks (ResNet) and ResNeXt\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_6\"},{\"text\":\"Densely Connected Networks (DenseNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_7\"},{\"text\":\"Designing Convolution Network Architectures\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_8\"}]},{\"text\":\"Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_0\"},{\"text\":\"Working with Sequences\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_1\"},{\"text\":\"Converting Raw Text into Sequence Data\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_2\"},{\"text\":\"Language Models\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_3\"},{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_4\"},{\"text\":\"Recurrent Neural Network Implementation from Scratch\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_5\"},{\"text\":\"Concise Implementation of Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_6\"},{\"text\":\"Backpropagation Through Time\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_7\"}]},{\"text\":\"Modern Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Long Short-Term Memory (LSTM)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_1\"},{\"text\":\"Gated Recurrent Units (GRU)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_2\"},{\"text\":\"Deep Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_3\"},{\"text\":\"Bidirectional Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_4\"},{\"text\":\"Machine Translation and the Dataset\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_5\"},{\"text\":\"The Encoderâ€“Decoder Architecture\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_6\"},{\"text\":\"Sequence-to-Sequence Learning for Machine Translation\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_7\"}]},{\"text\":\"Attention Mechanisms and Transformers\",\"collapsed\":false,\"items\":[{\"text\":\"Queries, Keys, and Values\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_1\"},{\"text\":\"Attention Pooling by Similarity\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_2\"},{\"text\":\"Attention Scoring Functions\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_3\"},{\"text\":\"The Bahdanau Attention Mechanism\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_4\"},{\"text\":\"Multi-Head Attention\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_5\"},{\"text\":\"Self-Attention and Positional Encoding\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_6\"},{\"text\":\"CH10.Attention_Mechanisms_and_Transformers/Untitled\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/Untitled\"}]}]},{\"text\":\"References\",\"link\":\"/references\"}],\"editLink\":{\"pattern\":\"https://https://github.com/ashutosh-b-b/d2l-julia/edit/master/docs/src/:path\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/ashutosh-b-b/d2l-julia\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/dev/\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>\",\"copyright\":\"Â© Copyright 2025.\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>