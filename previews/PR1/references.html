<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>References | d2l Julia</title>
    <meta name="description" content="Documentation for d2l-julia">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/d2l-julia/previews/PR1/assets/style.BUOi7SFr.css" as="style">
    <link rel="preload stylesheet" href="/d2l-julia/previews/PR1/vp-icons.css" as="style">
    
    <script type="module" src="/d2l-julia/previews/PR1/assets/app.DyCk50An.js"></script>
    <link rel="preload" href="/d2l-julia/previews/PR1/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/chunks/theme.CszcN3qP.js">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/chunks/framework.DjA5121Y.js">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/references.md.Dwf5fRK0.lean.js">
    <script src="/d2l-julia/versions.js"></script>
    <script src="/d2l-julia/previews/PR1/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/d2l-julia/previews/PR1/" data-v-0f4f798b><!--[--><!--]--><!--[--><img class="VPImage logo" src="/d2l-julia/previews/PR1/logo.png" width="24" height="24" alt data-v-35a7d0b8><!--]--><span data-v-0f4f798b>d2l Julia</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/d2l-julia/previews/PR1/index" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Chapters</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/chapters" data-v-acbfed09><!--[--><span data-v-acbfed09>📘 Chapters Overview</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Linear Neural Networks for Regression</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Linear Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Multiple Dispatch Design for Implementation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Synthetic Regression Data</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Linear Regression Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Linear Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Weight Decay</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Linear Neural Networks for Classification</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Softmax Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>The Image Classification Dataset</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Softmax Regression Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Softmax Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization in Classification</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Environment and Distribution Shift</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Multilayer Perceptron</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Multilayer Perceptrons</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Implementation of Multilayer Perceptrons</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Forward Propagation, Backward Propagation, and Computational Graphs</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Numerical Stability and Initialization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization in Deep Learning</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Dropout</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Convolutional Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Convolutions for Images</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Padding and Stride</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Multiple Input and Multiple Output Channels</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Pooling</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Convolutional Neural Networks (LeNet)</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Modern Convolutional Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_0" data-v-acbfed09><!--[--><span data-v-acbfed09>Modern Convolutional Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Deep Convolutional Neural Networks (AlexNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Networks Using Blocks (VGG)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Network in Network (NiN)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Branch Networks  (GoogLeNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Batch Normalization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Residual Networks (ResNet) and ResNeXt</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Densely Connected Networks (DenseNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-acbfed09><!--[--><span data-v-acbfed09>Designing Convolution Network Architectures</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Recurrent Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_0" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Working with Sequences</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Converting Raw Text into Sequence Data</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Language Models</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Network Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Backpropagation Through Time</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Modern Recurrent Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Long Short-Term Memory (LSTM)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Gated Recurrent Units (GRU)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Deep Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Bidirectional Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Machine Translation and the Dataset</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>The Encoder–Decoder Architecture</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Sequence-to-Sequence Learning for Machine Translation</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Attention Mechanisms and Transformers</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Queries, Keys, and Values</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Attention Pooling by Similarity</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Attention Scoring Functions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>The Bahdanau Attention Mechanism</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Head Attention</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Self-Attention and Positional Encoding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-acbfed09><!--[--><span data-v-acbfed09>CH10.Attention_Mechanisms_and_Transformers/Untitled</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/d2l-julia/previews/PR1/references" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>References</span><!--]--></a><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ashutosh-b-b/d2l-julia" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ashutosh-b-b/d2l-julia" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/index" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Home</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Chapters</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/chapters" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>📘 Chapters Overview</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Linear Neural Networks for Regression</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Linear Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multiple Dispatch Design for Implementation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Synthetic Regression Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Linear Regression Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Linear Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Weight Decay</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Linear Neural Networks for Classification</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Softmax Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Image Classification Dataset</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Softmax Regression Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Softmax Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization in Classification</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Environment and Distribution Shift</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Multilayer Perceptron</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multilayer Perceptrons</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Implementation of Multilayer Perceptrons</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Forward Propagation, Backward Propagation, and Computational Graphs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Numerical Stability and Initialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization in Deep Learning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Dropout</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Convolutional Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Convolutions for Images</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Padding and Stride</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multiple Input and Multiple Output Channels</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Pooling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Convolutional Neural Networks (LeNet)</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Modern Convolutional Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_0" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Modern Convolutional Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Deep Convolutional Neural Networks (AlexNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Networks Using Blocks (VGG)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Network in Network (NiN)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multi-Branch Networks  (GoogLeNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Batch Normalization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Residual Networks (ResNet) and ResNeXt</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Densely Connected Networks (DenseNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Designing Convolution Network Architectures</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Recurrent Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_0" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Working with Sequences</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Converting Raw Text into Sequence Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Language Models</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Network Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Backpropagation Through Time</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Modern Recurrent Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Long Short-Term Memory (LSTM)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Gated Recurrent Units (GRU)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Deep Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Bidirectional Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Machine Translation and the Dataset</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Encoder–Decoder Architecture</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Sequence-to-Sequence Learning for Machine Translation</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Attention Mechanisms and Transformers</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Queries, Keys, and Values</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Attention Pooling by Similarity</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Attention Scoring Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Bahdanau Attention Mechanism</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multi-Head Attention</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Self-Attention and Positional Encoding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>CH10.Attention_Mechanisms_and_Transformers/Untitled</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 has-active" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/references" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>References</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _d2l-julia_previews_PR1_references" data-v-83890dd9><div><h1 id="References" tabindex="-1">References <a class="header-anchor" href="#References" aria-label="Permalink to &quot;References {#References}&quot;">​</a></h1><ol><li><p><a id="Legendre_1805"></a> A. M. Legendre. <em>Mémoire sur les Opérations Trigonométriques: dont les Résultats Dépendent de la Figure de la Terre</em> (F. Didot, 1805).</p></li><li><p><a id="Gauss_1809"></a> C. F. Gauss. <em>Theoria motus corporum coelestum</em>. In: <em>Werke</em> (Königlich Preussische Akademie der Wissenschaften, 1809).</p></li><li><p><a id="Golub_Van-Loan_1996"></a> G. H. Golub and C. F. Van Loan. <em>Matrix Computations</em> (Johns Hopkins University Press, 1996).</p></li><li><p><a id="Liu_Nocedal_1989"></a> D. C. Liu and J. Nocedal. <em>On the limited memory BFGS method for large scale optimization</em>. Mathematical Programming <strong>45</strong>, 503–528 (1989).</p></li><li><p><a id="Bottou_2010"></a> L. Bottou. <em>Large-scale machine learning with stochastic gradient descent</em>. In: <em>Proceedings of COMPSTAT&#39;2010</em> (Springer, 2010); pp. 177–186.</p></li><li><p><a id="Li_Zhang_Chen_ea_2014"></a> M. Li, T. Zhang, Y. Chen and A. J. Smola. <em>Efficient mini-batch training for stochastic optimization</em>. In: <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2014); pp. 661–670.</p></li><li><p><a id="Frazier_2018"></a> P. I. Frazier. <em>A tutorial on Bayesian optimization</em>. ArXiv:1807.02811 (2018).</p></li><li><p><a id="Izmailov_Podoprikhin_Garipov_ea_2018"></a> P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov and A. G. Wilson. <em>Averaging weights leads to wider optima and better generalization</em>. ArXiv:1803.05407 (2018).</p></li><li><p><a id="Frankle_Carbin_2018"></a> J. Frankle and M. Carbin. <em>The lottery ticket hypothesis: Finding sparse, trainable neural networks</em>. ArXiv:1803.03635 (2018).</p></li><li><p><a id="Russell_Norvig_2016"></a> S. J. Russell and P. Norvig. <em>Artificial Intelligence: A Modern Approach</em> (Pearson Education Limited, 2016).</p></li><li><p><a id="Black_Scholes_1973"></a> F. Black and M. Scholes. <em>The pricing of options and corporate liabilities</em>. Journal of Political Economy <strong>81</strong>, 637–654 (1973).</p></li><li><p><a id="Naor_Reingold_1999"></a> M. Naor and O. Reingold. <em>On the construction of pseudorandom permutations: Luby–Rackoff revisited</em>. Journal of Cryptology <strong>12</strong>, 29–66 (1999).</p></li><li><p><a id="Vapnik_1992"></a> V. Vapnik. <em>Principles of risk minimization for learning theory</em>. In: <em>Advances in Neural Information Processing Systems</em> (1992); pp. 831–838.</p></li><li><p><a id="Bergstra_Breuleux_Bastien_ea_2010"></a> J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley and Y. Bengio. <em>Theano: A CPU and GPU math compiler in Python</em>. In: <em>Proc. 9th Python in Science Conference</em>, Vol. 1 (2010); pp. 3–10.</p></li><li><p><a id="Dean_Corrado_Monga_ea_2012"></a> J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker and al. <em>Large scale distributed deep networks</em>. In: <em>Proceedings of the 25th International Conference on Neural Information Processing Systems, Volume 1</em> (2012); pp. 1223–1231.</p></li><li><p><a id="Jia_Shelhamer_Donahue_ea_2014"></a> Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama and T. Darrell. <em>Caffe: Convolutional architecture for fast feature embedding</em>. In: <em>Proceedings of the 22nd ACM International Conference on Multimedia</em> (2014); pp. 675–678.</p></li><li><p><a id="Bottou_Le-Cun_1988"></a> L. Bottou and Y. Le Cun. <a href="http://leon.bottou.org/papers/bottou-lecun-88" target="_blank" rel="noreferrer"><em>SN: A simulator for connectionist models</em></a>. In: <em>Proceedings of NeuroNimes 88</em> (Nimes, France, 1988); pp. 371–382.</p></li><li><p><a id="Chen_Li_Li_ea_2015"></a> T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang and Z. Zhang. <em>MXNET: A flexible and efficient machine learning library for heterogeneous distributed systems</em>. ArXiv:1512.01274 (2015).</p></li><li><p><a id="Frostig_Johnson_Leary_2018"></a> R. Frostig, M. J. Johnson and C. Leary. <em>Compiling machine learning programs via high-level tracing</em>. In: <em>Proceedings of Systems for Machine Learning</em> (GoogleResearch, 2018).</p></li><li><p><a id="Paszke_Gross_Massa_ea_2019"></a> A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga and al. <em>PyTorch: An imperative style, high-performance deep learning library</em>. Advances in Neural Information Processing Systems <strong>32</strong>, 8026–8037 (2019).</p></li><li><p><a id="Abadi_Barham_Chen_ea_2016"></a> M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard and al. <em>TensorFlow: A system for large-scale machine learning</em>. In: <em>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</em> (2016); pp. 265–283.</p></li><li><p><a id="Deng_Dong_Socher_ea_2009"></a> J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and L. Fei-Fei. <em>Imagenet: A large-scale hierarchical image database</em>. In: <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> (IEEE, 2009); pp. 248–255.</p></li><li><p><a id="thomee2016yfcc100m"></a> B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth and L.-J. Li. <em>YFCC100M: The new data in multimedia research</em>. Communications of the ACM <strong>59</strong>, 64–73 (2016).</p></li><li><p><a id="Vapnik98"></a> V. Vapnik. <em>Statistical Learning Theory</em> (John Wiley and Sons, New York, 1998).</p></li><li><p><a id="boucheron2005theory"></a> S. Boucheron, O. Bousquet and G. Lugosi. <em>Theory of classification: A survey of some recent advances</em>. ESAIM: Probability and Statistics <strong>9</strong>, 323–375 (2005).</p></li><li><p><a id="vapnik1994measuring"></a> V. Vapnik, E. Levin and Y. Le Cun. <em>Measuring the VC-dimension of a learning machine</em>. Neural Computation <strong>6</strong>, 851–876 (1994).</p></li><li><p><a id="Scholkopf_Smola_2002"></a> B. Schölkopf and A. J. Smola. <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</em> (MIT Press, 2002).</p></li><li><p><a id="ong2005learning"></a> C. S. Ong, A. Smola and R. Williamson. <em>Learning the kernel with hyperkernels</em>. Journal of Machine Learning Research <strong>6</strong>, 1043–1071 (2005).</p></li><li><p><a id="Tsoumakas_Katakis_2007"></a> G. Tsoumakas and I. Katakis. <em>Multi-label classification: An overview</em>. International Journal of Data Warehousing and Mining <strong>3</strong>, 1–13 (2007).</p></li><li><p><a id="Huang_Xu_Yu_2015"></a> Z. Huang, W. Xu and K. Yu. <em>Bidirectional LSTM–CRF models for sequence tagging</em>. ArXiv:1508.01991 (2015).</p></li><li><p><a id="Moon_Smola_Chang_ea_2010"></a> T. Moon, A. Smola, Y. Chang and Z. Zheng. <em>Intervalrank: isotonic regression with listwise and pairwise constraints</em>. In: <em>Proceedings of the 3rd ACM International Conference on Web Search and Data Mining</em> (2010); pp. 151–160.</p></li><li><p><a id="Beutel_Murray_Faloutsos_ea_2014"></a> A. Beutel, K. Murray, C. Faloutsos and A. J. Smola. <em>CoBaFi: collaborative Bayesian filtering</em>. In: <em>Proceedings of the 23rd International Conference on World Wide Web</em> (2014); pp. 97–108.</p></li><li><p><a id="Shannon_1948"></a> C. E. Shannon. <em>A Mathematical Theory of Communication</em>. The Bell System Technical Journal <strong>27</strong>, 379–423 (1948).</p></li><li><p><a id="Yang_Moczulski_Denil_ea_2015"></a> Z. Yang, M. Moczulski, M. Denil, N. De Freitas, A. Smola, L. Song and Z. Wang. <em>Deep fried convnets</em>. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em> (2015); pp. 1476–1483.</p></li><li><p><a id="sindhwani2015structured"></a> V. Sindhwani, T. N. Sainath and S. Kumar. <em>Structured transforms for small-footprint deep learning</em>. ArXiv:1510.01722 (2015).</p></li><li><p><a id="Zhang_Tay_Zhang_ea_2021"></a> A. Zhang, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui and J. Fu. <em>Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</em>. In: <em>International Conference on Learning Representations</em> (2021).</p></li><li><p><a id="Bradley_Terry_1952"></a> R. A. Bradley and M. E. Terry. <em>Rank analysis of incomplete block designs: I. The method of paired comparisons</em>. Biometrika <strong>39</strong>, 324–345 (1952).</p></li><li><p><a id="LeCun_Bottou_Bengio_ea_1998"></a> Y. LeCun, L. Bottou, Y. Bengio and P. Haffner. <em>Gradient-based learning applied to document recognition</em>. Proceedings of the IEEE <strong>86</strong>, 2278–2324 (1998).</p></li><li><p><a id="LeCun_Jackel_Bottou_ea_1995"></a> Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker, H. Drucker, I. Guyon, U. Muller, E. Sackinger and al. <em>Comparison of learning algorithms for handwritten digit recognition</em>. In: <em>International Conference on Artificial Neural Networks</em> (1995); pp. 53–60.</p></li><li><p><a id="Scholkopf_Burges_Vapnik_1996"></a> B. Schölkopf, C. Burges and V. Vapnik. <em>Incorporating invariances in support vector learning machines</em>. In: <em>International Conference on Artificial Neural Networks</em> (Springer, 1996); pp. 47–52.</p></li><li><p><a id="Simard_LeCun_Denker_ea_1998"></a> P. Y. Simard, Y. A. LeCun, J. S. Denker and B. Victorri. <em>Transformation invariance in pattern recognition – tangent distance and tangent propagation</em>. In: <em>Neural Networks: Tricks of the Trade</em>, Vol. 529 no. 7587 (Springer, 1998); pp. 239–274.</p></li><li><p><a id="Xiao_Rasul_Vollgraf_2017"></a> H. Xiao, K. Rasul and R. Vollgraf. <em>Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</em>. ArXiv:1708.07747 (2017).</p></li><li><p><a id="dwork2015preserving"></a> C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold and A. L. Roth. <em>Preserving statistical validity in adaptive data analysis</em>. In: <em>Proceedings of the 47th Annual ACM Symposium on Theory of Computing</em> (2015); pp. 117–126.</p></li><li><p><a id="VapChe64"></a> V. Vapnik and A. Chervonenkis. <em>A note on one class of perceptrons</em>. Automation and Remote Control <strong>25</strong> (1964).</p></li><li><p><a id="VapChe68"></a> V. Vapnik and A. Chervonenkis. <em>Uniform convergence of frequencies of occurence of events to their probabilities</em>. Dokl.Ãkad.Ñauk SSSR <strong>181</strong>, 915–918 (1968).</p></li><li><p><a id="VapChe71"></a> V. Vapnik and A. Chervonenkis. <em>On the uniform convergence of relative frequencies of events to their probabilities</em>. Theory Probab. Appl. <strong>16</strong>, 264–281 (1971).</p></li><li><p><a id="VapChe74b"></a> V. Vapnik and A. Chervonenkis. <em>Ordered risk minimization</em>. Automation and Remote Control <strong>35</strong>, 1226–1235, 1403–1412 (1974).</p></li><li><p><a id="VapChe81"></a> V. Vapnik and A. Chervonenkis. <em>The necessary and sufficient conditions for the uniform convergence of averages to their expected values</em>. Teoriya Veroyatnostei i Ee Primeneniya <strong>26</strong>, 543–564 (1981).</p></li><li><p><a id="VapChe91"></a> V. Vapnik and A. Chervonenkis. <em>The necessary and sufficient conditions for consistency in the empirical risk minimization method</em>. Pattern Recognition and Image Analysis <strong>1</strong>, 283–305 (1991).</p></li><li><p><a id="Shao_Yao_Sun_ea_2020"></a> H. Shao, S. Yao, D. Sun, A. Zhang, S. Liu, D. Liu, J. Wang and T. Abdelzaher. <em>ControlVAE: Controllable variational autoencoder</em>. In: <em>Proceedings of the 37th International Conference on Machine Learning</em> (JMLR. org, 2020).</p></li><li><p><a id="Fisher_1928"></a> R. A. Fisher. <em>Statistical Methods for Research Workers.</em> (Oliver &amp; Boyd, 1925).</p></li><li><p><a id="quinlan2014c4"></a> J. R. Quinlan. <em>C4.5: Programs for Machine Learning</em>. Vol. 51 no. 4 (Elsevier, 1993); p. 66.</p></li><li><p><a id="Aronszajn_1950"></a> N. Aronszajn. <em>Theory of reproducing kernels</em>. Transactions of the American Mathematical Society <strong>68</strong>, 337–404 (1950).</p></li><li><p><a id="Wahba_1990"></a> G. Wahba. <em>Spline Models for Observational Data</em> (SIAM, 1990).</p></li><li><p><a id="Cajal_Azoulay_1894"></a> S. Ramón y Cajal and L. Azoulay. <em>Les Nouvelles Idées sur la Structure du Système Nerveux chez l&#39;Homme et chez les Vertébrés</em>. Vol. 11 no. 2 (Paris, C. Reinwald &amp; Cie, 1894); p. 125.</p></li><li><p><a id="McCulloch_Pitts_1943"></a> W. S. McCulloch and W. Pitts. <em>A logical calculus of the ideas immanent in nervous activity</em>. Bulletin of Mathematical Biophysics <strong>5</strong>, 115–133 (1943).</p></li><li><p><a id="LeCun_Bottou_Orr_ea_1998"></a> Y. LeCun, L. Bottou, G. Orr and K.-R. Muller. <em>Efficient backprop</em>. In: <em>Neural Networks: Tricks of the Trade</em> (Springer, 1998).</p></li><li><p><a id="Kalman_Kwasny_1992"></a> B. L. Kalman and S. C. Kwasny. <em>Why tanh: choosing a sigmoidal function</em>. In: <em>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</em> (IEEE, 1992); pp. 578–581.</p></li><li><p><a id="Hendrycks_Gimpel_2016"></a> D. Hendrycks and K. Gimpel. <em>Gaussian error linear units (GELUs)</em>. ArXiv:1606.08415 (2016).</p></li><li><p><a id="Ramachandran_Zoph_Le_2017"></a> P. Ramachandran, B. Zoph and Q. V. Le. <em>Searching for activation functions</em>. ArXiv:1710.05941 (2017).</p></li><li><p><a id="Ioffe_Szegedy_2015"></a> S. Ioffe and C. Szegedy. <em>Batch normalization: Accelerating deep network training by reducing internal covariate shift</em>. ArXiv:1502.03167 (2015).</p></li><li><p><a id="Xiao_Bahri_Sohl-Dickstein_ea_2018"></a> L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. Schoenholz and J. Pennington. <em>Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks</em>. In: <em>International Conference on Machine Learning</em> (2018); pp. 5393–5402.</p></li><li><p><a id="You_Gitman_Ginsburg_2017"></a> Y. You, I. Gitman and B. Ginsburg. <em>Large batch training of convolutional networks</em>. ArXiv:1708.03888 (2017).</p></li><li><p><a id="wolpert1995no"></a> D. H. Wolpert and W. G. Macready. <em>No free lunch theorems for search</em> (Technical Report SFI-TR-95-02-010, Santa Fe Institute, 1995).</p></li><li><p><a id="zhang2021understanding"></a> C. Zhang, S. Bengio, M. Hardt, B. Recht and O. Vinyals. <em>Understanding deep learning (still) requires rethinking generalization</em>. Communications of the ACM <strong>64</strong>, 107–115 (2021).</p></li><li><p><a id="nakkiran2021deep"></a> P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak and I. Sutskever. <em>Deep double descent: Where bigger models and more data hurt</em>. Journal of Statistical Mechanics: Theory and Experiment <strong>2021</strong>, 124003 (2021).</p></li><li><p><a id="Jacot_Grabriel_Hongler_2018"></a> A. Jacot, F. Gabriel and C. Hongler. <em>Neural tangent kernel: Convergence and generalization in neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em>, Vol. 31 (2018).</p></li><li><p><a id="Rolnick_Veit_Belongie_Shavit_2017"></a> D. Rolnick, A. Veit, S. Belongie and N. Shavit. <em>Deep learning is robust to massive label noise</em>. ArXiv:1705.10694 (2017).</p></li><li><p><a id="Garg_Balakrishnan_Kolter_Lipton_2021"></a> S. Garg, S. Balakrishnan, Z. Kolter and Z. Lipton. <em>RATT: Leveraging unlabeled data to guarantee generalization</em>. In: <em>International Conference on Machine Learning</em>, Vol. 31 (PMLR, 2021); pp. 3598–3609.</p></li><li><p><a id="Srivastava_Hinton_Krizhevsky_ea_2014"></a> N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov. <em>Dropout: a simple way to prevent neural networks from overfitting</em>. Journal of Machine Learning Research <strong>15</strong>, 1929–1958 (2014).</p></li><li><p><a id="Bishop_1995"></a> C. M. Bishop. <em>Training with noise is equivalent to Tikhonov regularization</em>. Neural Computation <strong>7</strong>, 108–116 (1995).</p></li><li><p><a id="Hubel_Wiesel_1959"></a> D. H. Hubel and T. N. Wiesel. <em>Receptive fields of single neurones in the cat&#39;s striate cortex</em>. Journal of Physiology <strong>148</strong>, 574–591 (1959).</p></li><li><p><a id="Hubel_Wiesel_1962"></a> D. H. Hubel and T. N. Wiesel. <em>Receptive fields, binocular interaction and functional architecture in the cat&#39;s visual cortex</em>. Journal of Physiology <strong>160</strong>, 106–154 (1962).</p></li><li><p><a id="Hubel_Wiesel_1968"></a> D. H. Hubel and T. N. Wiesel. <em>Receptive fields and functional architecture of monkey striate cortex</em>. Journal of Physiology <strong>195</strong>, 215–243 (1968).</p></li><li><p><a id="Field_1987"></a> D. J. Field. <em>Relations between the statistics of natural images and the response properties of cortical cells</em>. JOSA A <strong>4</strong>, 2379–2394 (1987).</p></li><li><p><a id="Kuzovkin_Vicente_Petton_ea_2018"></a> I. Kuzovkin, R. Vicente, M. Petton, J.-P. Lachaux, M. Baciu, P. Kahane, S. Rheims, J. R. Vidal and J. Aru. <em>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex</em>. Communications Biology <strong>1</strong>, 1–12 (2018).</p></li><li><p><a id="Alsallakh_Kokhlikyan_Miglani_ea_2020"></a> B. Alsallakh, N. Kokhlikyan, V. Miglani, J. Yuan and O. Reblitz-Richardson. <em>Mind the PAD – CNNs can develop blind spots</em>. ArXiv:2010.02178 (2020).</p></li><li><p><a id="Lin_Chen_Yan_2013"></a> M. Lin, Q. Chen and S. Yan. <em>Network in network</em>. ArXiv:1312.4400 (2013).</p></li><li><p><a id="Szegedy_Ioffe_Vanhoucke_ea_2017"></a> C. Szegedy, S. Ioffe, V. Vanhoucke and A. A. Alemi. <em>Inception-v4, Inception-ResNet and the impact of residual connections on learning</em>. In: <em>31st AAAI Conference on Artificial Intelligence</em> (2017).</p></li><li><p><a id="Xie_Girshick_Dollar_ea_2017"></a> S. Xie, R. Girshick, P. Dollár, Z. Tu and K. He. <em>Aggregated residual transformations for deep neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2017); pp. 1492–1500.</p></li><li><p><a id="Riesenhuber_Poggio_1999"></a> M. Riesenhuber and T. Poggio. <em>Hierarchical models of object recognition in cortex</em>. Nature Neuroscience <strong>2</strong>, 1019–1025 (1999).</p></li><li><p><a id="Yamaguchi_Sakamoto_Akabane_ea_1990"></a> K. Yamaguchi, K. Sakamoto, T. Akabane and Y. Fujimoto. <em>A neural network for speaker-independent isolated word recognition</em>. In: <em>First International Conference on Spoken Language Processing</em> (1990).</p></li><li><p><a id="LeCun_Boser_Denker_ea_1989"></a> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard and L. D. Jackel. <em>Backpropagation applied to handwritten zip code recognition</em>. Neural Computation <strong>1</strong>, 541–551 (1989).</p></li><li><p><a id="Zhang_Sun_Jiang_ea_2021"></a> Y. Zhang, P. Sun, Y. Jiang, D. Yu, Z. Yuan, P. Luo, W. Liu and X. Wang. <em>ByteTrack: Multi-object tracking by associating every detection box</em>. ArXiv:2110.06864 (2021).</p></li><li><p><a id="Long_Shelhamer_Darrell_2015"></a> J. Long, E. Shelhamer and T. Darrell. <em>Fully convolutional networks for semantic segmentation</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015); pp. 3431–3440.</p></li><li><p><a id="Redmon_Farhadi_2018"></a> J. Redmon and A. Farhadi. <em>YOLOv3: An incremental improvement</em>. ArXiv:1804.02767 (2018).</p></li><li><p><a id="Gatys_Ecker_Bethge_2016"></a> L. A. Gatys, A. S. Ecker and M. Bethge. <em>Image style transfer using convolutional neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp. 2414–2423.</p></li><li><p><a id="Dosovitskiy_Beyer_Kolesnikov_ea_2021"></a> A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly and al. <em>An image is worth 16 x 16 words: Transformers for image recognition at scale</em>. In: <em>International Conference on Learning Representations</em> (2021).</p></li><li><p><a id="liu2021swin"></a> Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin and B. Guo. <em>Swin transformer: Hierarchical vision transformer using shifted windows</em>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, Vol. 34 (2021); pp. 10012–10022.</p></li><li><p><a id="Krizhevsky_Sutskever_Hinton_2012"></a> A. Krizhevsky, I. Sutskever and G. E. Hinton. <em>ImageNet classification with deep convolutional neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2012); pp. 1097–1105.</p></li><li><p><a id="Simonyan_Zisserman_2014"></a> K. Simonyan and A. Zisserman. <em>Very deep convolutional networks for large-scale image recognition</em>. ArXiv:1409.1556 (2014).</p></li><li><p><a id="Szegedy_Liu_Jia_ea_2015"></a> C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke and A. Rabinovich. <em>Going deeper with convolutions</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015); pp. 1–9.</p></li><li><p><a id="He_Zhang_Ren_ea_2016"></a> K. He, X. Zhang, S. Ren and J. Sun. <em>Deep residual learning for image recognition</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp. 770–778.</p></li><li><p><a id="Huang_Liu_Van-Der-Maaten_ea_2017"></a> G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger. <em>Densely connected convolutional networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2017); pp. 4700–4708.</p></li><li><p><a id="wu2018shift"></a> B. Wu, A. Wan, X. Yue, P. Jin, S. Zhao, N. Golmant, A. Gholaminejad, J. Gonzalez and K. Keutzer. <em>Shift: A zero flop, zero parameter alternative to spatial convolutions</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018); pp. 9127–9135.</p></li><li><p><a id="Howard_Sandler_Chu_ea_2019"></a> A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le and H. Adam. <em>Searching for MobileNetV3</em>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (2019); pp. 1314–1324.</p></li><li><p><a id="Radosavovic_Kosaraju_Girshick_ea_2020"></a> I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He and P. Dollár. <em>Designing network design spaces</em>. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (2020); pp. 10428–10436.</p></li><li><p><a id="liu2022convnet"></a> Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell and S. Xie. <em>A ConvNet for the 2020s</em>. ArXiv:2201.03545 (2022).</p></li><li><p><a id="Freund_Schapire_ea_1996"></a> Y. Freund and R. E. Schapire. <em>Experiments with a new boosting algorithm</em>. In: <em>Proceedings of the International Conference on Machine Learning</em>, Vol. 96 (Citeseer, 1996); pp. 148–156.</p></li><li><p><a id="Taskar_Guestrin_Koller_2004"></a> B. Taskar, C. Guestrin and D. Koller. <em>Max-margin Markov networks</em>. Advances in Neural Information Processing Systems <strong>16</strong>, 25 (2004).</p></li><li><p><a id="Lowe_2004"></a> D. G. Lowe. <em>Distinctive image features from scale-invariant keypoints</em>. International Journal of Computer Vision <strong>60</strong>, 91–110 (2004).</p></li><li><p><a id="Bay_Tuytelaars_Van-Gool_2006"></a> H. Bay, T. Tuytelaars and L. Van Gool. <em>SURF: Speeded up robust features</em>. In: <em>European Conference on Computer Vision</em> (Springer, 2006); pp. 404–417.</p></li><li><p><a id="Sivic_Zisserman_2003"></a> J. Sivic and A. Zisserman. <em>Video Google: A text retrieval approach to object matching in videos</em>. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em>, Vol. 3 (IEEE Computer Society, 2003); pp. 1470–1470.</p></li><li><p><a id="Hartley_Zisserman_2000"></a> R. Hartley and A. Zisserman. <em>Multiple View Geometry in Computer Vision</em> (Cambridge University Press, 2000).</p></li><li><p><a id="Glorot_Bengio_2010"></a> X. Glorot and Y. Bengio. <em>Understanding the difficulty of training deep feedforward neural networks</em>. In: <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em>, Vol. 4 (2010); pp. 249–256.</p></li><li><p><a id="Kingma_Ba_2014"></a> D. P. Kingma and J. Ba. <em>Adam: A method for stochastic optimization</em>. ArXiv:1412.6980 (2014).</p></li><li><p><a id="Nair_Hinton_2010"></a> V. Nair and G. E. Hinton. <em>Rectified linear units improve restricted Boltzmann machines</em>. In: <em>ICML</em> (2010).</p></li><li><p><a id="Boyd_Vandenberghe_2004"></a> S. Boyd and L. Vandenberghe. <em>Convex Optimization</em> (Cambridge University Press, Cambridge, England, 2004).</p></li><li><p><a id="hartley2009global"></a> R. I. Hartley and F. Kahl. <em>Global optimization through rotation space search</em>. International Journal of Computer Vision <strong>82</strong>, 64–79 (2009).</p></li><li><p><a id="Dalal_Triggs_2005"></a> N. Dalal and B. Triggs. <em>Histograms of oriented gradients for human detection</em>. In: <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#39;05)</em>, Vol. 1 (IEEE, 2005); pp. 886–893.</p></li><li><p><a id="olshausen1996emergence"></a> B. A. Olshausen and D. J. Field. <em>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</em>. Nature <strong>381</strong>, 607–609 (1996).</p></li><li><p><a id="le2013building"></a> Q. V. Le. <em>Building high-level features using large scale unsupervised learning</em>. In: <em>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</em> (IEEE, 2013); pp. 8595–8598.</p></li><li><p><a id="Miller_1995"></a> G. A. Miller. <em>WordNet: a lexical database for English</em>. Communications of the ACM <strong>38</strong>, 39–41 (1995).</p></li><li><p><a id="Torralba_Fergus_Freeman_2008"></a> A. Torralba, R. Fergus and W. T. Freeman. <em>80 million tiny images: A large data set for nonparametric object and scene recognition</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>30</strong>, 1958–1970 (2008).</p></li><li><p><a id="russakovsky2015imagenet"></a> O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein and al. <em>ImageNet large scale visual recognition challenge</em>. International Journal of Computer Vision <strong>115</strong>, 211–252 (2015).</p></li><li><p><a id="schuhmann2022laion"></a> C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman and al. <em>LAION-5B: An open large-scale dataset for training next generation image-text models</em>. ArXiv:2210.08402 (2022).</p></li><li><p><a id="Fernando_2004"></a> R. Fernando. <em>GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics</em>. Vol. 2 (Addison-Wesley, 2004).</p></li><li><p><a id="Russakovsky_Deng_Huang_ea_2013"></a> O. Russakovsky, J. Deng, Z. Huang, A. C. Berg and L. Fei-Fei. <em>Detecting avocados to zucchinis: what have we done, and where are we going?</em> In: <em>International Conference on Computer Vision (ICCV)</em>, Vol. 5 no. 3 (2013); p. 1.</p></li><li><p><a id="Buslaev_Iglovikov_Khvedchenya_ea_2020"></a> A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin and A. A. Kalinin. <em>Albumentations: Fast and flexible image augmentations</em>. Information <strong>11</strong>, 125 (2020).</p></li><li><p><a id="Mead_1980"></a> C. Mead. <em>Introduction to VLSI systems</em>. IEE Proceedings I-Solid-State and Electron Devices <strong>128</strong>, 18 (1980).</p></li><li><p><a id="bommasani2021opportunities"></a> R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill and al. <em>On the opportunities and risks of foundation models</em>. ArXiv:2108.07258 (2021).</p></li><li><p><a id="lavin2016fast"></a> A. Lavin and S. Gray. <em>Fast algorithms for convolutional neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, No. 3 (MIT Press, 2016); pp. 4013–4021.</p></li><li><p><a id="Goyal_Bochkovskiy_Deng_ea_2021"></a> A. Goyal, A. Bochkovskiy, J. Deng and V. Koltun. <em>Non-deep networks</em>. ArXiv:2110.07641 (2021).</p></li><li><p><a id="Szegedy_Vanhoucke_Ioffe_ea_2016"></a> C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna. <em>Rethinking the Inception architecture for computer vision</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp. 2818–2826.</p></li><li><p><a id="friedman1987exploratory"></a> J. H. Friedman. <em>Exploratory projection pursuit</em>. Journal of the American Statistical Association <strong>82</strong>, 249–266 (1987).</p></li><li><p><a id="guyon2008feature"></a> I. Guyon, S. Gunn, M. Nikravesh and L. A. Zadeh. <em>Feature Extraction: Foundations and Applications</em> (Springer, 2008).</p></li><li><p><a id="Vapnik95"></a> V. Vapnik. <em>The Nature of Statistical Learning Theory</em> (Springer, New York, 1995).</p></li><li><p><a id="Novikoff62"></a> A. B. J. Novikoff. <em>On convergence proofs on perceptrons</em>. In: <em>Proceedings of the Symposium on the Mathematical Theory of Automata</em> (Polytechnic Institute of Brooklyn, 1962); pp. 615–622.</p></li><li><p><a id="Ba_Kiros_Hinton_2016"></a> J. L. Ba, J. R. Kiros and G. E. Hinton. <em>Layer normalization</em>. ArXiv:1607.06450 (2016).</p></li><li><p><a id="Duchi_Hazan_Singer_2011"></a> J. Duchi, E. Hazan and Y. Singer. <em>Adaptive subgradient methods for online learning and stochastic optimization</em>. Journal of Machine Learning Research <strong>12</strong>, 2121–2159 (2011).</p></li><li><p><a id="Zaheer_Reddi_Sachan_ea_2018"></a> M. Zaheer, S. Reddi, D. Sachan, S. Kale and S. Kumar. <em>Adaptive methods for nonconvex optimization</em>. In: <em>Advances in Neural Information Processing Systems</em> (2018); pp. 9793–9803.</p></li><li><p><a id="anil2020scalable"></a> R. Anil, V. Gupta, T. Koren, K. Regan and Y. Singer. <em>Scalable second-order optimization for deep learning</em>. ArXiv:2002.09018 (2020).</p></li><li><p><a id="Teye_Azizpour_Smith_2018"></a> M. Teye, H. Azizpour and K. Smith. <em>Bayesian uncertainty estimation for batch normalized deep networks</em>. ArXiv:1802.06455 (2018).</p></li><li><p><a id="Luo_Wang_Shao_ea_2018"></a> P. Luo, X. Wang, W. Shao and Z. Peng. <em>Towards understanding regularization in batch normalization</em>. ArXiv:1809.00846 (2018).</p></li><li><p><a id="Lipton_Steinhardt_2018"></a> Z. C. Lipton and J. Steinhardt. <em>Troubling trends in machine learning scholarship</em>. Communications of the ACM <strong>17</strong>, 45–77 (2018).</p></li><li><p><a id="Santurkar_Tsipras_Ilyas_ea_2018"></a> S. Santurkar, D. Tsipras, A. Ilyas and A. Madry. <em>How does batch normalization help optimization?</em> In: <em>Advances in Neural Information Processing Systems</em> (2018); pp. 2483–2493.</p></li><li><p><a id="wang2022removing"></a> H. Wang, A. Zhang, S. Zheng, X. Shi, M. Li and Z. Wang. <em>Removing batch normalization boosts adversarial training</em>. In: <em>International Conference on Machine Learning</em> (PMLR, 2022); pp. 23433–23445.</p></li><li><p><a id="tikhonov1977solutions"></a> A. N. Tikhonov and V. Y. Arsenin. <em>Solutions of Ill-Posed Problems</em>. Vol. 2021 no. 12 (W.H. Winston, 1977); p. 124003.</p></li><li><p><a id="morozov2012methods"></a> V. A. Morozov. <em>Methods for Solving Incorrectly Posed Problems</em> (Springer, 1984).</p></li><li><p><a id="prakash2016neural"></a> A. Prakash, S. A. Hasan, K. Lee, V. Datla, A. Qadir, J. Liu and O. Farri. <em>Neural paraphrase generation with stacked residual LSTM networks</em>. ArXiv:1610.03098 (2016).</p></li><li><p><a id="kim2017residual"></a> J. Kim, M. El-Khamy and J. Lee. <em>Residual LSTM: Design of a deep recurrent architecture for distant speech recognition</em>. ArXiv:1701.03360 (2017).</p></li><li><p><a id="Vaswani_Shazeer_Parmar_ea_2017"></a> A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser and I. Polosukhin. <em>Attention is all you need</em>. In: <em>Advances in Neural Information Processing Systems</em> (2017); pp. 5998–6008.</p></li><li><p><a id="Kipf_Welling_2016"></a> T. N. Kipf and M. Welling. <em>Semi-supervised classification with graph convolutional networks</em>. ArXiv:1609.02907 (2016).</p></li><li><p><a id="Ren_He_Girshick_ea_2015"></a> S. Ren, K. He, R. Girshick and J. Sun. <em>Faster R-CNN: Towards real-time object detection with region proposal networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2015); pp. 91–99.</p></li><li><p><a id="srivastava2015highway"></a> R. K. Srivastava, K. Greff and J. Schmidhuber. <em>Highway networks</em>. ArXiv:1505.00387 (2015).</p></li><li><p><a id="He_Zhang_Ren_ea_2016_1"></a> K. He, X. Zhang, S. Ren and J. Sun. <em>Identity mappings in deep residual networks</em>. In: <em>European Conference on Computer Vision</em> (Springer, 2016); pp. 630–645.</p></li><li><p><a id="pleiss2017memory"></a> G. Pleiss, D. Chen, G. Huang, T. Li, L. Van Der Maaten and K. Q. Weinberger. <em>Memory-efficient implementation of densenets</em>. ArXiv:1707.06990 (2017).</p></li><li><p><a id="Hu_Shen_Sun_2018"></a> J. Hu, L. Shen and G. Sun. <em>Squeeze-and-excitation networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018); pp. 7132–7141.</p></li><li><p><a id="zoph2016neural"></a> B. Zoph and Q. V. Le. <em>Neural architecture search with reinforcement learning</em>. ArXiv:1611.01578 (2016).</p></li><li><p><a id="liu2018darts"></a> H. Liu, K. Simonyan and Y. Yang. <em>DARTS: Differentiable architecture search</em>. ArXiv:1806.09055 (2018).</p></li><li><p><a id="tan2019efficientnet"></a> M. Tan and Q. Le. <em>EfficientNet: Rethinking model scaling for convolutional neural networks</em>. In: <em>International Conference on Machine Learning</em> (PMLR, 2019); pp. 6105–6114.</p></li><li><p><a id="graves2008novel"></a> A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke and J. Schmidhuber. <em>A novel connectionist system for unconstrained handwriting recognition</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>31</strong>, 855–868 (2008).</p></li><li><p><a id="Sutskever_Vinyals_Le_2014"></a> I. Sutskever, O. Vinyals and Q. V. Le. <em>Sequence to sequence learning with neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2014); pp. 3104–3112.</p></li><li><p><a id="Lipton_Kale_2016"></a> Z. C. Lipton, D. C. Kale, C. Elkan and R. Wetzel. <em>Learning to diagnose with LSTM recurrent neural networks</em>. In: <em>International Conference on Learning Representations (ICLR)</em> (2016).</p></li><li><p><a id="Lipton_Berkowitz_Elkan_2015"></a> Z. C. Lipton, J. Berkowitz and C. Elkan. <em>A critical review of recurrent neural networks for sequence learning</em>. ArXiv:1506.00019 <strong>17</strong>, 45–77 (2015).</p></li><li><p><a id="Hoyer_Janzing_Mooij_ea_2009"></a> P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters and B. Schölkopf. <em>Nonlinear causal discovery with additive noise models</em>. In: <em>Advances in Neural Information Processing Systems</em> (2009); pp. 689–696.</p></li><li><p><a id="Peters_Janzing_Scholkopf_2017"></a> J. Peters, D. Janzing and B. Schölkopf. <em>Elements of Causal Inference: Foundations and Learning Algorithms</em> (MIT Press, 2017).</p></li><li><p><a id="Wood_Gasthaus_Archambeau_ea_2011"></a> F. Wood, J. Gasthaus, C. Archambeau, L. James and Y. W. Teh. <em>The sequence memoizer</em>. Communications of the ACM <strong>54</strong>, 91–98 (2011).</p></li><li><p><a id="Bengio_Ducharme_Vincent_ea_2003"></a> Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. <em>A neural probabilistic language model</em>. Journal of Machine Learning Research <strong>3</strong>, 1137–1155 (2003).</p></li><li><p><a id="Werbos_1990"></a> P. J. Werbos. <em>Backpropagation through time: what it does and how to do it</em>. Proceedings of the IEEE <strong>78</strong>, 1550–1560 (1990).</p></li><li><p><a id="Jaeger_2002"></a> H. Jaeger. <em>Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the ``echo state network&#39;&#39; approach</em>. Vol. 31 (GMD-Forschungszentrum Informationstechnik Bonn, 2002).</p></li><li><p><a id="Tallec_Ollivier_2017"></a> C. Tallec and Y. Ollivier. <em>Unbiasing truncated backpropagation through time</em>. ArXiv:1705.08209 (2017).</p></li><li><p><a id="elman1990finding"></a> J. L. Elman. <em>Finding structure in time</em>. Cognitive Science <strong>14</strong>, 179–211 (1990).</p></li><li><p><a id="bengio1994learning"></a> Y. Bengio, P. Simard and P. Frasconi. <em>Learning long-term dependencies with gradient descent is difficult</em>. IEEE Transactions on Neural Networks <strong>5</strong>, 157–166 (1994).</p></li><li><p><a id="Hochreiter_Bengio_Frasconi_ea_2001"></a> S. Hochreiter, Y. Bengio, P. Frasconi and J. Schmidhuber. <em>Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</em>. In: <em>A Field Guide to Dynamical Recurrent Neural Networks</em> (IEEE Press, 2001).</p></li><li><p><a id="Hochreiter_Schmidhuber_1997"></a> S. Hochreiter and J. Schmidhuber. <em>Long short-term memory</em>. Neural Computation <strong>9</strong>, 1735–1780 (1997).</p></li><li><p><a id="Cho_Van-Merrienboer_Bahdanau_ea_2014"></a> K. Cho, B. Van Merriënboer, D. Bahdanau and Y. Bengio. <em>On the properties of neural machine translation: Encoder–decoder approaches</em>. ArXiv:1409.1259 (2014).</p></li><li><p><a id="Chung_Gulcehre_Cho_ea_2014"></a> J. Chung, C. Gulcehre, K. Cho and Y. Bengio. <em>Empirical evaluation of gated recurrent neural networks on sequence modeling</em>. ArXiv:1412.3555 (2014).</p></li><li><p><a id="Schuster_Paliwal_1997"></a> M. Schuster and K. K. Paliwal. <em>Bidirectional recurrent neural networks</em>. IEEE Transactions on Signal Processing <strong>45</strong>, 2673–2681 (1997).</p></li><li><p><a id="Brown_Cocke_Della-Pietra_ea_1988"></a> P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, R. L. Mercer and P. Roossin. <em>A statistical approach to language translation</em>. In: <em>COLING Budapest 1988 Volume 1: International Conference on Computational Linguistics</em> (1988).</p></li><li><p><a id="Brown_Cocke_Della-Pietra_ea_1990"></a> P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. Lafferty, R. L. Mercer and P. S. Roossin. <em>A statistical approach to machine translation</em>. Computational Linguistics <strong>16</strong>, 79–85 (1990).</p></li><li><p><a id="Cho_Van-Merrienboer_Gulcehre_ea_2014"></a> K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk and Y. Bengio. <em>Learning phrase representations using RNN encoder–decoder for statistical machine translation</em>. ArXiv:1406.1078 (2014).</p></li><li><p><a id="Papineni_Roukos_Ward_ea_2002"></a> K. Papineni, S. Roukos, T. Ward and W.-J. Zhu. <em>BLEU: a method for automatic evaluation of machine translation</em>. In: <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em> (2002); pp. 311–318.</p></li><li><p><a id="Kalchbrenner_Grefenstette_Blunsom_2014"></a> N. Kalchbrenner, E. Grefenstette and P. Blunsom. <em>A convolutional neural network for modelling sentences</em>. ArXiv:1404.2188 (2014).</p></li><li><p><a id="yang2016neural"></a> Z. Yang, Z. Hu, Y. Deng, C. Dyer and A. Smola. <em>Neural machine translation with recurrent attention modeling</em>. ArXiv:1607.05108 (2016).</p></li><li><p><a id="Bahdanau_Cho_Bengio_2014"></a> D. Bahdanau, K. Cho and Y. Bengio. <em>Neural machine translation by jointly learning to align and translate</em>. ArXiv:1409.0473 (2014).</p></li><li><p><a id="Mnih_Heess_Graves_ea_2014"></a> V. Mnih, N. Heess, A. Graves and others. <em>Recurrent models of visual attention</em>. In: <em>Advances in Neural Information Processing Systems</em> (2014); pp. 2204–2212.</p></li><li><p><a id="Nadaraya_1964"></a> E. A. Nadaraya. <em>On estimating regression</em>. Theory of Probability &amp; its Applications <strong>9</strong>, 141–142 (1964).</p></li><li><p><a id="Watson_1964"></a> G. S. Watson. <em>Smooth regression analysis</em>. Sankhyā: The Indian Journal of Statistics, Series A, 359–372 (1964).</p></li><li><p><a id="mack1982weak"></a> Y.-P. Mack and B. W. Silverman. <em>Weak and strong uniform consistency of kernel regression estimates</em>. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete <strong>61</strong>, 405–415 (1982).</p></li><li><p><a id="Silverman86"></a> B. Silverman. <em>Density Estimation for Statistical and Data Analysis</em> (Chapman and Hall, 1986).</p></li><li><p><a id="norelli2022asif"></a> A. Norelli, M. Fumero, V. Maiorca, L. Moschella, E. Rodolà and F. Locatello. <em>ASIF: Coupled data turns unimodal models to multimodal without training</em>. ArXiv:2210.01738 (2022).</p></li><li><p><a id="shoeybi2019megatron"></a> M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper and B. Catanzaro. <em>Megatron-LM: Training multi-billion parameter language models using model parallelism</em>. ArXiv:1909.08053 (2019).</p></li><li><p><a id="Graves_2013"></a> A. Graves. <em>Generating sequences with recurrent neural networks</em>. ArXiv:1308.0850 (2013).</p></li><li><p><a id="rabiner1993fundamentals"></a> L. Rabiner and B.-H. Juang. <em>Fundamentals of Speech Recognition</em> (Prentice-Hall., 1993).</p></li><li><p><a id="chan2015listen"></a> W. Chan, N. Jaitly, Q. V. Le and O. Vinyals. <em>Listen, attend and spell</em>. ArXiv:1508.01211 (2015).</p></li><li><p><a id="Lin_Feng_Santos_ea_2017"></a> Z. Lin, M. Feng, C. N. Santos, M. Yu, B. Xiang, B. Zhou and Y. Bengio. <em>A structured self-attentive sentence embedding</em>. ArXiv:1703.03130 (2017).</p></li><li><p><a id="Cheng_Dong_Lapata_2016"></a> J. Cheng, L. Dong and M. Lapata. <em>Long short-term memory-networks for machine reading</em>. In: <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em> (2016); pp. 551–561.</p></li><li><p><a id="Parikh_Tackstrom_Das_ea_2016"></a> A. P. Parikh, O. Täckström, D. Das and J. Uszkoreit. <em>A decomposable attention model for natural language inference</em>. ArXiv:1606.01933 (2016).</p></li><li><p><a id="Paulus_Xiong_Socher_2017"></a> R. Paulus, C. Xiong and R. Socher. <em>A deep reinforced model for abstractive summarization</em>. ArXiv:1705.04304 (2017).</p></li><li><p><a id="shaw2018self"></a> P. Shaw, J. Uszkoreit and A. Vaswani. <em>Self-attention with relative position representations</em>. ArXiv:1803.02155 (2018).</p></li><li><p><a id="huang2018music"></a> C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Dinculescu and D. Eck. <em>Music transformer: generating music with long-term structure</em>. In: <em>International Conference on Learning Representations</em> (2018).</p></li></ol></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://https://github.com/ashutosh-b-b/d2l-julia/edit/master/docs/src/references.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>CH10.Attention_Mechanisms_and_Transformers/Untitled</span><!--]--></a></div><div class="pager" data-v-4f9813fa><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://luxdl.github.io/DocumenterVitepress.jl/dev/" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br></p><p class="copyright" data-v-c970a860>© Copyright 2025.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"ch10.attention_mechanisms_and_transformers_attn_1.md\":\"BPTHOId7\",\"ch10.attention_mechanisms_and_transformers_attn_2.md\":\"CSvHAO07\",\"ch10.attention_mechanisms_and_transformers_attn_3.md\":\"B6P-XMmW\",\"ch10.attention_mechanisms_and_transformers_attn_4.md\":\"CB5DEaB1\",\"ch10.attention_mechanisms_and_transformers_attn_5.md\":\"DQHNoxkK\",\"ch10.attention_mechanisms_and_transformers_attn_6.md\":\"DJp_Cd3H\",\"ch10.attention_mechanisms_and_transformers_untitled.md\":\"AZxRWAaB\",\"ch3.linear_regression_lnn_1.md\":\"DTK1fX6I\",\"ch3.linear_regression_lnn_2.md\":\"BD8w6uR8\",\"ch3.linear_regression_lnn_3.md\":\"C4MB6zPT\",\"ch3.linear_regression_lnn_4.md\":\"B7J5MW04\",\"ch3.linear_regression_lnn_5.md\":\"CfF2CfNK\",\"ch3.linear_regression_lnn_6.md\":\"uj6xm8fY\",\"ch3.linear_regression_lnn_7.md\":\"B_zpSpiN\",\"ch4.linear_classification_lcn_1.md\":\"Boi1cDsh\",\"ch4.linear_classification_lcn_2.md\":\"CkOR2Iia\",\"ch4.linear_classification_lcn_3.md\":\"V-0rtib8\",\"ch4.linear_classification_lcn_4.md\":\"BX5UP2HZ\",\"ch4.linear_classification_lcn_5.md\":\"C-7KZE9h\",\"ch4.linear_classification_lcn_6.md\":\"DNDvKaZ3\",\"ch5.mlp_mlp_1.md\":\"DNcZmrDZ\",\"ch5.mlp_mlp_2.md\":\"MI21_tyz\",\"ch5.mlp_mlp_3.md\":\"DVB63m8H\",\"ch5.mlp_mlp_4.md\":\"BYjKXVG9\",\"ch5.mlp_mlp_5.md\":\"CxEzVy5G\",\"ch5.mlp_mlp_6.md\":\"CpygNHoD\",\"ch6.convolutional_neural_networks_cnn_2.md\":\"pAndzyT8\",\"ch6.convolutional_neural_networks_cnn_3.md\":\"CyHC0BXV\",\"ch6.convolutional_neural_networks_cnn_4.md\":\"-fYwA9iM\",\"ch6.convolutional_neural_networks_cnn_5.md\":\"DKCH7I6h\",\"ch6.convolutional_neural_networks_cnn_6.md\":\"BxTNLj1_\",\"ch7.modernconvolutionalneuralnetworks_mcnn_0.md\":\"Do42Pcb-\",\"ch7.modernconvolutionalneuralnetworks_mcnn_1.md\":\"DyrfEGE0\",\"ch7.modernconvolutionalneuralnetworks_mcnn_2.md\":\"BdK6e3J7\",\"ch7.modernconvolutionalneuralnetworks_mcnn_3.md\":\"CvJU8raL\",\"ch7.modernconvolutionalneuralnetworks_mcnn_4.md\":\"DfMUvHOO\",\"ch7.modernconvolutionalneuralnetworks_mcnn_5.md\":\"CVkY2ACS\",\"ch7.modernconvolutionalneuralnetworks_mcnn_6.md\":\"Cin-j3ht\",\"ch7.modernconvolutionalneuralnetworks_mcnn_7.md\":\"BY0qhW6e\",\"ch7.modernconvolutionalneuralnetworks_mcnn_8.md\":\"CfFt59lS\",\"ch8.recurrent_neural_networks_rnn_0.md\":\"CIW34d_V\",\"ch8.recurrent_neural_networks_rnn_1.md\":\"Dqwc86d0\",\"ch8.recurrent_neural_networks_rnn_2.md\":\"DVOnaLjw\",\"ch8.recurrent_neural_networks_rnn_3.md\":\"BzfHLTyo\",\"ch8.recurrent_neural_networks_rnn_4.md\":\"DnQjCEE3\",\"ch8.recurrent_neural_networks_rnn_5.md\":\"6IPEYW7M\",\"ch8.recurrent_neural_networks_rnn_6.md\":\"GS6Ynndo\",\"ch8.recurrent_neural_networks_rnn_7.md\":\"B3soXDbW\",\"ch9.modern_recurrent_neural_networks_mrnn_1.md\":\"zGzQsIzl\",\"ch9.modern_recurrent_neural_networks_mrnn_2.md\":\"CV4lwlnE\",\"ch9.modern_recurrent_neural_networks_mrnn_3.md\":\"DI9bj1mT\",\"ch9.modern_recurrent_neural_networks_mrnn_4.md\":\"CvMg8EjP\",\"ch9.modern_recurrent_neural_networks_mrnn_5.md\":\"CuOfLj02\",\"ch9.modern_recurrent_neural_networks_mrnn_6.md\":\"BrLASjSi\",\"ch9.modern_recurrent_neural_networks_mrnn_7.md\":\"D5R7Lv49\",\"chapters.md\":\"BCfSINH3\",\"index.md\":\"C9fKtYo7\",\"references.md\":\"Dwf5fRK0\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"d2l Julia\",\"description\":\"Documentation for d2l-julia\",\"base\":\"/d2l-julia/previews/PR1/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"src\":\"/logo.png\",\"width\":24,\"height\":24},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Chapters\",\"collapsed\":false,\"items\":[{\"text\":\"📘 Chapters Overview\",\"link\":\"/chapters\"},{\"text\":\"Linear Neural Networks for Regression\",\"collapsed\":false,\"items\":[{\"text\":\"Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_1\"},{\"text\":\"Multiple Dispatch Design for Implementation\",\"link\":\"/CH3.Linear_Regression/LNN_2\"},{\"text\":\"Synthetic Regression Data\",\"link\":\"/CH3.Linear_Regression/LNN_3\"},{\"text\":\"Linear Regression Implementation from Scratch\",\"link\":\"/CH3.Linear_Regression/LNN_4\"},{\"text\":\"Concise Implementation of Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_5\"},{\"text\":\"Generalization\",\"link\":\"/CH3.Linear_Regression/LNN_6\"},{\"text\":\"Weight Decay\",\"link\":\"/CH3.Linear_Regression/LNN_7\"}]},{\"text\":\"Linear Neural Networks for Classification\",\"collapsed\":false,\"items\":[{\"text\":\"Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_1\"},{\"text\":\"The Image Classification Dataset\",\"link\":\"/CH4.Linear_Classification/LCN_2\"},{\"text\":\"Softmax Regression Implementation from Scratch\",\"link\":\"/CH4.Linear_Classification/LCN_3\"},{\"text\":\"Concise Implementation of Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_4\"},{\"text\":\"Generalization in Classification\",\"link\":\"/CH4.Linear_Classification/LCN_5\"},{\"text\":\"Environment and Distribution Shift\",\"link\":\"/CH4.Linear_Classification/LCN_6\"}]},{\"text\":\"Multilayer Perceptron\",\"collapsed\":false,\"items\":[{\"text\":\"Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_1\"},{\"text\":\"Implementation of Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_2\"},{\"text\":\"Forward Propagation, Backward Propagation, and Computational Graphs\",\"link\":\"/CH5.MLP/MLP_3\"},{\"text\":\"Numerical Stability and Initialization\",\"link\":\"/CH5.MLP/MLP_4\"},{\"text\":\"Generalization in Deep Learning\",\"link\":\"/CH5.MLP/MLP_5\"},{\"text\":\"Dropout\",\"link\":\"/CH5.MLP/MLP_6\"}]},{\"text\":\"Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Convolutions for Images\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_2\"},{\"text\":\"Padding and Stride\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_3\"},{\"text\":\"Multiple Input and Multiple Output Channels\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_4\"},{\"text\":\"Pooling\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_5\"},{\"text\":\"Convolutional Neural Networks (LeNet)\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_6\"}]},{\"text\":\"Modern Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Modern Convolutional Neural Networks\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_0\"},{\"text\":\"Deep Convolutional Neural Networks (AlexNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_1\"},{\"text\":\"Networks Using Blocks (VGG)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_2\"},{\"text\":\"Network in Network (NiN)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_3\"},{\"text\":\"Multi-Branch Networks  (GoogLeNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_4\"},{\"text\":\"Batch Normalization\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_5\"},{\"text\":\"Residual Networks (ResNet) and ResNeXt\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_6\"},{\"text\":\"Densely Connected Networks (DenseNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_7\"},{\"text\":\"Designing Convolution Network Architectures\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_8\"}]},{\"text\":\"Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_0\"},{\"text\":\"Working with Sequences\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_1\"},{\"text\":\"Converting Raw Text into Sequence Data\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_2\"},{\"text\":\"Language Models\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_3\"},{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_4\"},{\"text\":\"Recurrent Neural Network Implementation from Scratch\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_5\"},{\"text\":\"Concise Implementation of Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_6\"},{\"text\":\"Backpropagation Through Time\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_7\"}]},{\"text\":\"Modern Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Long Short-Term Memory (LSTM)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_1\"},{\"text\":\"Gated Recurrent Units (GRU)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_2\"},{\"text\":\"Deep Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_3\"},{\"text\":\"Bidirectional Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_4\"},{\"text\":\"Machine Translation and the Dataset\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_5\"},{\"text\":\"The Encoder–Decoder Architecture\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_6\"},{\"text\":\"Sequence-to-Sequence Learning for Machine Translation\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_7\"}]},{\"text\":\"Attention Mechanisms and Transformers\",\"collapsed\":false,\"items\":[{\"text\":\"Queries, Keys, and Values\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_1\"},{\"text\":\"Attention Pooling by Similarity\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_2\"},{\"text\":\"Attention Scoring Functions\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_3\"},{\"text\":\"The Bahdanau Attention Mechanism\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_4\"},{\"text\":\"Multi-Head Attention\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_5\"},{\"text\":\"Self-Attention and Positional Encoding\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_6\"},{\"text\":\"CH10.Attention_Mechanisms_and_Transformers/Untitled\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/Untitled\"}]}]},{\"text\":\"References\",\"link\":\"/references\"},{\"component\":\"VersionPicker\"}],\"sidebar\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Chapters\",\"collapsed\":false,\"items\":[{\"text\":\"📘 Chapters Overview\",\"link\":\"/chapters\"},{\"text\":\"Linear Neural Networks for Regression\",\"collapsed\":false,\"items\":[{\"text\":\"Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_1\"},{\"text\":\"Multiple Dispatch Design for Implementation\",\"link\":\"/CH3.Linear_Regression/LNN_2\"},{\"text\":\"Synthetic Regression Data\",\"link\":\"/CH3.Linear_Regression/LNN_3\"},{\"text\":\"Linear Regression Implementation from Scratch\",\"link\":\"/CH3.Linear_Regression/LNN_4\"},{\"text\":\"Concise Implementation of Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_5\"},{\"text\":\"Generalization\",\"link\":\"/CH3.Linear_Regression/LNN_6\"},{\"text\":\"Weight Decay\",\"link\":\"/CH3.Linear_Regression/LNN_7\"}]},{\"text\":\"Linear Neural Networks for Classification\",\"collapsed\":false,\"items\":[{\"text\":\"Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_1\"},{\"text\":\"The Image Classification Dataset\",\"link\":\"/CH4.Linear_Classification/LCN_2\"},{\"text\":\"Softmax Regression Implementation from Scratch\",\"link\":\"/CH4.Linear_Classification/LCN_3\"},{\"text\":\"Concise Implementation of Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_4\"},{\"text\":\"Generalization in Classification\",\"link\":\"/CH4.Linear_Classification/LCN_5\"},{\"text\":\"Environment and Distribution Shift\",\"link\":\"/CH4.Linear_Classification/LCN_6\"}]},{\"text\":\"Multilayer Perceptron\",\"collapsed\":false,\"items\":[{\"text\":\"Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_1\"},{\"text\":\"Implementation of Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_2\"},{\"text\":\"Forward Propagation, Backward Propagation, and Computational Graphs\",\"link\":\"/CH5.MLP/MLP_3\"},{\"text\":\"Numerical Stability and Initialization\",\"link\":\"/CH5.MLP/MLP_4\"},{\"text\":\"Generalization in Deep Learning\",\"link\":\"/CH5.MLP/MLP_5\"},{\"text\":\"Dropout\",\"link\":\"/CH5.MLP/MLP_6\"}]},{\"text\":\"Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Convolutions for Images\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_2\"},{\"text\":\"Padding and Stride\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_3\"},{\"text\":\"Multiple Input and Multiple Output Channels\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_4\"},{\"text\":\"Pooling\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_5\"},{\"text\":\"Convolutional Neural Networks (LeNet)\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_6\"}]},{\"text\":\"Modern Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Modern Convolutional Neural Networks\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_0\"},{\"text\":\"Deep Convolutional Neural Networks (AlexNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_1\"},{\"text\":\"Networks Using Blocks (VGG)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_2\"},{\"text\":\"Network in Network (NiN)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_3\"},{\"text\":\"Multi-Branch Networks  (GoogLeNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_4\"},{\"text\":\"Batch Normalization\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_5\"},{\"text\":\"Residual Networks (ResNet) and ResNeXt\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_6\"},{\"text\":\"Densely Connected Networks (DenseNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_7\"},{\"text\":\"Designing Convolution Network Architectures\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_8\"}]},{\"text\":\"Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_0\"},{\"text\":\"Working with Sequences\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_1\"},{\"text\":\"Converting Raw Text into Sequence Data\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_2\"},{\"text\":\"Language Models\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_3\"},{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_4\"},{\"text\":\"Recurrent Neural Network Implementation from Scratch\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_5\"},{\"text\":\"Concise Implementation of Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_6\"},{\"text\":\"Backpropagation Through Time\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_7\"}]},{\"text\":\"Modern Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Long Short-Term Memory (LSTM)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_1\"},{\"text\":\"Gated Recurrent Units (GRU)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_2\"},{\"text\":\"Deep Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_3\"},{\"text\":\"Bidirectional Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_4\"},{\"text\":\"Machine Translation and the Dataset\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_5\"},{\"text\":\"The Encoder–Decoder Architecture\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_6\"},{\"text\":\"Sequence-to-Sequence Learning for Machine Translation\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_7\"}]},{\"text\":\"Attention Mechanisms and Transformers\",\"collapsed\":false,\"items\":[{\"text\":\"Queries, Keys, and Values\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_1\"},{\"text\":\"Attention Pooling by Similarity\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_2\"},{\"text\":\"Attention Scoring Functions\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_3\"},{\"text\":\"The Bahdanau Attention Mechanism\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_4\"},{\"text\":\"Multi-Head Attention\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_5\"},{\"text\":\"Self-Attention and Positional Encoding\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_6\"},{\"text\":\"CH10.Attention_Mechanisms_and_Transformers/Untitled\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/Untitled\"}]}]},{\"text\":\"References\",\"link\":\"/references\"}],\"editLink\":{\"pattern\":\"https://https://github.com/ashutosh-b-b/d2l-julia/edit/master/docs/src/:path\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/ashutosh-b-b/d2l-julia\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/dev/\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>\",\"copyright\":\"© Copyright 2025.\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>