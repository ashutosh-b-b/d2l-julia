<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multi-Head Attention · d2l Julia</title><meta name="title" content="Multi-Head Attention · d2l Julia"/><meta property="og:title" content="Multi-Head Attention · d2l Julia"/><meta property="twitter:title" content="Multi-Head Attention · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../ATTN_4/">The Bahdanau Attention Mechanism</a></li><li class="is-active"><a class="tocitem" href>Multi-Head Attention</a><ul class="internal"><li><a class="tocitem" href="#Model"><span>Model</span></a></li><li><a class="tocitem" href="#Implementation"><span>Implementation</span></a></li></ul></li><li><a class="tocitem" href="../ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Attention Mechanisms and Transformers</a></li><li class="is-active"><a href>Multi-Head Attention</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multi-Head Attention</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Multi-Head-Attention"><a class="docs-heading-anchor" href="#Multi-Head-Attention">Multi-Head Attention</a><a id="Multi-Head-Attention-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Head-Attention" title="Permalink"></a></h1><p>In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.</p><p>To this end, instead of performing  a single attention pooling, queries, keys, and values can be transformed with <span>$h$</span> independently learned linear projections. Then these <span>$h$</span> projected queries, keys, and values are fed into attention pooling in parallel. In the end, <span>$h$</span> attention-pooling outputs are concatenated and  transformed with another learned linear projection to produce the final output. This design is called <em>multi-head attention</em>, where each of the <span>$h$</span> attention pooling outputs is a <em>head</em> :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>. Using fully connected layers to perform learnable linear transformations, :numref:<code>fig_multi-head-attention</code> describes multi-head attention.</p><p><img src="../../img/multi-head-attention.svg" alt="Multi-head attention, where multiple heads are concatenated then linearly transformed."/> :label:<code>fig_multi-head-attention</code></p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using LinearAlgebra
using d2lai
using Flux 
using Downloads
using StatsBase
using Plots
using d2lai: DotProductAttention, Seq2Seq, AbstractEncoderDecoder, Seq2SeqEncoder, AdditiveAttention, StackedRNN, Seq2SeqAttentionDecoder</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/workspace/d2l-julia/d2lai`</code></pre><h2 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h2><p>Before providing the implementation of multi-head attention, let&#39;s formalize this model mathematically. Given a query <span>$\mathbf{q} \in \mathbb{R}^{d_q}$</span>, a key <span>$\mathbf{k} \in \mathbb{R}^{d_k}$</span>, and a value <span>$\mathbf{v} \in \mathbb{R}^{d_v}$</span>, each attention head <span>$\mathbf{h}_i$</span>  (<span>$i = 1, \ldots, h$</span>) is computed as</p><p class="math-container">\[\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},\]</p><p>where  <span>$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$</span>, <span>$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$</span>, and <span>$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$</span> are learnable parameters and <span>$f$</span> is attention pooling, such as additive attention and scaled dot product attention in :numref:<code>sec_attention-scoring-functions</code>. The multi-head attention output is another linear transformation via  learnable parameters <span>$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$</span> of the concatenation of <span>$h$</span> heads:</p><p class="math-container">\[\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\]</p><p>Based on this design, each head may attend to different parts of the input. More sophisticated functions  than the simple weighted average can be expressed.</p><h2 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h2><p>In our implementation, we choose the scaled dot product attention for each head of the multi-head attention. To avoid significant growth of computational cost and parametrization cost, we set <span>$p_q = p_k = p_v = p_o / h$</span>. Note that <span>$h$</span> heads can be computed in parallel if we set the number of outputs  of linear transformations for the query, key, and value to <span>$p_q h = p_k h = p_v h = p_o$</span>. In the following implementation, <span>$p_o$</span> is specified via the argument <code>num_hiddens</code>.</p><pre><code class="language-julia hljs">struct MultiHeadedAttention{W, AT, A} &lt;: AbstractModel
    weights::W 
    attention::AT
    args::A
end

Flux.@layer MultiHeadedAttention trainable = (weights, )
function MultiHeadedAttention(num_hiddens::Int64, num_heads::Int64, dropout::AbstractFloat; bias=false)
    W_q = Dense(num_hiddens, num_hiddens, bias = bias)
    W_k = Dense(num_hiddens, num_hiddens, bias = bias)
    W_v = Dense(num_hiddens, num_hiddens, bias = bias)
    W_o = Dense(num_hiddens, num_hiddens, bias = bias)
    attn = DotProductAttention(Dropout(dropout), (;))
    MultiHeadedAttention((; W_q, W_k, W_v, W_o), attn, (; num_hiddens, num_heads, dropout))
end

function (m::MultiHeadedAttention)(queries, keys, values, valid_lens)
    # queries -&gt; q_d x num_queries x batch_size 
    # keys -&gt; k_d x num_key_val x batch_size 
    # values -&gt; v_d x num_key_val x batch_size
    queries = m.weights.W_q(queries) # num_hiddens x num_queries x batch_size
    queries = transpose_qkv(m, queries) # (num_hiddens / num_heads) x num_queries x (num_heads * batch_size)
    keys = transpose_qkv(m, m.weights.W_k(keys)) # (num_hiddens / num_heads) x num_key_val x (num_heads * batch_size)
    values = transpose_qkv(m, m.weights.W_v(values))# (num_hiddens / num_heads) x num_key_val x (num_heads * batch_size)
    valid_lens = if !isnothing(valid_lens)
        isa(valid_lens, AbstractVector) ? repeat(valid_lens, inner = m.args.num_heads) : repeat(valid_lens, inner = (m.args.num_heads, 1))
        
    end
    scores, attn_wts = m.attention(queries, keys, values, valid_lens) # (num_hiddens / num_heads) x num_queries x (num_heads * batch_size) 
    # attn_wts -&gt; num_key_val x num_queries x batch_size
    output_concat = transpose_output(m, scores) # num_hiddens x num_queries x batch_size
    return m.weights.W_o(output_concat), attn_wts # 
end

function transpose_qkv(m::MultiHeadedAttention, x)
    # x -&gt; num_hiddens x (num_queries or num_key_val) x batch_size 
    num_q_or_key_val = size(x, 2)
    batch_size = size(x, 3)
    x_ = reshape(x, :, m.args.num_heads, num_q_or_key_val, batch_size)
    x_permuted = permutedims(x_, [1, 3, 2, 4]) # (num_hiddens / num_heads) x num_queries x num_heads x batch_size
    return reshape(x_permuted, size(x_permuted)[1], size(x_permuted)[2], :) # (num_hiddens / num_heads) x num_queries x (num_heads * batch_size)
end

function transpose_output(m::MultiHeadedAttention, x)
    x_ = reshape(x, size(x)[1], size(x)[2], m.args.num_heads, :)
    x_permuted = permutedims(x_, [1, 3, 2, 4])
    return reshape(x, :, size(x_permuted)[3], size(x_permuted)[4]) 
end
</code></pre><pre><code class="nohighlight hljs">transpose_output (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">num_hiddens, num_heads = 100, 5
attention = MultiHeadedAttention(num_hiddens, num_heads, 0.5)
batch_size, num_queries, num_kvpairs = 2, 4, 6
valid_lens = [3, 2]
X = ones((num_hiddens, num_queries, batch_size))
Y = ones((num_hiddens, num_kvpairs, batch_size))

context, attn_wt = attention(X, Y, Y, valid_lens)
@assert size(context) == (num_hiddens, num_queries, batch_size)</code></pre><pre><code class="nohighlight hljs">┌ Warning: Layer with Float32 parameters got Float64 input.
│   The input will be converted, but any earlier layers may be very slow.
│   layer = Dense(100 =&gt; 100; bias=false)  # 10_000 parameters
│   summary(x) = &quot;100×8 Matrix{Float64}&quot;
└ @ Flux ~/.julia/packages/Flux/3711C/src/layers/stateless.jl:60</code></pre><pre><code class="language-julia hljs">function Seq2SeqMultiAttentionDecoder(vocab_size::Int, embed_size::Int, num_hiddens, num_layers, num_heads, dropout=0.)
    embedding = Embedding(vocab_size =&gt; embed_size)
    rnn = StackedRNN(embed_size + num_hiddens, num_hiddens, num_layers; rnn = Flux.LSTM)
    attention = MultiHeadedAttention(num_hiddens, num_heads, dropout)
    dense = Dense(num_hiddens, vocab_size) 
    args = (; vocab_size, embed_size, num_hiddens, num_layers)
    Seq2SeqAttentionDecoder(attention, embedding, rnn, dense, args)
end




data = d2lai.MTFraEng(128)
embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2
num_heads = 8

encoder = Seq2SeqEncoder(length(data.src_vocab), embed_size, num_hiddens, num_layers)
decoder = Seq2SeqMultiAttentionDecoder(length(data.tgt_vocab), embed_size, num_hiddens, 1, num_heads)
model = Seq2Seq(encoder, decoder, data.tgt_vocab[&quot;&lt;pad&gt;&quot;])

opt = Flux.Adam(0.01)
trainer = Trainer(model, data, opt; max_epochs = 30, gpu = true, gradient_clip_val = 1.)
m, _ = d2lai.fit(trainer);</code></pre><pre><code class="nohighlight hljs">Internal error: during type inference of
_pullback_generator(UInt64, LineNumberNode, Type, Type, Type, NTuple{1026, DataType})
Encountered stack overflow.
This might be caused by recursion over very long tuples or argument lists.
Internal error: during type inference of
_pullback(Zygote.Context{false}, typeof(Core.kwcall), NamedTuple{(:dims,), Tuple{Int64}}, typeof(Base.cat), Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2}, Array{Bool, 2})
Encountered stack overflow.
This might be caused by recursion over very long tuples or argument lists.
Internal error: during type inference of
_pullback_generator(UInt64, LineNumberNode, Type, Type, Type, NTuple{1026, DataType})
Encountered stack overflow.
This might be caused by recursion over very long tuples or argument lists.</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 4.8276343, Val Loss: 5.4717245
    [ Info: Train Loss: 4.3027673, Val Loss: 5.6018443
    [ Info: Train Loss: 3.273195, Val Loss: 4.5906806
    [ Info: Train Loss: 2.963611, Val Loss: 4.9624586
    [ Info: Train Loss: 2.6126888, Val Loss: 4.4686055
    [ Info: Train Loss: 2.471468, Val Loss: 5.0693927
    [ Info: Train Loss: 2.0902474, Val Loss: 4.507521
    [ Info: Train Loss: 1.9265594, Val Loss: 4.5847497
    [ Info: Train Loss: 1.6047107, Val Loss: 5.0555325
    [ Info: Train Loss: 1.4116989, Val Loss: 4.5631604
    [ Info: Train Loss: 1.2966719, Val Loss: 4.7002335
    [ Info: Train Loss: 1.1375505, Val Loss: 4.794996
    [ Info: Train Loss: 0.92881125, Val Loss: 4.6975164
    [ Info: Train Loss: 0.91435474, Val Loss: 4.6757035
    [ Info: Train Loss: 0.7080981, Val Loss: 4.946134
    [ Info: Train Loss: 0.7106923, Val Loss: 4.8681574
    [ Info: Train Loss: 0.60006666, Val Loss: 4.701098
    [ Info: Train Loss: 0.6312529, Val Loss: 5.1001134
    [ Info: Train Loss: 0.55986136, Val Loss: 4.9545436
    [ Info: Train Loss: 0.5404057, Val Loss: 4.772108
    [ Info: Train Loss: 0.5702013, Val Loss: 4.9377155
    [ Info: Train Loss: 0.5331384, Val Loss: 5.2816014
    [ Info: Train Loss: 0.48318955, Val Loss: 4.9302464
    [ Info: Train Loss: 0.49934065, Val Loss: 5.0310044
    [ Info: Train Loss: 0.49979848, Val Loss: 5.26014
    [ Info: Train Loss: 0.5294968, Val Loss: 5.193127
    [ Info: Train Loss: 0.46126893, Val Loss: 5.001597
    [ Info: Train Loss: 0.38484415, Val Loss: 5.1434236
    [ Info: Train Loss: 0.4936912, Val Loss: 5.6913314
    [ Info: Train Loss: 0.5257853, Val Loss: 5.145835</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip170">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip170)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip171">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip170)" d="M225.325 1423.18 L2352.76 1423.18 L2352.76 47.2441 L225.325 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip172">
    <rect x="225" y="47" width="2128" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="562.364,1423.18 562.364,47.2441 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="908.4,1423.18 908.4,47.2441 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1254.44,1423.18 1254.44,47.2441 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1600.47,1423.18 1600.47,47.2441 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1946.51,1423.18 1946.51,47.2441 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2292.55,1423.18 2292.55,47.2441 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="225.325,1175.09 2352.76,1175.09 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="225.325,879.886 2352.76,879.886 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="225.325,584.681 2352.76,584.681 "/>
<polyline clip-path="url(#clip172)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="225.325,289.477 2352.76,289.477 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="225.325,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="562.364,1423.18 562.364,1404.28 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="908.4,1423.18 908.4,1404.28 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1254.44,1423.18 1254.44,1404.28 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1600.47,1423.18 1600.47,1404.28 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1946.51,1423.18 1946.51,1404.28 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2292.55,1423.18 2292.55,1404.28 "/>
<path clip-path="url(#clip170)" d="M552.642 1451.02 L570.998 1451.02 L570.998 1454.96 L556.924 1454.96 L556.924 1463.43 Q557.943 1463.08 558.961 1462.92 Q559.98 1462.73 560.998 1462.73 Q566.785 1462.73 570.165 1465.9 Q573.545 1469.08 573.545 1474.49 Q573.545 1480.07 570.072 1483.17 Q566.6 1486.25 560.281 1486.25 Q558.105 1486.25 555.836 1485.88 Q553.591 1485.51 551.184 1484.77 L551.184 1480.07 Q553.267 1481.2 555.489 1481.76 Q557.711 1482.32 560.188 1482.32 Q564.193 1482.32 566.531 1480.21 Q568.869 1478.1 568.869 1474.49 Q568.869 1470.88 566.531 1468.77 Q564.193 1466.67 560.188 1466.67 Q558.313 1466.67 556.438 1467.08 Q554.586 1467.5 552.642 1468.38 L552.642 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M883.088 1481.64 L890.727 1481.64 L890.727 1455.28 L882.417 1456.95 L882.417 1452.69 L890.681 1451.02 L895.356 1451.02 L895.356 1481.64 L902.995 1481.64 L902.995 1485.58 L883.088 1485.58 L883.088 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M922.44 1454.1 Q918.829 1454.1 917 1457.66 Q915.194 1461.2 915.194 1468.33 Q915.194 1475.44 917 1479.01 Q918.829 1482.55 922.44 1482.55 Q926.074 1482.55 927.879 1479.01 Q929.708 1475.44 929.708 1468.33 Q929.708 1461.2 927.879 1457.66 Q926.074 1454.1 922.44 1454.1 M922.44 1450.39 Q928.25 1450.39 931.305 1455 Q934.384 1459.58 934.384 1468.33 Q934.384 1477.06 931.305 1481.67 Q928.25 1486.25 922.44 1486.25 Q916.63 1486.25 913.551 1481.67 Q910.495 1477.06 910.495 1468.33 Q910.495 1459.58 913.551 1455 Q916.63 1450.39 922.44 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1229.62 1481.64 L1237.26 1481.64 L1237.26 1455.28 L1228.95 1456.95 L1228.95 1452.69 L1237.21 1451.02 L1241.89 1451.02 L1241.89 1481.64 L1249.53 1481.64 L1249.53 1485.58 L1229.62 1485.58 L1229.62 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1259.02 1451.02 L1277.38 1451.02 L1277.38 1454.96 L1263.3 1454.96 L1263.3 1463.43 Q1264.32 1463.08 1265.34 1462.92 Q1266.36 1462.73 1267.38 1462.73 Q1273.16 1462.73 1276.54 1465.9 Q1279.92 1469.08 1279.92 1474.49 Q1279.92 1480.07 1276.45 1483.17 Q1272.98 1486.25 1266.66 1486.25 Q1264.48 1486.25 1262.21 1485.88 Q1259.97 1485.51 1257.56 1484.77 L1257.56 1480.07 Q1259.64 1481.2 1261.87 1481.76 Q1264.09 1482.32 1266.57 1482.32 Q1270.57 1482.32 1272.91 1480.21 Q1275.25 1478.1 1275.25 1474.49 Q1275.25 1470.88 1272.91 1468.77 Q1270.57 1466.67 1266.57 1466.67 Q1264.69 1466.67 1262.82 1467.08 Q1260.96 1467.5 1259.02 1468.38 L1259.02 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1579.25 1481.64 L1595.57 1481.64 L1595.57 1485.58 L1573.62 1485.58 L1573.62 1481.64 Q1576.28 1478.89 1580.87 1474.26 Q1585.47 1469.61 1586.65 1468.27 Q1588.9 1465.74 1589.78 1464.01 Q1590.68 1462.25 1590.68 1460.56 Q1590.68 1457.8 1588.74 1456.07 Q1586.82 1454.33 1583.71 1454.33 Q1581.51 1454.33 1579.06 1455.09 Q1576.63 1455.86 1573.85 1457.41 L1573.85 1452.69 Q1576.68 1451.55 1579.13 1450.97 Q1581.58 1450.39 1583.62 1450.39 Q1588.99 1450.39 1592.19 1453.08 Q1595.38 1455.77 1595.38 1460.26 Q1595.38 1462.39 1594.57 1464.31 Q1593.78 1466.2 1591.68 1468.8 Q1591.1 1469.47 1588 1472.69 Q1584.89 1475.88 1579.25 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1615.38 1454.1 Q1611.77 1454.1 1609.94 1457.66 Q1608.13 1461.2 1608.13 1468.33 Q1608.13 1475.44 1609.94 1479.01 Q1611.77 1482.55 1615.38 1482.55 Q1619.01 1482.55 1620.82 1479.01 Q1622.65 1475.44 1622.65 1468.33 Q1622.65 1461.2 1620.82 1457.66 Q1619.01 1454.1 1615.38 1454.1 M1615.38 1450.39 Q1621.19 1450.39 1624.25 1455 Q1627.32 1459.58 1627.32 1468.33 Q1627.32 1477.06 1624.25 1481.67 Q1621.19 1486.25 1615.38 1486.25 Q1609.57 1486.25 1606.49 1481.67 Q1603.44 1477.06 1603.44 1468.33 Q1603.44 1459.58 1606.49 1455 Q1609.57 1450.39 1615.38 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1925.78 1481.64 L1942.1 1481.64 L1942.1 1485.58 L1920.16 1485.58 L1920.16 1481.64 Q1922.82 1478.89 1927.4 1474.26 Q1932.01 1469.61 1933.19 1468.27 Q1935.43 1465.74 1936.31 1464.01 Q1937.22 1462.25 1937.22 1460.56 Q1937.22 1457.8 1935.27 1456.07 Q1933.35 1454.33 1930.25 1454.33 Q1928.05 1454.33 1925.6 1455.09 Q1923.16 1455.86 1920.39 1457.41 L1920.39 1452.69 Q1923.21 1451.55 1925.66 1450.97 Q1928.12 1450.39 1930.16 1450.39 Q1935.53 1450.39 1938.72 1453.08 Q1941.91 1455.77 1941.91 1460.26 Q1941.91 1462.39 1941.1 1464.31 Q1940.32 1466.2 1938.21 1468.8 Q1937.63 1469.47 1934.53 1472.69 Q1931.43 1475.88 1925.78 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1951.96 1451.02 L1970.32 1451.02 L1970.32 1454.96 L1956.24 1454.96 L1956.24 1463.43 Q1957.26 1463.08 1958.28 1462.92 Q1959.3 1462.73 1960.32 1462.73 Q1966.1 1462.73 1969.48 1465.9 Q1972.86 1469.08 1972.86 1474.49 Q1972.86 1480.07 1969.39 1483.17 Q1965.92 1486.25 1959.6 1486.25 Q1957.42 1486.25 1955.16 1485.88 Q1952.91 1485.51 1950.5 1484.77 L1950.5 1480.07 Q1952.59 1481.2 1954.81 1481.76 Q1957.03 1482.32 1959.51 1482.32 Q1963.51 1482.32 1965.85 1480.21 Q1968.19 1478.1 1968.19 1474.49 Q1968.19 1470.88 1965.85 1468.77 Q1963.51 1466.67 1959.51 1466.67 Q1957.63 1466.67 1955.76 1467.08 Q1953.91 1467.5 1951.96 1468.38 L1951.96 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M2281.39 1466.95 Q2284.74 1467.66 2286.62 1469.93 Q2288.52 1472.2 2288.52 1475.53 Q2288.52 1480.65 2285 1483.45 Q2281.48 1486.25 2275 1486.25 Q2272.82 1486.25 2270.51 1485.81 Q2268.22 1485.39 2265.76 1484.54 L2265.76 1480.02 Q2267.71 1481.16 2270.02 1481.74 Q2272.34 1482.32 2274.86 1482.32 Q2279.26 1482.32 2281.55 1480.58 Q2283.87 1478.84 2283.87 1475.53 Q2283.87 1472.48 2281.71 1470.77 Q2279.58 1469.03 2275.76 1469.03 L2271.74 1469.03 L2271.74 1465.19 L2275.95 1465.19 Q2279.4 1465.19 2281.23 1463.82 Q2283.05 1462.43 2283.05 1459.84 Q2283.05 1457.18 2281.16 1455.77 Q2279.28 1454.33 2275.76 1454.33 Q2273.84 1454.33 2271.64 1454.75 Q2269.44 1455.16 2266.8 1456.04 L2266.8 1451.88 Q2269.47 1451.14 2271.78 1450.77 Q2274.12 1450.39 2276.18 1450.39 Q2281.5 1450.39 2284.61 1452.83 Q2287.71 1455.23 2287.71 1459.35 Q2287.71 1462.22 2286.06 1464.21 Q2284.42 1466.18 2281.39 1466.95 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M2307.38 1454.1 Q2303.77 1454.1 2301.94 1457.66 Q2300.14 1461.2 2300.14 1468.33 Q2300.14 1475.44 2301.94 1479.01 Q2303.77 1482.55 2307.38 1482.55 Q2311.02 1482.55 2312.82 1479.01 Q2314.65 1475.44 2314.65 1468.33 Q2314.65 1461.2 2312.82 1457.66 Q2311.02 1454.1 2307.38 1454.1 M2307.38 1450.39 Q2313.19 1450.39 2316.25 1455 Q2319.33 1459.58 2319.33 1468.33 Q2319.33 1477.06 2316.25 1481.67 Q2313.19 1486.25 2307.38 1486.25 Q2301.57 1486.25 2298.49 1481.67 Q2295.44 1477.06 2295.44 1468.33 Q2295.44 1459.58 2298.49 1455 Q2301.57 1450.39 2307.38 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1209.23 1548.76 L1209.23 1551.62 L1182.3 1551.62 Q1182.69 1557.67 1185.93 1560.85 Q1189.21 1564 1195.03 1564 Q1198.41 1564 1201.56 1563.17 Q1204.74 1562.35 1207.86 1560.69 L1207.86 1566.23 Q1204.71 1567.57 1201.4 1568.27 Q1198.09 1568.97 1194.68 1568.97 Q1186.15 1568.97 1181.16 1564 Q1176.19 1559.04 1176.19 1550.57 Q1176.19 1541.82 1180.9 1536.69 Q1185.65 1531.54 1193.67 1531.54 Q1200.86 1531.54 1205.03 1536.18 Q1209.23 1540.8 1209.23 1548.76 M1203.37 1547.04 Q1203.31 1542.23 1200.67 1539.37 Q1198.06 1536.5 1193.73 1536.5 Q1188.83 1536.5 1185.87 1539.27 Q1182.94 1542.04 1182.49 1547.07 L1203.37 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1224.51 1562.7 L1224.51 1581.6 L1218.62 1581.6 L1218.62 1532.4 L1224.51 1532.4 L1224.51 1537.81 Q1226.35 1534.62 1229.15 1533.1 Q1231.99 1531.54 1235.9 1531.54 Q1242.4 1531.54 1246.44 1536.69 Q1250.51 1541.85 1250.51 1550.25 Q1250.51 1558.65 1246.44 1563.81 Q1242.4 1568.97 1235.9 1568.97 Q1231.99 1568.97 1229.15 1567.44 Q1226.35 1565.88 1224.51 1562.7 M1244.43 1550.25 Q1244.43 1543.79 1241.76 1540.13 Q1239.12 1536.44 1234.47 1536.44 Q1229.82 1536.44 1227.15 1540.13 Q1224.51 1543.79 1224.51 1550.25 Q1224.51 1556.71 1227.15 1560.4 Q1229.82 1564.07 1234.47 1564.07 Q1239.12 1564.07 1241.76 1560.4 Q1244.43 1556.71 1244.43 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1274.03 1536.5 Q1269.32 1536.5 1266.59 1540.19 Q1263.85 1543.85 1263.85 1550.25 Q1263.85 1556.65 1266.55 1560.34 Q1269.29 1564 1274.03 1564 Q1278.71 1564 1281.45 1560.31 Q1284.19 1556.62 1284.19 1550.25 Q1284.19 1543.92 1281.45 1540.23 Q1278.71 1536.5 1274.03 1536.5 M1274.03 1531.54 Q1281.67 1531.54 1286.03 1536.5 Q1290.39 1541.47 1290.39 1550.25 Q1290.39 1559 1286.03 1564 Q1281.67 1568.97 1274.03 1568.97 Q1266.36 1568.97 1262 1564 Q1257.67 1559 1257.67 1550.25 Q1257.67 1541.47 1262 1536.5 Q1266.36 1531.54 1274.03 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1325.75 1533.76 L1325.75 1539.24 Q1323.27 1537.87 1320.76 1537.2 Q1318.27 1536.5 1315.73 1536.5 Q1310.03 1536.5 1306.88 1540.13 Q1303.73 1543.73 1303.73 1550.25 Q1303.73 1556.78 1306.88 1560.4 Q1310.03 1564 1315.73 1564 Q1318.27 1564 1320.76 1563.33 Q1323.27 1562.63 1325.75 1561.26 L1325.75 1566.68 Q1323.3 1567.82 1320.66 1568.39 Q1318.05 1568.97 1315.09 1568.97 Q1307.04 1568.97 1302.3 1563.91 Q1297.55 1558.85 1297.55 1550.25 Q1297.55 1541.53 1302.33 1536.53 Q1307.13 1531.54 1315.47 1531.54 Q1318.18 1531.54 1320.76 1532.11 Q1323.34 1532.65 1325.75 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1365.57 1546.53 L1365.57 1568.04 L1359.72 1568.04 L1359.72 1546.72 Q1359.72 1541.66 1357.74 1539.14 Q1355.77 1536.63 1351.82 1536.63 Q1347.08 1536.63 1344.34 1539.65 Q1341.61 1542.68 1341.61 1547.9 L1341.61 1568.04 L1335.72 1568.04 L1335.72 1518.52 L1341.61 1518.52 L1341.61 1537.93 Q1343.71 1534.72 1346.54 1533.13 Q1349.4 1531.54 1353.13 1531.54 Q1359.27 1531.54 1362.42 1535.36 Q1365.57 1539.14 1365.57 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M1399.98 1533.45 L1399.98 1538.98 Q1397.5 1537.71 1394.82 1537.07 Q1392.15 1536.44 1389.28 1536.44 Q1384.92 1536.44 1382.73 1537.77 Q1380.56 1539.11 1380.56 1541.79 Q1380.56 1543.82 1382.12 1545 Q1383.68 1546.15 1388.39 1547.2 L1390.4 1547.64 Q1396.64 1548.98 1399.25 1551.43 Q1401.89 1553.85 1401.89 1558.21 Q1401.89 1563.17 1397.94 1566.07 Q1394.03 1568.97 1387.15 1568.97 Q1384.29 1568.97 1381.17 1568.39 Q1378.08 1567.85 1374.64 1566.74 L1374.64 1560.69 Q1377.89 1562.38 1381.04 1563.24 Q1384.19 1564.07 1387.28 1564.07 Q1391.42 1564.07 1393.64 1562.66 Q1395.87 1561.23 1395.87 1558.65 Q1395.87 1556.27 1394.25 1554.99 Q1392.66 1553.72 1387.22 1552.54 L1385.18 1552.07 Q1379.74 1550.92 1377.32 1548.56 Q1374.9 1546.18 1374.9 1542.04 Q1374.9 1537.01 1378.46 1534.27 Q1382.03 1531.54 1388.58 1531.54 Q1391.83 1531.54 1394.7 1532.01 Q1397.56 1532.49 1399.98 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="225.325,1423.18 225.325,47.2441 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="225.325,1175.09 244.222,1175.09 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="225.325,879.886 244.222,879.886 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="225.325,584.681 244.222,584.681 "/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="225.325,289.477 244.222,289.477 "/>
<path clip-path="url(#clip170)" d="M51.6634 1194.88 L59.3023 1194.88 L59.3023 1168.52 L50.9921 1170.18 L50.9921 1165.92 L59.256 1164.26 L63.9319 1164.26 L63.9319 1194.88 L71.5707 1194.88 L71.5707 1198.82 L51.6634 1198.82 L51.6634 1194.88 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M91.0151 1167.34 Q87.404 1167.34 85.5753 1170.9 Q83.7697 1174.44 83.7697 1181.57 Q83.7697 1188.68 85.5753 1192.24 Q87.404 1195.79 91.0151 1195.79 Q94.6493 1195.79 96.4548 1192.24 Q98.2835 1188.68 98.2835 1181.57 Q98.2835 1174.44 96.4548 1170.9 Q94.6493 1167.34 91.0151 1167.34 M91.0151 1163.63 Q96.8252 1163.63 99.8808 1168.24 Q102.959 1172.82 102.959 1181.57 Q102.959 1190.3 99.8808 1194.91 Q96.8252 1199.49 91.0151 1199.49 Q85.2049 1199.49 82.1262 1194.91 Q79.0707 1190.3 79.0707 1181.57 Q79.0707 1172.82 82.1262 1168.24 Q85.2049 1163.63 91.0151 1163.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M102.959 1157.73 L127.071 1157.73 L127.071 1160.93 L102.959 1160.93 L102.959 1157.73 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M143.396 1145.83 Q140.462 1145.83 138.976 1148.72 Q137.509 1151.6 137.509 1157.4 Q137.509 1163.17 138.976 1166.07 Q140.462 1168.94 143.396 1168.94 Q146.349 1168.94 147.816 1166.07 Q149.302 1163.17 149.302 1157.4 Q149.302 1151.6 147.816 1148.72 Q146.349 1145.83 143.396 1145.83 M143.396 1142.82 Q148.117 1142.82 150.6 1146.56 Q153.101 1150.29 153.101 1157.4 Q153.101 1164.49 150.6 1168.23 Q148.117 1171.95 143.396 1171.95 Q138.675 1171.95 136.174 1168.23 Q133.691 1164.49 133.691 1157.4 Q133.691 1150.29 136.174 1146.56 Q138.675 1142.82 143.396 1142.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M159.778 1166.63 L163.746 1166.63 L163.746 1171.41 L159.778 1171.41 L159.778 1166.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M183.532 1156.27 Q186.259 1156.85 187.782 1158.69 Q189.325 1160.54 189.325 1163.24 Q189.325 1167.4 186.466 1169.68 Q183.607 1171.95 178.341 1171.95 Q176.573 1171.95 174.692 1171.6 Q172.83 1171.26 170.837 1170.56 L170.837 1166.89 Q172.417 1167.81 174.297 1168.29 Q176.178 1168.76 178.228 1168.76 Q181.802 1168.76 183.664 1167.34 Q185.544 1165.93 185.544 1163.24 Q185.544 1160.76 183.795 1159.37 Q182.065 1157.96 178.962 1157.96 L175.689 1157.96 L175.689 1154.84 L179.112 1154.84 Q181.914 1154.84 183.4 1153.73 Q184.886 1152.6 184.886 1150.49 Q184.886 1148.33 183.344 1147.18 Q181.82 1146.02 178.962 1146.02 Q177.401 1146.02 175.614 1146.36 Q173.827 1146.69 171.683 1147.41 L171.683 1144.02 Q173.846 1143.42 175.727 1143.12 Q177.626 1142.82 179.3 1142.82 Q183.626 1142.82 186.146 1144.79 Q188.666 1146.75 188.666 1150.1 Q188.666 1152.43 187.331 1154.05 Q185.996 1155.65 183.532 1156.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M81.8687 899.678 L89.5075 899.678 L89.5075 873.313 L81.1974 874.979 L81.1974 870.72 L89.4612 869.053 L94.1371 869.053 L94.1371 899.678 L101.776 899.678 L101.776 903.613 L81.8687 903.613 L81.8687 899.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M121.22 872.132 Q117.609 872.132 115.781 875.697 Q113.975 879.238 113.975 886.368 Q113.975 893.474 115.781 897.039 Q117.609 900.581 121.22 900.581 Q124.855 900.581 126.66 897.039 Q128.489 893.474 128.489 886.368 Q128.489 879.238 126.66 875.697 Q124.855 872.132 121.22 872.132 M121.22 868.428 Q127.03 868.428 130.086 873.035 Q133.165 877.618 133.165 886.368 Q133.165 895.095 130.086 899.701 Q127.03 904.285 121.22 904.285 Q115.41 904.285 112.331 899.701 Q109.276 895.095 109.276 886.368 Q109.276 877.618 112.331 873.035 Q115.41 868.428 121.22 868.428 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M142.87 850.624 Q139.936 850.624 138.45 853.521 Q136.983 856.398 136.983 862.191 Q136.983 867.965 138.45 870.861 Q139.936 873.739 142.87 873.739 Q145.822 873.739 147.289 870.861 Q148.775 867.965 148.775 862.191 Q148.775 856.398 147.289 853.521 Q145.822 850.624 142.87 850.624 M142.87 847.615 Q147.59 847.615 150.073 851.358 Q152.574 855.082 152.574 862.191 Q152.574 869.282 150.073 873.024 Q147.59 876.748 142.87 876.748 Q138.149 876.748 135.647 873.024 Q133.165 869.282 133.165 862.191 Q133.165 855.082 135.647 851.358 Q138.149 847.615 142.87 847.615 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M159.251 871.426 L163.22 871.426 L163.22 876.203 L159.251 876.203 L159.251 871.426 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M179.62 850.624 Q176.686 850.624 175.2 853.521 Q173.733 856.398 173.733 862.191 Q173.733 867.965 175.2 870.861 Q176.686 873.739 179.62 873.739 Q182.573 873.739 184.04 870.861 Q185.526 867.965 185.526 862.191 Q185.526 856.398 184.04 853.521 Q182.573 850.624 179.62 850.624 M179.62 847.615 Q184.341 847.615 186.823 851.358 Q189.325 855.082 189.325 862.191 Q189.325 869.282 186.823 873.024 Q184.341 876.748 179.62 876.748 Q174.899 876.748 172.398 873.024 Q169.915 869.282 169.915 862.191 Q169.915 855.082 172.398 851.358 Q174.899 847.615 179.62 847.615 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M82.3953 604.474 L90.0342 604.474 L90.0342 578.108 L81.724 579.775 L81.724 575.516 L89.9879 573.849 L94.6638 573.849 L94.6638 604.474 L102.303 604.474 L102.303 608.409 L82.3953 608.409 L82.3953 604.474 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M121.747 576.928 Q118.136 576.928 116.307 580.493 Q114.502 584.034 114.502 591.164 Q114.502 598.27 116.307 601.835 Q118.136 605.377 121.747 605.377 Q125.381 605.377 127.187 601.835 Q129.015 598.27 129.015 591.164 Q129.015 584.034 127.187 580.493 Q125.381 576.928 121.747 576.928 M121.747 573.224 Q127.557 573.224 130.613 577.83 Q133.691 582.414 133.691 591.164 Q133.691 599.891 130.613 604.497 Q127.557 609.08 121.747 609.08 Q115.937 609.08 112.858 604.497 Q109.803 599.891 109.803 591.164 Q109.803 582.414 112.858 577.83 Q115.937 573.224 121.747 573.224 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M143.396 555.42 Q140.462 555.42 138.976 558.316 Q137.509 561.194 137.509 566.987 Q137.509 572.761 138.976 575.657 Q140.462 578.535 143.396 578.535 Q146.349 578.535 147.816 575.657 Q149.302 572.761 149.302 566.987 Q149.302 561.194 147.816 558.316 Q146.349 555.42 143.396 555.42 M143.396 552.411 Q148.117 552.411 150.6 556.154 Q153.101 559.877 153.101 566.987 Q153.101 574.077 150.6 577.82 Q148.117 581.544 143.396 581.544 Q138.675 581.544 136.174 577.82 Q133.691 574.077 133.691 566.987 Q133.691 559.877 136.174 556.154 Q138.675 552.411 143.396 552.411 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M159.778 576.221 L163.746 576.221 L163.746 580.999 L159.778 580.999 L159.778 576.221 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M183.532 565.858 Q186.259 566.441 187.782 568.285 Q189.325 570.128 189.325 572.836 Q189.325 576.993 186.466 579.268 Q183.607 581.544 178.341 581.544 Q176.573 581.544 174.692 581.187 Q172.83 580.848 170.837 580.152 L170.837 576.485 Q172.417 577.406 174.297 577.877 Q176.178 578.347 178.228 578.347 Q181.802 578.347 183.664 576.936 Q185.544 575.526 185.544 572.836 Q185.544 570.353 183.795 568.962 Q182.065 567.551 178.962 567.551 L175.689 567.551 L175.689 564.429 L179.112 564.429 Q181.914 564.429 183.4 563.319 Q184.886 562.191 184.886 560.084 Q184.886 557.921 183.344 556.774 Q181.82 555.608 178.962 555.608 Q177.401 555.608 175.614 555.947 Q173.827 556.285 171.683 557 L171.683 553.615 Q173.846 553.013 175.727 552.712 Q177.626 552.411 179.3 552.411 Q183.626 552.411 186.146 554.386 Q188.666 556.342 188.666 559.689 Q188.666 562.022 187.331 563.639 Q185.996 565.238 183.532 565.858 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M81.737 309.27 L89.3759 309.27 L89.3759 282.904 L81.0657 284.571 L81.0657 280.311 L89.3296 278.645 L94.0055 278.645 L94.0055 309.27 L101.644 309.27 L101.644 313.205 L81.737 313.205 L81.737 309.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M121.089 281.723 Q117.478 281.723 115.649 285.288 Q113.843 288.83 113.843 295.959 Q113.843 303.066 115.649 306.631 Q117.478 310.172 121.089 310.172 Q124.723 310.172 126.528 306.631 Q128.357 303.066 128.357 295.959 Q128.357 288.83 126.528 285.288 Q124.723 281.723 121.089 281.723 M121.089 278.02 Q126.899 278.02 129.954 282.626 Q133.033 287.21 133.033 295.959 Q133.033 304.686 129.954 309.293 Q126.899 313.876 121.089 313.876 Q115.279 313.876 112.2 309.293 Q109.144 304.686 109.144 295.959 Q109.144 287.21 112.2 282.626 Q115.279 278.02 121.089 278.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M142.738 260.216 Q139.804 260.216 138.318 263.112 Q136.851 265.99 136.851 271.783 Q136.851 277.557 138.318 280.453 Q139.804 283.331 142.738 283.331 Q145.691 283.331 147.158 280.453 Q148.644 277.557 148.644 271.783 Q148.644 265.99 147.158 263.112 Q145.691 260.216 142.738 260.216 M142.738 257.207 Q147.459 257.207 149.941 260.949 Q152.443 264.673 152.443 271.783 Q152.443 278.873 149.941 282.616 Q147.459 286.34 142.738 286.34 Q138.017 286.34 135.516 282.616 Q133.033 278.873 133.033 271.783 Q133.033 264.673 135.516 260.949 Q138.017 257.207 142.738 257.207 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M159.119 281.017 L163.088 281.017 L163.088 285.794 L159.119 285.794 L159.119 281.017 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M179.958 270.24 Q177.401 270.24 175.896 271.989 Q174.41 273.739 174.41 276.785 Q174.41 279.813 175.896 281.581 Q177.401 283.331 179.958 283.331 Q182.516 283.331 184.002 281.581 Q185.507 279.813 185.507 276.785 Q185.507 273.739 184.002 271.989 Q182.516 270.24 179.958 270.24 M187.5 258.335 L187.5 261.796 Q186.071 261.119 184.604 260.761 Q183.156 260.404 181.726 260.404 Q177.965 260.404 175.971 262.943 Q173.996 265.482 173.714 270.616 Q174.824 268.98 176.498 268.115 Q178.172 267.231 180.184 267.231 Q184.416 267.231 186.861 269.808 Q189.325 272.366 189.325 276.785 Q189.325 281.111 186.767 283.726 Q184.209 286.34 179.958 286.34 Q175.087 286.34 172.511 282.616 Q169.934 278.873 169.934 271.783 Q169.934 265.125 173.094 261.175 Q176.253 257.207 181.576 257.207 Q183.005 257.207 184.454 257.489 Q185.921 257.771 187.5 258.335 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip172)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="285.535,86.1857 354.742,208.146 423.95,333.792 493.157,375.782 562.364,447.834 631.571,501.251 700.779,561.714 769.986,630.233 839.193,678.917 908.4,784.783 977.608,829.465 1046.81,903.363 1116.02,976.993 1185.23,1020.5 1254.44,1093.16 1323.64,1137.97 1392.85,1177.12 1462.06,1232.9 1531.27,1273.37 1600.47,1260.43 1669.68,1291.95 1738.89,1323.56 1808.09,1334.4 1877.3,1337.44 1946.51,1324.73 2015.72,1335.69 2084.92,1384.24 2154.13,1363.57 2223.34,1376.51 2292.55,1358.2 "/>
<polyline clip-path="url(#clip172)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="285.535,153.561 354.742,143.517 423.95,228.589 493.157,195.31 562.364,240.107 631.571,186.199 700.779,236.402 769.986,229.142 839.193,187.369 908.4,231.159 977.608,218.511 1046.81,209.98 1116.02,218.758 1185.23,220.747 1254.44,196.718 1323.64,203.509 1392.85,218.432 1462.06,183.617 1531.27,195.992 1600.47,212.025 1669.68,197.446 1738.89,168.674 1808.09,198.093 1877.3,189.447 1946.51,170.414 2015.72,175.893 2084.92,191.953 2154.13,180.003 2223.34,136.745 2292.55,179.803 "/>
<path clip-path="url(#clip170)" d="M296.239 1377.32 L731.018 1377.32 L731.018 1221.8 L296.239 1221.8  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip170)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="296.239,1377.32 731.018,1377.32 731.018,1221.8 296.239,1221.8 296.239,1377.32 "/>
<polyline clip-path="url(#clip170)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="319.877,1273.64 461.706,1273.64 "/>
<path clip-path="url(#clip170)" d="M492.751 1257.63 L492.751 1264.99 L501.525 1264.99 L501.525 1268.3 L492.751 1268.3 L492.751 1282.37 Q492.751 1285.54 493.608 1286.45 Q494.488 1287.35 497.15 1287.35 L501.525 1287.35 L501.525 1290.92 L497.15 1290.92 Q492.219 1290.92 490.344 1289.09 Q488.469 1287.23 488.469 1282.37 L488.469 1268.3 L485.344 1268.3 L485.344 1264.99 L488.469 1264.99 L488.469 1257.63 L492.751 1257.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M522.149 1268.97 Q521.432 1268.55 520.575 1268.37 Q519.742 1268.16 518.724 1268.16 Q515.112 1268.16 513.168 1270.52 Q511.247 1272.86 511.247 1277.26 L511.247 1290.92 L506.964 1290.92 L506.964 1264.99 L511.247 1264.99 L511.247 1269.02 Q512.589 1266.66 514.742 1265.52 Q516.895 1264.36 519.974 1264.36 Q520.413 1264.36 520.946 1264.43 Q521.478 1264.48 522.126 1264.6 L522.149 1268.97 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M538.399 1277.88 Q533.237 1277.88 531.247 1279.06 Q529.256 1280.24 529.256 1283.09 Q529.256 1285.36 530.737 1286.7 Q532.242 1288.02 534.811 1288.02 Q538.353 1288.02 540.483 1285.52 Q542.635 1283 542.635 1278.83 L542.635 1277.88 L538.399 1277.88 M546.895 1276.12 L546.895 1290.92 L542.635 1290.92 L542.635 1286.98 Q541.177 1289.34 539.001 1290.48 Q536.825 1291.59 533.677 1291.59 Q529.696 1291.59 527.335 1289.36 Q524.997 1287.12 524.997 1283.37 Q524.997 1278.99 527.913 1276.77 Q530.853 1274.55 536.663 1274.55 L542.635 1274.55 L542.635 1274.13 Q542.635 1271.19 540.691 1269.6 Q538.77 1267.98 535.274 1267.98 Q533.052 1267.98 530.946 1268.51 Q528.839 1269.04 526.895 1270.11 L526.895 1266.17 Q529.233 1265.27 531.432 1264.83 Q533.631 1264.36 535.714 1264.36 Q541.339 1264.36 544.117 1267.28 Q546.895 1270.2 546.895 1276.12 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M555.668 1264.99 L559.927 1264.99 L559.927 1290.92 L555.668 1290.92 L555.668 1264.99 M555.668 1254.9 L559.927 1254.9 L559.927 1260.29 L555.668 1260.29 L555.668 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M590.39 1275.27 L590.39 1290.92 L586.131 1290.92 L586.131 1275.41 Q586.131 1271.73 584.695 1269.9 Q583.26 1268.07 580.39 1268.07 Q576.941 1268.07 574.95 1270.27 Q572.959 1272.47 572.959 1276.26 L572.959 1290.92 L568.677 1290.92 L568.677 1264.99 L572.959 1264.99 L572.959 1269.02 Q574.487 1266.68 576.547 1265.52 Q578.631 1264.36 581.339 1264.36 Q585.807 1264.36 588.098 1267.14 Q590.39 1269.9 590.39 1275.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M618.584 1298.79 L618.584 1302.1 L593.955 1302.1 L593.955 1298.79 L618.584 1298.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M622.589 1254.9 L626.848 1254.9 L626.848 1290.92 L622.589 1290.92 L622.589 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M645.806 1267.98 Q642.38 1267.98 640.39 1270.66 Q638.399 1273.32 638.399 1277.98 Q638.399 1282.63 640.366 1285.31 Q642.357 1287.98 645.806 1287.98 Q649.209 1287.98 651.2 1285.29 Q653.19 1282.61 653.19 1277.98 Q653.19 1273.37 651.2 1270.68 Q649.209 1267.98 645.806 1267.98 M645.806 1264.36 Q651.362 1264.36 654.533 1267.98 Q657.704 1271.59 657.704 1277.98 Q657.704 1284.34 654.533 1287.98 Q651.362 1291.59 645.806 1291.59 Q640.228 1291.59 637.056 1287.98 Q633.908 1284.34 633.908 1277.98 Q633.908 1271.59 637.056 1267.98 Q640.228 1264.36 645.806 1264.36 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M681.292 1265.75 L681.292 1269.78 Q679.487 1268.86 677.542 1268.39 Q675.598 1267.93 673.514 1267.93 Q670.343 1267.93 668.746 1268.9 Q667.172 1269.87 667.172 1271.82 Q667.172 1273.3 668.306 1274.16 Q669.44 1274.99 672.866 1275.75 L674.325 1276.08 Q678.862 1277.05 680.76 1278.83 Q682.681 1280.59 682.681 1283.76 Q682.681 1287.37 679.811 1289.48 Q676.963 1291.59 671.963 1291.59 Q669.88 1291.59 667.612 1291.17 Q665.366 1290.78 662.866 1289.97 L662.866 1285.57 Q665.227 1286.79 667.519 1287.42 Q669.811 1288.02 672.056 1288.02 Q675.065 1288.02 676.686 1287 Q678.306 1285.96 678.306 1284.09 Q678.306 1282.35 677.125 1281.42 Q675.968 1280.5 672.01 1279.64 L670.528 1279.3 Q666.57 1278.46 664.811 1276.75 Q663.051 1275.01 663.051 1272 Q663.051 1268.35 665.644 1266.36 Q668.237 1264.36 673.005 1264.36 Q675.366 1264.36 677.45 1264.71 Q679.533 1265.06 681.292 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M705.991 1265.75 L705.991 1269.78 Q704.186 1268.86 702.241 1268.39 Q700.297 1267.93 698.213 1267.93 Q695.042 1267.93 693.445 1268.9 Q691.871 1269.87 691.871 1271.82 Q691.871 1273.3 693.005 1274.16 Q694.139 1274.99 697.565 1275.75 L699.023 1276.08 Q703.561 1277.05 705.459 1278.83 Q707.38 1280.59 707.38 1283.76 Q707.38 1287.37 704.51 1289.48 Q701.662 1291.59 696.662 1291.59 Q694.579 1291.59 692.311 1291.17 Q690.065 1290.78 687.565 1289.97 L687.565 1285.57 Q689.926 1286.79 692.218 1287.42 Q694.51 1288.02 696.755 1288.02 Q699.764 1288.02 701.385 1287 Q703.005 1285.96 703.005 1284.09 Q703.005 1282.35 701.824 1281.42 Q700.667 1280.5 696.709 1279.64 L695.227 1279.3 Q691.269 1278.46 689.51 1276.75 Q687.75 1275.01 687.75 1272 Q687.75 1268.35 690.343 1266.36 Q692.936 1264.36 697.704 1264.36 Q700.065 1264.36 702.148 1264.71 Q704.232 1265.06 705.991 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip170)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="319.877,1325.48 461.706,1325.48 "/>
<path clip-path="url(#clip170)" d="M485.344 1316.83 L489.858 1316.83 L497.96 1338.59 L506.062 1316.83 L510.575 1316.83 L500.853 1342.76 L495.066 1342.76 L485.344 1316.83 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M528.237 1329.72 Q523.075 1329.72 521.085 1330.9 Q519.094 1332.08 519.094 1334.93 Q519.094 1337.2 520.575 1338.54 Q522.08 1339.86 524.649 1339.86 Q528.191 1339.86 530.321 1337.36 Q532.473 1334.84 532.473 1330.67 L532.473 1329.72 L528.237 1329.72 M536.733 1327.96 L536.733 1342.76 L532.473 1342.76 L532.473 1338.82 Q531.015 1341.18 528.839 1342.32 Q526.663 1343.43 523.515 1343.43 Q519.534 1343.43 517.173 1341.2 Q514.835 1338.96 514.835 1335.21 Q514.835 1330.83 517.751 1328.61 Q520.691 1326.39 526.501 1326.39 L532.473 1326.39 L532.473 1325.97 Q532.473 1323.03 530.529 1321.44 Q528.608 1319.82 525.112 1319.82 Q522.89 1319.82 520.784 1320.35 Q518.677 1320.88 516.733 1321.95 L516.733 1318.01 Q519.071 1317.11 521.27 1316.67 Q523.469 1316.2 525.552 1316.2 Q531.177 1316.2 533.955 1319.12 Q536.733 1322.04 536.733 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M545.506 1306.74 L549.765 1306.74 L549.765 1342.76 L545.506 1342.76 L545.506 1306.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M578.376 1350.63 L578.376 1353.94 L553.746 1353.94 L553.746 1350.63 L578.376 1350.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M582.381 1306.74 L586.64 1306.74 L586.64 1342.76 L582.381 1342.76 L582.381 1306.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M605.598 1319.82 Q602.172 1319.82 600.181 1322.5 Q598.191 1325.16 598.191 1329.82 Q598.191 1334.47 600.158 1337.15 Q602.149 1339.82 605.598 1339.82 Q609.001 1339.82 610.992 1337.13 Q612.982 1334.45 612.982 1329.82 Q612.982 1325.21 610.992 1322.52 Q609.001 1319.82 605.598 1319.82 M605.598 1316.2 Q611.154 1316.2 614.325 1319.82 Q617.496 1323.43 617.496 1329.82 Q617.496 1336.18 614.325 1339.82 Q611.154 1343.43 605.598 1343.43 Q600.019 1343.43 596.848 1339.82 Q593.7 1336.18 593.7 1329.82 Q593.7 1323.43 596.848 1319.82 Q600.019 1316.2 605.598 1316.2 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M641.084 1317.59 L641.084 1321.62 Q639.278 1320.7 637.334 1320.23 Q635.39 1319.77 633.306 1319.77 Q630.135 1319.77 628.538 1320.74 Q626.964 1321.71 626.964 1323.66 Q626.964 1325.14 628.098 1326 Q629.232 1326.83 632.658 1327.59 L634.116 1327.92 Q638.653 1328.89 640.552 1330.67 Q642.473 1332.43 642.473 1335.6 Q642.473 1339.21 639.603 1341.32 Q636.755 1343.43 631.755 1343.43 Q629.672 1343.43 627.404 1343.01 Q625.158 1342.62 622.658 1341.81 L622.658 1337.41 Q625.019 1338.63 627.311 1339.26 Q629.603 1339.86 631.848 1339.86 Q634.857 1339.86 636.478 1338.84 Q638.098 1337.8 638.098 1335.93 Q638.098 1334.19 636.917 1333.26 Q635.76 1332.34 631.802 1331.48 L630.32 1331.14 Q626.362 1330.3 624.603 1328.59 Q622.843 1326.85 622.843 1323.84 Q622.843 1320.19 625.436 1318.2 Q628.029 1316.2 632.797 1316.2 Q635.158 1316.2 637.241 1316.55 Q639.325 1316.9 641.084 1317.59 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip170)" d="M665.783 1317.59 L665.783 1321.62 Q663.977 1320.7 662.033 1320.23 Q660.089 1319.77 658.005 1319.77 Q654.834 1319.77 653.237 1320.74 Q651.663 1321.71 651.663 1323.66 Q651.663 1325.14 652.797 1326 Q653.931 1326.83 657.357 1327.59 L658.815 1327.92 Q663.352 1328.89 665.251 1330.67 Q667.172 1332.43 667.172 1335.6 Q667.172 1339.21 664.301 1341.32 Q661.454 1343.43 656.454 1343.43 Q654.371 1343.43 652.102 1343.01 Q649.857 1342.62 647.357 1341.81 L647.357 1337.41 Q649.718 1338.63 652.01 1339.26 Q654.302 1339.86 656.547 1339.86 Q659.556 1339.86 661.176 1338.84 Q662.797 1337.8 662.797 1335.93 Q662.797 1334.19 661.616 1333.26 Q660.459 1332.34 656.501 1331.48 L655.019 1331.14 Q651.061 1330.3 649.302 1328.59 Q647.542 1326.85 647.542 1323.84 Q647.542 1320.19 650.135 1318.2 Q652.727 1316.2 657.496 1316.2 Q659.857 1316.2 661.94 1316.55 Q664.024 1316.9 665.783 1317.59 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="language-julia hljs">engs = [&quot;go .&quot;, &quot;i lost .&quot;, &quot;he&#39;s calm .&quot;, &quot;i&#39;m home .&quot;]
fras = [&quot;va !&quot;, &quot;j&#39;ai perdu .&quot;, &quot;il est calme .&quot;, &quot;je suis chez moi .&quot;]


batch = d2lai.build(data, engs, fras)
preds, _ = d2lai.predict_step(m, batch, cpu, data.args.num_steps; save_attention_wts = true)

for (en, fr, p) in zip(engs, fras, eachcol(preds))
    translation = []
    for token in d2lai.to_tokens(data.tgt_vocab, p)
        if token == &quot;&lt;eos&gt;&quot;
            break
        end
        push!(translation, token)
    end 
    bleu_score = d2lai.bleu(join(translation, &quot; &quot;), fr, 2)
    println(&quot;$en =&gt; $translation&quot;, &quot;bleu: $bleu_score&quot;)
end</code></pre><pre><code class="nohighlight hljs">go . =&gt; Any[&quot;va&quot;, &quot;!&quot;]bleu: 1.0
i lost . =&gt; Any[&quot;j&#39;ai&quot;, &quot;perdu&quot;, &quot;.&quot;]bleu: 1.0
he&#39;s calm . =&gt; Any[&quot;j&#39;en&quot;, &quot;fais&quot;, &quot;.&quot;]bleu: 0.0
i&#39;m home . =&gt; Any[&quot;je&quot;, &quot;suis&quot;, &quot;chez&quot;, &quot;moi&quot;, &quot;.&quot;]bleu: 1.0</code></pre><pre><code class="language-julia hljs">btch = d2lai.build(data, [engs[end]], [fras[end]])
_, dec_attention_weights = d2lai.predict_step(m, btch, cpu, data.args.num_steps; save_attention_wts = true);</code></pre><pre><code class="language-julia hljs">attention_weights = cat([step[1] for step in dec_attention_weights]..., dims = 3)
attention_weights = reshape(attention_weights, :, data.args.num_steps, 1, 1)</code></pre><pre><code class="nohighlight hljs">72×9×1×1 Array{Float64, 4}:
[:, :, 1, 1] =
 0.0197803   0.50357      0.88467      …  0.673018   0.602622   0.658632
 0.294253    0.111737     0.0296061       0.110218   0.128447   0.102955
 0.501421    0.138298     0.029734        0.0741955  0.106384   0.0875043
 0.184546    0.246395     0.05599         0.142569   0.162548   0.150908
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0          …  0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.867143    0.000183062  0.863314        0.167611   0.0753995  0.0480836
 0.12625     0.00259461   0.0396935    …  0.161772   0.107583   0.0798361
 0.00453269  0.241328     0.0382949       0.318684   0.337655   0.345838
 0.00207432  0.755895     0.0586977       0.351933   0.479362   0.526242
 ⋮                                     ⋱                        
 0.0         0.0          0.0          …  0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.881675    0.994476     0.989049        0.924161   0.922438   0.932097
 0.0969292   0.00491062   0.00987073      0.0437425  0.048495   0.0454433
 0.0106992   0.0003172    0.000686157  …  0.018651   0.0157849  0.0124275
 0.0106962   0.000295896  0.000394499     0.0134455  0.0132822  0.010032
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0
 0.0         0.0          0.0          …  0.0        0.0        0.0
 0.0         0.0          0.0             0.0        0.0        0.0</code></pre><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ATTN_4/">« The Bahdanau Attention Mechanism</a><a class="docs-footer-nextpage" href="../ATTN_6/">Self-Attention and Positional Encoding »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
