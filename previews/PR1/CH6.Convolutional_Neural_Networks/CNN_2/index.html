<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Convolutions for Images · d2l Julia</title><meta name="title" content="Convolutions for Images · d2l Julia"/><meta property="og:title" content="Convolutions for Images · d2l Julia"/><meta property="twitter:title" content="Convolutions for Images · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li class="is-active"><a class="tocitem" href>Convolutions for Images</a><ul class="internal"><li><a class="tocitem" href="#Convolutional-Layers"><span>Convolutional Layers</span></a></li><li><a class="tocitem" href="#Object-Edge-Detection-in-Images"><span>Object Edge Detection in Images</span></a></li><li><a class="tocitem" href="#Learning-a-Kernel"><span>Learning a Kernel</span></a></li><li><a class="tocitem" href="#Cross-Correlation-and-Convolution"><span>Cross-Correlation and Convolution</span></a></li><li><a class="tocitem" href="#Feature-Map-and-Receptive-Field"><span>Feature Map and Receptive Field</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li><li><a class="tocitem" href="#Answers"><span>Answers</span></a></li></ul></li><li><a class="tocitem" href="../CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../CNN_5/">Pooling</a></li><li><a class="tocitem" href="../CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Convolutional Neural Networks</a></li><li class="is-active"><a href>Convolutions for Images</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Convolutions for Images</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Convolutions-for-Images"><a class="docs-heading-anchor" href="#Convolutions-for-Images">Convolutions for Images</a><a id="Convolutions-for-Images-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutions-for-Images" title="Permalink"></a></h1><p>Now that we understand how convolutional layers work in theory, we are ready to see how they work in practice. Building on our motivation of convolutional neural networks as efficient architectures for exploring structure in image data, we stick with images as our running example.</p><pre><code class="language-julia hljs">using Pkg
Pkg.activate(&quot;d2lai&quot;)</code></pre><pre><code class="nohighlight hljs">  Activating project at `~/Projects/D2L/d2lai`</code></pre><pre><code class="language-julia hljs">using d2lai, Flux</code></pre><p>Recall that strictly speaking, convolutional layers are a  misnomer, since the operations they express are more accurately described as cross-correlations. Based on our descriptions of convolutional layers in :numref:<code>sec_why-conv</code>, in such a layer, an input tensor and a kernel tensor are combined to produce an output tensor through a (<strong>cross-correlation operation.</strong>)</p><p>Let&#39;s ignore channels for now and see how this works with two-dimensional data and hidden representations. In :numref:<code>fig_correlation</code>, the input is a two-dimensional tensor with a height of 3 and width of 3. We mark the shape of the tensor as <span>$3 \times 3$</span> or (<span>$3$</span>, <span>$3$</span>). The height and width of the kernel are both 2. The shape of the <em>kernel window</em> (or <em>convolution window</em>) is given by the height and width of the kernel (here it is <span>$2 \times 2$</span>).</p><p><img src="../../img/correlation.svg" alt="Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: \$0\\times0+1\\times1+3\\times2+4\\times3=19\$."/> :label:<code>fig_correlation</code></p><p>In the two-dimensional cross-correlation operation, we begin with the convolution window positioned at the upper-left corner of the input tensor and slide it across the input tensor, both from left to right and top to bottom. When the convolution window slides to a certain position, the input subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting tensor is summed up yielding a single scalar value. This result gives the value of the output tensor at the corresponding location. Here, the output tensor has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:</p><p class="math-container">\[
0\times0+1\times1+3\times2+4\times3=19,\\
1\times0+2\times1+4\times2+5\times3=25,\\
3\times0+4\times1+6\times2+7\times3=37,\\
4\times0+5\times1+7\times2+8\times3=43.
$$

Note that along each axis, the output size
is slightly smaller than the input size.
Because the kernel has width and height greater than $1$,
we can only properly compute the cross-correlation
for locations where the kernel fits wholly within the image,
the output size is given by the input size $n_\textrm{h} \times n_\textrm{w}$
minus the size of the convolution kernel $k_\textrm{h} \times k_\textrm{w}$
via

$$(n_\textrm{h}-k_\textrm{h}+1) \times (n_\textrm{w}-k_\textrm{w}+1).\]</p><p>This is the case since we need enough space to &quot;shift&quot; the convolution kernel across the image. Later we will see how to keep the size unchanged by padding the image with zeros around its boundary so that there is enough space to shift the kernel. Next, we implement this process in the <code>corr2d</code> function, which accepts an input tensor <code>X</code> and a kernel tensor <code>K</code> and returns an output tensor <code>Y</code>.</p><pre><code class="language-julia hljs">function corr2d(X::AbstractArray, K::AbstractArray)
    Y = zeros(size(X) .- size(K) .+ 1)
    kh, kw = size(K)
    for i in 1:size(Y, 1)
        for j in 1:size(Y, 2)
            Y[i, j] = sum(X[i:(i+kh-1), j:j+kw-1] .* K)
        end
    end
    Y
end</code></pre><pre><code class="nohighlight hljs">corr2d (generic function with 1 method)</code></pre><p>We can construct the input array <code>X</code> and the kernel array <code>K</code> from :numref:<code>fig_correlation</code> to validate the output of the above implementation of the two-dimensional cross-correlation operation.</p><pre><code class="language-julia hljs">X = [0. 1. 2; 3. 4. 5; 6. 7. 8.]
K = [0. 1.; 2. 3.]</code></pre><pre><code class="nohighlight hljs">2×2 Matrix{Float64}:
 0.0  1.0
 2.0  3.0</code></pre><pre><code class="language-julia hljs">corr2d(X, K)</code></pre><pre><code class="nohighlight hljs">2×2 Matrix{Float64}:
 19.0  25.0
 37.0  43.0</code></pre><h2 id="Convolutional-Layers"><a class="docs-heading-anchor" href="#Convolutional-Layers">Convolutional Layers</a><a id="Convolutional-Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-Layers" title="Permalink"></a></h2><p>A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output. The two parameters of a convolutional layer are the kernel and the scalar bias. When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully connected layer.</p><p>We are now ready to [<strong>implement a two-dimensional convolutional layer</strong>] based on the <code>corr2d</code> function defined above. In the <code>__init__</code> constructor method, we declare <code>weight</code> and <code>bias</code> as the two model parameters. The forward propagation method calls the <code>corr2d</code> function and adds the bias.</p><pre><code class="language-julia hljs">struct Conv2D{B, K} &lt;: AbstractModel
    weight::K 
    bias::B 
end

function Conv2D(kernel_size)
    kernel = rand(kernel_size...)
    b = zeros(1)  
    Conv2D(kernel, b)
end

# 
function (conv_layer::Conv2D)(x)
    corr2d(x, conv_layer.kernel) .+ conv_layer.bias
end

Flux.Functors.@functor Conv2D</code></pre><p>In <span>$h \times w$</span> convolution or an <span>$h \times w$</span> convolution kernel, the height and width of the convolution kernel are <span>$h$</span> and <span>$w$</span>, respectively. We also refer to a convolutional layer with an <span>$h \times w$</span> convolution kernel simply as an <span>$h \times w$</span> convolutional layer.</p><h2 id="Object-Edge-Detection-in-Images"><a class="docs-heading-anchor" href="#Object-Edge-Detection-in-Images">Object Edge Detection in Images</a><a id="Object-Edge-Detection-in-Images-1"></a><a class="docs-heading-anchor-permalink" href="#Object-Edge-Detection-in-Images" title="Permalink"></a></h2><p>Let&#39;s take a moment to parse a simple application of a convolutional layer: detecting the edge of an object in an image by finding the location of the pixel change. First, we construct an &quot;image&quot; of <span>$6\times 8$</span> pixels. The middle four columns are black (<span>$0$</span>) and the rest are white (<span>$1$</span>).</p><pre><code class="language-julia hljs">X = ones(6, 8)
X[:, 3:6] .= 0
X</code></pre><pre><code class="nohighlight hljs">6×8 Matrix{Float64}:
 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0
 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0
 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0
 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0
 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0
 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0</code></pre><p>Next, we construct a kernel <code>K</code> with a height of 1 and a width of 2. When we perform the cross-correlation operation with the input, if the horizontally adjacent elements are the same, the output is 0. Otherwise, the output is nonzero. Note that this kernel is a special case of a finite difference operator. At location <span>$(i,j)$</span> it computes <span>$x_{i,j} - x_{(i+1),j}$</span>, i.e., it computes the difference between the values of horizontally adjacent pixels. This is a discrete approximation of the first derivative in the horizontal direction. After all, for a function <span>$f(i,j)$</span> its derivative <span>$-\partial_i f(i,j) = \lim_{\epsilon \to 0} \frac{f(i,j) - f(i+\epsilon,j)}{\epsilon}$</span>. Let&#39;s see how this works in practice.</p><pre><code class="language-julia hljs">K = [1.0 -1.0]</code></pre><pre><code class="nohighlight hljs">1×2 Matrix{Float64}:
 1.0  -1.0</code></pre><p>We are ready to perform the cross-correlation operation with arguments <code>X</code> (our input) and <code>K</code> (our kernel). As you can see, [<strong>we detect <span>$1$</span> for the edge from white to black and <span>$-1$</span> for the edge from black to white.</strong>] All other outputs take value <span>$0$</span>.</p><pre><code class="language-julia hljs">Y = corr2d(X, K)</code></pre><pre><code class="nohighlight hljs">6×7 Matrix{Float64}:
 0.0  1.0  0.0  0.0  0.0  -1.0  0.0
 0.0  1.0  0.0  0.0  0.0  -1.0  0.0
 0.0  1.0  0.0  0.0  0.0  -1.0  0.0
 0.0  1.0  0.0  0.0  0.0  -1.0  0.0
 0.0  1.0  0.0  0.0  0.0  -1.0  0.0
 0.0  1.0  0.0  0.0  0.0  -1.0  0.0</code></pre><p>We can now apply the kernel to the transposed image. As expected, it vanishes. The kernel <code>K</code> only detects vertical edges.</p><pre><code class="language-julia hljs">corr2d(X&#39;, K)</code></pre><pre><code class="nohighlight hljs">8×5 Matrix{Float64}:
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0</code></pre><h2 id="Learning-a-Kernel"><a class="docs-heading-anchor" href="#Learning-a-Kernel">Learning a Kernel</a><a id="Learning-a-Kernel-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-a-Kernel" title="Permalink"></a></h2><p>Designing an edge detector by finite differences <code>[1, -1]</code> is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually.</p><p>Now let&#39;s see whether we can [<strong>learn the kernel that generated <code>Y</code> from <code>X</code></strong>] by looking at the input–output pairs only. We first construct a convolutional layer and initialize its kernel as a random tensor. Next, in each iteration, we will use the squared error to compare <code>Y</code> with the output of the convolutional layer. We can then calculate the gradient to update the kernel. For the sake of simplicity, in the following we use the built-in class for two-dimensional convolutional layers and ignore the bias.</p><pre><code class="language-julia hljs">conv2d = Conv((1,2), 1 =&gt; 1; bias = false)
X = reshape(X, 6, 8, 1, 1)
Y = reshape(Y, 6, 7, 1, 1)
lr = 3e-2 
for i in 1:10
    ps = Flux.params(conv2d)
    gs = gradient(ps) do 
        Y_pred = conv2d(X)
        l = sum((Y_pred - Y).^2)
    end
    l = sum((conv2d(X) - Y).^2)
    conv2d.weight .-= lr*gs[ps[1]]
    if(i%2 == 0)
        println(&quot;epoch $i loss $l&quot;)
    end
end</code></pre><pre><code class="nohighlight hljs">epoch 2 loss 23.584481099119426
epoch 4 loss 3.9607391083212136
epoch 6 loss 0.6661072908585268
epoch 8 loss 0.11241184883977695
epoch 10 loss 0.01912891086388413</code></pre><pre><code class="language-julia hljs">conv2d.weight</code></pre><pre><code class="nohighlight hljs">1×2×1×1 Array{Float32, 4}:
[:, :, 1, 1] =
 -0.972548  0.97696</code></pre><h2 id="Cross-Correlation-and-Convolution"><a class="docs-heading-anchor" href="#Cross-Correlation-and-Convolution">Cross-Correlation and Convolution</a><a id="Cross-Correlation-and-Convolution-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-Correlation-and-Convolution" title="Permalink"></a></h2><p>Recall our observation from :numref:<code>sec_why-conv</code> of the correspondence between the cross-correlation and convolution operations. Here let&#39;s continue to consider two-dimensional convolutional layers. What if such layers perform strict convolution operations as defined in :eqref:<code>eq_2d-conv-discrete</code> instead of cross-correlations? In order to obtain the output of the strict <em>convolution</em> operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the <em>cross-correlation</em> operation with the input tensor.</p><p>It is noteworthy that since kernels are learned from data in deep learning, the outputs of convolutional layers remain unaffected no matter such layers perform either the strict convolution operations or the cross-correlation operations.</p><p>To illustrate this, suppose that a convolutional layer performs <em>cross-correlation</em> and learns the kernel in :numref:<code>fig_correlation</code>, which is here denoted as the matrix <span>$\mathbf{K}$</span>. Assuming that other conditions remain unchanged, when this layer instead performs strict <em>convolution</em>, the learned kernel <span>$\mathbf{K}&#39;$</span> will be the same as <span>$\mathbf{K}$</span> after <span>$\mathbf{K}&#39;$</span> is flipped both horizontally and vertically. That is to say, when the convolutional layer performs strict <em>convolution</em> for the input in :numref:<code>fig_correlation</code> and <span>$\mathbf{K}&#39;$</span>, the same output in :numref:<code>fig_correlation</code> (cross-correlation of the input and <span>$\mathbf{K}$</span>) will be obtained.</p><p>In keeping with standard terminology in deep learning literature, we will continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different. Furthermore, we use the term <em>element</em> to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel.</p><h2 id="Feature-Map-and-Receptive-Field"><a class="docs-heading-anchor" href="#Feature-Map-and-Receptive-Field">Feature Map and Receptive Field</a><a id="Feature-Map-and-Receptive-Field-1"></a><a class="docs-heading-anchor-permalink" href="#Feature-Map-and-Receptive-Field" title="Permalink"></a></h2><p>As described in :numref:<code>subsec_why-conv-channels</code>, the convolutional layer output in :numref:<code>fig_correlation</code> is sometimes called a <em>feature map</em>, as it can be regarded as the learned representations (features) in the spatial dimensions (e.g., width and height) to the subsequent layer. In CNNs, for any element <span>$x$</span> of some layer, its <em>receptive field</em> refers to all the elements (from all the previous layers) that may affect the calculation of <span>$x$</span> during the forward propagation. Note that the receptive field may be larger than the actual size of the input.</p><p>Let&#39;s continue to use :numref:<code>fig_correlation</code> to explain the receptive field. Given the <span>$2 \times 2$</span> convolution kernel, the receptive field of the shaded output element (of value <span>$19$</span>) is the four elements in the shaded portion of the input. Now let&#39;s denote the <span>$2 \times 2$</span> output as <span>$\mathbf{Y}$</span> and consider a deeper CNN with an additional <span>$2 \times 2$</span> convolutional layer that takes <span>$\mathbf{Y}$</span> as its input, outputting a single element <span>$z$</span>. In this case, the receptive field of <span>$z$</span> on <span>$\mathbf{Y}$</span> includes all the four elements of <span>$\mathbf{Y}$</span>, while the receptive field on the input includes all the nine input elements. Thus, when any element in a feature map needs a larger receptive field to detect input features over a broader area, we can build a deeper network.</p><p>Receptive fields derive their name from neurophysiology. A series of experiments on a range of animals using different stimuli :cite:<code>Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968</code> explored the response of what is called the visual cortex on said stimuli. By and large they found that lower levels respond to edges and related shapes. Later on, :citet:<code>Field.1987</code> illustrated this effect on natural images with, what can only be called, convolutional kernels. We reprint a key figure in :numref:<code>field_visual</code> to illustrate the striking similarities.</p><p><img src="../../img/field-visual.png" alt="Figure and caption taken from :citet:`Field.1987`: An example of coding with six different channels. (Left) Examples of the six types of sensor associated with each channel. (Right) Convolution of the image in (Middle) with the six sensors shown in (Left). The response of the individual sensors is determined by sampling these filtered images at a distance proportional to the size of the sensor (shown with dots). This diagram shows the response of only the even symmetric sensors."/> :label:<code>field_visual</code></p><p>As it turns out, this relation even holds for the features computed by deeper layers of networks trained on image classification tasks, as demonstrated in, for example, :citet:<code>Kuzovkin.Vicente.Petton.ea.2018</code>. Suffice it to say, convolutions have proven to be an incredibly powerful tool for computer vision, both in biology and in code. As such, it is not surprising (in hindsight) that they heralded the recent success in deep learning.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>The core computation required for a convolutional layer is a cross-correlation operation. We saw that a simple nested for-loop is all that is required to compute its value. If we have multiple input and multiple output channels, we are  performing a matrix–matrix operation between channels. As can be seen, the computation is straightforward and, most importantly, highly <em>local</em>. This affords significant hardware optimization and many recent results in computer vision are only possible because of that. After all, it means that chip designers can invest in fast computation rather than memory when it comes to optimizing for convolutions. While this may not lead to optimal designs for other applications, it does open the door to ubiquitous and affordable computer vision.</p><p>In terms of convolutions themselves, they can be used for many purposes, for example detecting edges and lines, blurring images, or sharpening them. Most importantly, it is not necessary that the statistician (or engineer) invents suitable filters. Instead, we can simply <em>learn</em> them from data. This replaces feature engineering heuristics by evidence-based statistics. Lastly, and quite delightfully, these filters are not just advantageous for building deep networks but they also correspond to receptive fields and feature maps in the brain. This gives us confidence that we are on the right track.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Construct an image <code>X</code> with diagonal edges.<ol><li>What happens if you apply the kernel <code>K</code> in this section to it?</li><li>What happens if you transpose <code>X</code>?</li><li>What happens if you transpose <code>K</code>?</li></ol></li><li>Design some kernels manually.<ol><li>Given a directional vector <span>$\mathbf{v} = (v_1, v_2)$</span>, derive an edge-detection kernel that detects edges orthogonal to <span>$\mathbf{v}$</span>, i.e., edges in the direction <span>$(v_2, -v_1)$</span>.</li><li>Derive a finite difference operator for the second derivative. What is the minimum size of the convolutional kernel associated with it? Which structures in images respond most strongly to it?</li><li>How would you design a blur kernel? Why might you want to use such a kernel?</li><li>What is the minimum size of a kernel to obtain a derivative of order <span>$d$</span>?</li></ol></li><li>When you try to automatically find the gradient for the <code>Conv2D</code> class we created, what kind of error message do you see?</li><li>How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?</li></ol><h2 id="Answers"><a class="docs-heading-anchor" href="#Answers">Answers</a><a id="Answers-1"></a><a class="docs-heading-anchor-permalink" href="#Answers" title="Permalink"></a></h2><h3 id="1.-Construct-an-image-X-with-diagonal-edges."><a class="docs-heading-anchor" href="#1.-Construct-an-image-X-with-diagonal-edges.">1. Construct an image <code>X</code> with diagonal edges.</a><a id="1.-Construct-an-image-X-with-diagonal-edges.-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Construct-an-image-X-with-diagonal-edges." title="Permalink"></a></h3><ol><li>What happens if you apply the kernel <code>K</code> in this section to it?</li><li>What happens if you transpose <code>X</code>?</li><li>What happens if you transpose <code>K</code>?</li></ol><pre><code class="language-julia hljs"># 1.1
M = zeros(6, 6)
for i in 1:6
    M[i, i] = 1
    M[i, 6-i+1] = 1
end
M</code></pre><pre><code class="nohighlight hljs">6×6 Matrix{Float64}:
 1.0  0.0  0.0  0.0  0.0  1.0
 0.0  1.0  0.0  0.0  1.0  0.0
 0.0  0.0  1.0  1.0  0.0  0.0
 0.0  0.0  1.0  1.0  0.0  0.0
 0.0  1.0  0.0  0.0  1.0  0.0
 1.0  0.0  0.0  0.0  0.0  1.0</code></pre><pre><code class="language-julia hljs">K = [1.0 -1.0]
corr2d(M, K)</code></pre><pre><code class="nohighlight hljs">6×5 Matrix{Float64}:
  1.0   0.0  0.0   0.0  -1.0
 -1.0   1.0  0.0  -1.0   1.0
  0.0  -1.0  0.0   1.0   0.0
  0.0  -1.0  0.0   1.0   0.0
 -1.0   1.0  0.0  -1.0   1.0
  1.0   0.0  0.0   0.0  -1.0</code></pre><pre><code class="language-julia hljs"># 1.2
corr2d(transpose(M), K)</code></pre><pre><code class="nohighlight hljs">6×5 Matrix{Float64}:
  1.0   0.0  0.0   0.0  -1.0
 -1.0   1.0  0.0  -1.0   1.0
  0.0  -1.0  0.0   1.0   0.0
  0.0  -1.0  0.0   1.0   0.0
 -1.0   1.0  0.0  -1.0   1.0
  1.0   0.0  0.0   0.0  -1.0</code></pre><pre><code class="language-julia hljs"># 1.3
corr2d(M, transpose(K))</code></pre><pre><code class="nohighlight hljs">5×6 Matrix{Float64}:
  1.0  -1.0   0.0   0.0  -1.0   1.0
  0.0   1.0  -1.0  -1.0   1.0   0.0
  0.0   0.0   0.0   0.0   0.0   0.0
  0.0  -1.0   1.0   1.0  -1.0   0.0
 -1.0   1.0   0.0   0.0   1.0  -1.0</code></pre><h3 id=".-Design-some-kernels-manually."><a class="docs-heading-anchor" href="#.-Design-some-kernels-manually.">. Design some kernels manually.</a><a id=".-Design-some-kernels-manually.-1"></a><a class="docs-heading-anchor-permalink" href="#.-Design-some-kernels-manually." title="Permalink"></a></h3><ol><li>Given a directional vector <span>$\mathbf{v} = (v_1, v_2)$</span>, derive an edge-detection kernel that detects edges orthogonal to <span>$\mathbf{v}$</span>, i.e., edges in the direction <span>$(v_2, -v_1)$</span>.</li><li>Derive a finite difference operator for the second derivative. What is the minimum size of the convolutional kernel associated with it? Which structures in images respond most strongly to it?</li><li>How would you design a blur kernel? Why might you want to use such a kernel?</li><li>What is the minimum size of a kernel to obtain a derivative of order <span>$d$</span>?</li></ol><pre><code class="language-julia hljs">K = ones(2, 1)
K[2, 1] = -1.
X = ones(6, 8)
X[2:5, :] .= 0
corr2d(X, K)</code></pre><pre><code class="nohighlight hljs">5×8 Matrix{Float64}:
  1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0
  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
 -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0</code></pre><pre><code class="language-julia hljs">K = [1.0 -2.0 1.0]
X = ones(6, 8)
X[:, 2:5] .= 0.
corr2d(X, K)</code></pre><pre><code class="nohighlight hljs">6×6 Matrix{Float64}:
 1.0  0.0  0.0  1.0  -1.0  0.0
 1.0  0.0  0.0  1.0  -1.0  0.0
 1.0  0.0  0.0  1.0  -1.0  0.0
 1.0  0.0  0.0  1.0  -1.0  0.0
 1.0  0.0  0.0  1.0  -1.0  0.0
 1.0  0.0  0.0  1.0  -1.0  0.0</code></pre><p>To blur a kernel we would use values &lt; |1.0|</p><p>We would need d + 1 as the length of the kernel</p><h4 id="Cross-Correlation-as-a-simple-matrix-multiplication"><a class="docs-heading-anchor" href="#Cross-Correlation-as-a-simple-matrix-multiplication">Cross Correlation as a simple matrix multiplication</a><a id="Cross-Correlation-as-a-simple-matrix-multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-Correlation-as-a-simple-matrix-multiplication" title="Permalink"></a></h4></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../CH5.MLP/MLP_6/">« Dropout</a><a class="docs-footer-nextpage" href="../CNN_3/">Padding and Stride »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
