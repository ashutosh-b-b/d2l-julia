<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multiple Input and Multiple Output Channels · d2l Julia</title><meta name="title" content="Multiple Input and Multiple Output Channels · d2l Julia"/><meta property="og:title" content="Multiple Input and Multiple Output Channels · d2l Julia"/><meta property="twitter:title" content="Multiple Input and Multiple Output Channels · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../CNN_3/">Padding and Stride</a></li><li class="is-active"><a class="tocitem" href>Multiple Input and Multiple Output Channels</a><ul class="internal"><li><a class="tocitem" href="#Multiple-Input-Channels"><span>Multiple Input Channels</span></a></li><li><a class="tocitem" href="#Multiple-Output-Channels"><span>Multiple Output Channels</span></a></li><li><a class="tocitem" href="#1\\times-1-Convolutional-Layer"><span><span>$1\times 1$</span> Convolutional Layer</span></a></li><li><a class="tocitem" href="#Discussion"><span>Discussion</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../CNN_5/">Pooling</a></li><li><a class="tocitem" href="../CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Convolutional Neural Networks</a></li><li class="is-active"><a href>Multiple Input and Multiple Output Channels</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multiple Input and Multiple Output Channels</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Multiple-Input-and-Multiple-Output-Channels"><a class="docs-heading-anchor" href="#Multiple-Input-and-Multiple-Output-Channels">Multiple Input and Multiple Output Channels</a><a id="Multiple-Input-and-Multiple-Output-Channels-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Input-and-Multiple-Output-Channels" title="Permalink"></a></h1><p>:label:<code>sec_channels</code></p><p>While we described the multiple channels that comprise each image (e.g., color images have the standard RGB channels to indicate the amount of red, green and blue) and convolutional layers for multiple channels in :numref:<code>subsec_why-conv-channels</code>, until now, we simplified all of our numerical examples by working with just a single input and a single output channel. This allowed us to think of our inputs, convolution kernels, and outputs each as two-dimensional tensors.</p><p>When we add channels into the mix, our inputs and hidden representations both become three-dimensional tensors. For example, each RGB input image has shape <span>$3\times h\times w$</span>. We refer to this axis, with a size of 3, as the <em>channel</em> dimension. The notion of channels is as old as CNNs themselves: for instance LeNet-5 :cite:<code>LeCun.Jackel.Bottou.ea.1995</code> uses them.  In this section, we will take a deeper look at convolution kernels with multiple input and multiple output channels.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;d2lai&quot;)
using d2lai, Flux
</code></pre><pre><code class="nohighlight hljs">  Activating project at `~/Projects/D2L/d2lai`</code></pre><h2 id="Multiple-Input-Channels"><a class="docs-heading-anchor" href="#Multiple-Input-Channels">Multiple Input Channels</a><a id="Multiple-Input-Channels-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Input-Channels" title="Permalink"></a></h2><p>When the input data contains multiple channels, we need to construct a convolution kernel with the same number of input channels as the input data, so that it can perform cross-correlation with the input data. Assuming that the number of channels for the input data is <span>$c_\textrm{i}$</span>, the number of input channels of the convolution kernel also needs to be <span>$c_\textrm{i}$</span>. If our convolution kernel&#39;s window shape is <span>$k_\textrm{h}\times k_\textrm{w}$</span>, then, when <span>$c_\textrm{i}=1$</span>, we can think of our convolution kernel as just a two-dimensional tensor of shape <span>$k_\textrm{h}\times k_\textrm{w}$</span>.</p><p>However, when <span>$c_\textrm{i}&gt;1$</span>, we need a kernel that contains a tensor of shape <span>$k_\textrm{h}\times k_\textrm{w}$</span> for <em>every</em> input channel. Concatenating these <span>$c_\textrm{i}$</span> tensors together yields a convolution kernel of shape <span>$c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$</span>. Since the input and convolution kernel each have <span>$c_\textrm{i}$</span> channels, we can perform a cross-correlation operation on the two-dimensional tensor of the input and the two-dimensional tensor of the convolution kernel for each channel, adding the <span>$c_\textrm{i}$</span> results together (summing over the channels) to yield a two-dimensional tensor. This is the result of a two-dimensional cross-correlation between a multi-channel input and a multi-input-channel convolution kernel.</p><p>:numref:<code>fig_conv_multi_in</code> provides an example  of a two-dimensional cross-correlation with two input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: <span>$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$</span>.</p><p><img src="../../img/conv-multi-in.svg" alt="Cross-correlation computation with two input channels."/> :label:<code>fig_conv_multi_in</code></p><p>To make sure we really understand what is going on here, we can (<strong>implement cross-correlation operations with multiple input channels</strong>) ourselves. Notice that all we are doing is performing a cross-correlation operation per channel and then adding up the results.</p><pre><code class="language-julia hljs">function corr2d_multi_in(X, K)
    return sum(d2lai.corr2d.(eachslice(X, dims=3), eachslice(K, dims=3)))
end</code></pre><pre><code class="nohighlight hljs">corr2d_multi_in (generic function with 1 method)</code></pre><p>We can construct the input tensor <code>X</code> and the kernel tensor <code>K</code> corresponding to the values in :numref:<code>fig_conv_multi_in</code> to (<strong>validate the output</strong>) of the cross-correlation operation.</p><pre><code class="language-julia hljs">X = cat([0. 1. 2.; 3. 4. 5.; 6. 7. 8.], [1. 2. 3.; 4. 5. 6.; 7. 8. 9], dims = 3)
K = cat([0. 1.; 2. 3], [1. 2.; 3. 4.], dims = 3)
corr2d_multi_in(X, K)</code></pre><pre><code class="nohighlight hljs">2×2 Matrix{Float64}:
  56.0   72.0
 104.0  120.0</code></pre><h2 id="Multiple-Output-Channels"><a class="docs-heading-anchor" href="#Multiple-Output-Channels">Multiple Output Channels</a><a id="Multiple-Output-Channels-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Output-Channels" title="Permalink"></a></h2><p>:label:<code>subsec_multi-output-channels</code></p><p>Regardless of the number of input channels, so far we always ended up with one output channel. However, as we discussed in :numref:<code>subsec_why-conv-channels</code>, it turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures, we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater <em>channel depth</em>. Intuitively, you could think of each channel as responding to a different set of features. The reality is a bit more complicated than this. A naive interpretation would suggest  that representations are learned independently per pixel or per channel.  Instead, channels are optimized to be jointly useful. This means that rather than mapping a single channel to an edge detector, it may simply mean  that some direction in channel space corresponds to detecting edges.</p><p>Denote by <span>$c_\textrm{i}$</span> and <span>$c_\textrm{o}$</span> the number of input and output channels, respectively, and by <span>$k_\textrm{h}$</span> and <span>$k_\textrm{w}$</span> the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape <span>$c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$</span> for <em>every</em> output channel. We concatenate them on the output channel dimension, so that the shape of the convolution kernel is <span>$c_\textrm{o}\times c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$</span>. In cross-correlation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input tensor.</p><p>We implement a cross-correlation function to [<strong>calculate the output of multiple channels</strong>] as shown below.</p><pre><code class="language-julia hljs">function corr2d_multi_in_out(X, K)
    mapreduce(k -&gt; corr2d_multi_in(X, k), (v1, v2) -&gt; cat(v1, v2, dims=3), eachslice(K, dims = 4))
end</code></pre><pre><code class="nohighlight hljs">corr2d_multi_in_out (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">K = cat(K, K .+ 1, K .+ 2; dims = 4 )
size(K)</code></pre><pre><code class="nohighlight hljs">(2, 2, 2, 3)</code></pre><pre><code class="language-julia hljs">corr2d_multi_in_out(X, K)</code></pre><pre><code class="nohighlight hljs">2×2×3 Array{Float64, 3}:
[:, :, 1] =
  56.0   72.0
 104.0  120.0

[:, :, 2] =
  76.0  100.0
 148.0  172.0

[:, :, 3] =
  96.0  128.0
 192.0  224.0</code></pre><pre><code class="language-julia hljs"></code></pre><h2 id="1\\times-1-Convolutional-Layer"><a class="docs-heading-anchor" href="#1\\times-1-Convolutional-Layer"><span>$1\times 1$</span> Convolutional Layer</a><a id="1\\times-1-Convolutional-Layer-1"></a><a class="docs-heading-anchor-permalink" href="#1\\times-1-Convolutional-Layer" title="Permalink"></a></h2><p>:label:<code>subsec_1x1</code></p><p>At first, a [<strong><span>$1 \times 1$</span> convolution</strong>], i.e., <span>$k_\textrm{h} = k_\textrm{w} = 1$</span>, does not seem to make much sense. After all, a convolution correlates adjacent pixels. A <span>$1 \times 1$</span> convolution obviously does not. Nonetheless, they are popular operations that are sometimes included in the designs of complex deep networks :cite:<code>Lin.Chen.Yan.2013,Szegedy.Ioffe.Vanhoucke.ea.2017</code>. Let&#39;s see in some detail what it actually does.</p><p>Because the minimum window is used, the <span>$1\times 1$</span> convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions. The only computation of the <span>$1\times 1$</span> convolution occurs on the channel dimension.</p><p>:numref:<code>fig_conv_1x1</code> shows the cross-correlation computation using the <span>$1\times 1$</span> convolution kernel with 3 input channels and 2 output channels. Note that the inputs and outputs have the same height and width. Each element in the output is derived from a linear combination of elements <em>at the same position</em> in the input image. You could think of the <span>$1\times 1$</span> convolutional layer as constituting a fully connected layer applied at every single pixel location to transform the <span>$c_\textrm{i}$</span> corresponding input values into <span>$c_\textrm{o}$</span> output values. Because this is still a convolutional layer, the weights are tied across pixel location. Thus the <span>$1\times 1$</span> convolutional layer requires <span>$c_\textrm{o}\times c_\textrm{i}$</span> weights (plus the bias). Also note that convolutional layers are typically followed  by nonlinearities. This ensures that <span>$1 \times 1$</span> convolutions cannot simply be  folded into other convolutions. </p><p><img src="../../img/conv-1x1.svg" alt="The cross-correlation computation uses the \$1\\times 1\$ convolution kernel with three input channels and two output channels. The input and output have the same height and width."/> :label:<code>fig_conv_1x1</code></p><p>Let&#39;s check whether this works in practice: we implement a <span>$1 \times 1$</span> convolution using a fully connected layer. The only thing is that we need to make some adjustments to the data shape before and after the matrix multiplication.</p><pre><code class="language-julia hljs">function corr2d_multi_in_out_1x1(X, K)
    h, w, cin = size(X)
    # size(K) = 1 x 1 x cin x cout 
    _, _, ch_in, ch_out = size(K)
    X = reshape(X, :, ch_in)
    K = reshape(K, ch_in, ch_out)
    Y = X * K
    return reshape(Y, h, w, ch_out)
end</code></pre><pre><code class="nohighlight hljs">corr2d_multi_in_out_1x1 (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">X = randn((3,3,3))
K = randn(1, 1, 3, 2)
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
@assert sum(abs.(Y1 - Y2)) &lt; 1e-6</code></pre><h2 id="Discussion"><a class="docs-heading-anchor" href="#Discussion">Discussion</a><a id="Discussion-1"></a><a class="docs-heading-anchor-permalink" href="#Discussion" title="Permalink"></a></h2><p>Channels allow us to combine the best of both worlds: MLPs that allow for significant nonlinearities and convolutions that allow for <em>localized</em> analysis of features. In particular, channels allow the CNN to reason with multiple features, such as edge and shape detectors at the same time. They also offer a practical trade-off between the drastic parameter reduction arising from translation invariance and locality, and the need for expressive and diverse models in computer vision. </p><p>Note, though, that this flexibility comes at a price. Given an image of size <span>$(h \times w)$</span>, the cost for computing a <span>$k \times k$</span> convolution is <span>$\mathcal{O}(h \cdot w \cdot k^2)$</span>. For <span>$c_\textrm{i}$</span> and <span>$c_\textrm{o}$</span> input and output channels respectively this increases to <span>$\mathcal{O}(h \cdot w \cdot k^2 \cdot c_\textrm{i} \cdot c_\textrm{o})$</span>. For a <span>$256 \times 256$</span> pixel image with a <span>$5 \times 5$</span> kernel and <span>$128$</span> input and output channels respectively this amounts to over 53 billion operations (we count multiplications and additions separately). Later on we will encounter effective strategies to cut down on the cost, e.g., by requiring the channel-wise operations to be block-diagonal, leading to architectures such as ResNeXt :cite:<code>Xie.Girshick.Dollar.ea.2017</code>. </p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Assume that we have two convolution kernels of size <span>$k_1$</span> and <span>$k_2$</span>, respectively  (with no nonlinearity in between).<ol><li>Prove that the result of the operation can be expressed by a single convolution.</li><li>What is the dimensionality of the equivalent single convolution?</li><li>Is the converse true, i.e., can you always decompose a convolution into two smaller ones?</li></ol></li><li>Assume an input of shape <span>$c_\textrm{i}\times h\times w$</span> and a convolution kernel of shape  <span>$c_\textrm{o}\times c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$</span>, padding of <span>$(p_\textrm{h}, p_\textrm{w})$</span>, and stride of <span>$(s_\textrm{h}, s_\textrm{w})$</span>.<ol><li>What is the computational cost (multiplications and additions) for the forward propagation?</li><li>What is the memory footprint?</li><li>What is the memory footprint for the backward computation?</li><li>What is the computational cost for the backpropagation?</li></ol></li><li>By what factor does the number of calculations increase if we double both the number of input channels  <span>$c_\textrm{i}$</span> and the number of output channels <span>$c_\textrm{o}$</span>? What happens if we double the padding?</li><li>Are the variables <code>Y1</code> and <code>Y2</code> in the final example of this section exactly the same? Why?</li><li>Express convolutions as a matrix multiplication, even when the convolution window is not <span>$1 \times 1$</span>. </li><li>Your task is to implement fast convolutions with a <span>$k \times k$</span> kernel. One of the algorithm candidates  is to scan horizontally across the source, reading a <span>$k$</span>-wide strip and computing the <span>$1$</span>-wide output strip  one value at a time. The alternative is to read a <span>$k + \Delta$</span> wide strip and compute a <span>$\Delta$</span>-wide  output strip. Why is the latter preferable? Is there a limit to how large you should choose <span>$\Delta$</span>?</li><li>Assume that we have a <span>$c \times c$</span> matrix. <ol><li>How much faster is it to multiply with a block-diagonal matrix if the matrix is broken up into <span>$b$</span> blocks?</li><li>What is the downside of having <span>$b$</span> blocks? How could you fix it, at least partly?</li></ol></li></ol><pre><code class="language-julia hljs"></code></pre><p>Number of convolution operations per input channel:  <span>$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor$</span></p><p>Number of convolution operations for all input channels: <span>$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor \times ci$</span> </p><p>Number of convolution operations for all output channels: <span>$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor \times ci \times co$</span> </p><p>Number of multiplications in a single convolution: $ k<em>h \times k</em>w$</p><p>Number of additions in a single convolution: $ k<em>h \times k</em>w$</p><p>Total number of multiplications and additions: <span>$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor \times ci \times co \times (k_h \times k_w)$</span> </p><p>Additions for for combining all cin: $ k_h \times kw \times cin $</p><p>Number of times we do the above addition: $ co $ </p><p>Number of additions combining all cin for co times: $ k_h \times kw \times cin \times co$</p><p>Total Number of additions:  <span>$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor \times ci \times co \times (k_h \times k_w) + k_h \times kw \times cin \times co$</span>  </p><p>Total Number of multiplications:  <span>$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor \times ci \times co \times (k_h \times k_w)$</span> </p><pre><code class="language-julia hljs">function fast_corr2d_kwide(X, K)
    Y = zeros(size(X) .- size(K) .+ 1)
    for j in 1:size(Y, 2)
        Y[:, j] .= X[:, j:j+k-1]
    end
end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../CNN_3/">« Padding and Stride</a><a class="docs-footer-nextpage" href="../CNN_5/">Pooling »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
