import{_ as s,c as i,o as a,aA as t}from"./chunks/framework.DjA5121Y.js";const n="/d2l-julia/previews/PR1/assets/encoder-decoder.Bn-UyWxK.svg",u=JSON.parse('{"title":"The Encoder–Decoder Architecture","description":"","frontmatter":{},"headers":[],"relativePath":"CH9.Modern_Recurrent_Neural_Networks/MRNN_6.md","filePath":"CH9.Modern_Recurrent_Neural_Networks/MRNN_6.md","lastUpdated":null}'),h={name:"CH9.Modern_Recurrent_Neural_Networks/MRNN_6.md"};function l(r,e,d,p,o,c){return a(),i("div",null,e[0]||(e[0]=[t('<h1 id="The-Encoder–Decoder-Architecture" tabindex="-1">The Encoder–Decoder Architecture <a class="header-anchor" href="#The-Encoder–Decoder-Architecture" aria-label="Permalink to &quot;The Encoder–Decoder Architecture {#The-Encoder–Decoder-Architecture}&quot;">​</a></h1><p>In general sequence-to-sequence problems like machine translation (:numref:<code>sec_machine_translation</code>), inputs and outputs are of varying lengths that are unaligned. The standard approach to handling this sort of data is to design an <em>encoder–decoder</em> architecture (<a href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6#fig_encoder_decoder">Figure</a>) consisting of two major components: an <em>encoder</em> that takes a variable-length sequence as input, and a <em>decoder</em> that acts as a conditional language model, taking in the encoded input and the leftwards context of the target sequence and predicting the subsequent token in the target sequence. <img src="'+n+`" id="fig_encoder_decoder"></p><p><em>The encoder–decoder architecture.</em></p><p>Let&#39;s take machine translation from English to French as an example. Given an input sequence in English: &quot;They&quot;, &quot;are&quot;, &quot;watching&quot;, &quot;.&quot;, this encoder–decoder architecture first encodes the variable-length input into a state, then decodes the state to generate the translated sequence, token by token, as output: &quot;Ils&quot;, &quot;regardent&quot;, &quot;.&quot;. Since the encoder–decoder architecture forms the basis of different sequence-to-sequence models in subsequent sections, this section will convert this architecture into an interface that will be implemented later.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Pkg; Pkg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">activate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;../../d2lai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d2lai</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Flux </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Downloads</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StatsBase</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Plots</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CUDA, cuDNN</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d2lai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> RNNScratch</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  Activating project at \`/workspace/d2l-julia/d2lai\`</span></span></code></pre></div><h2 id="Encoder" tabindex="-1">Encoder <a class="header-anchor" href="#Encoder" aria-label="Permalink to &quot;Encoder {#Encoder}&quot;">​</a></h2><p>In the encoder interface, we just specify that the encoder takes variable-length sequences as input <code>X</code>. The implementation will be provided by any model that subtypes this base <code>AbstractEncoder</code></p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">abstract type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AbstractEncoder </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> end</span></span></code></pre></div><h2 id="Decoder" tabindex="-1">Decoder <a class="header-anchor" href="#Decoder" aria-label="Permalink to &quot;Decoder {#Decoder}&quot;">​</a></h2><p>In the following decoder interface, we add an additional <code>init_state</code> method to convert the encoder output (<code>enc_all_outputs</code>) into the encoded state. Note that this step may require extra inputs, such as the valid length of the input, which was explained in :numref:<code>sec_machine_translation</code>. To generate a variable-length sequence token by token, every time the decoder may map an input (e.g., the generated token at the previous time step) and the encoded state into an output token at the current time step. However this is handled in the next section and we currently only define the abstract class and the <code>init_state</code> method</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">abstract type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AbstractDecoder </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_state</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(decoder</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractDecoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, args</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>init_state (generic function with 1 method)</span></span></code></pre></div><h2 id="Putting-the-Encoder-and-Decoder-Together" tabindex="-1">Putting the Encoder and Decoder Together <a class="header-anchor" href="#Putting-the-Encoder-and-Decoder-Together" aria-label="Permalink to &quot;Putting the Encoder and Decoder Together {#Putting-the-Encoder-and-Decoder-Together}&quot;">​</a></h2><p>In the forward propagation, the output of the encoder is used to produce the encoded state, and this state will be further used by the decoder as one of its input.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">abstract type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AbstractEncoderDecoder </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractClassifier</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> end</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractEncoderDecoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(enc_X, dec_X, args</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    enc_all_outputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">encoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(enc_X, args</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    dec_state </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> init_state</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">decoder, enc_all_outputs, args</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">decoder</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dec_X, dec_state)[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><p>In the next section, we will see how to apply RNNs to design sequence-to-sequence models based on this encoder–decoder architecture.</p><h2 id="Summary" tabindex="-1">Summary <a class="header-anchor" href="#Summary" aria-label="Permalink to &quot;Summary {#Summary}&quot;">​</a></h2><p>Encoder-decoder architectures can handle inputs and outputs that both consist of variable-length sequences and thus are suitable for sequence-to-sequence problems such as machine translation. The encoder takes a variable-length sequence as input and transforms it into a state with a fixed shape. The decoder maps the encoded state of a fixed shape to a variable-length sequence.</p><h2 id="Exercises" tabindex="-1">Exercises <a class="header-anchor" href="#Exercises" aria-label="Permalink to &quot;Exercises {#Exercises}&quot;">​</a></h2><ol><li><p>Suppose that we use neural networks to implement the encoder–decoder architecture. Do the encoder and the decoder have to be the same type of neural network?</p></li><li><p>Besides machine translation, can you think of another application where the encoder–decoder architecture can be applied?</p></li></ol><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"></span></code></pre></div>`,22)]))}const g=s(h,[["render",l]]);export{u as __pageData,g as default};
