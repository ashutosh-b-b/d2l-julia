import{_ as n,c as a,o as i,aA as o}from"./chunks/framework.DjA5121Y.js";const g=JSON.parse('{"title":"References","description":"","frontmatter":{},"headers":[],"relativePath":"references.md","filePath":"references.md","lastUpdated":null}'),r={name:"references.md"};function t(l,e,s,m,d,p){return i(),a("div",null,e[0]||(e[0]=[o('<h1 id="References" tabindex="-1">References <a class="header-anchor" href="#References" aria-label="Permalink to &quot;References {#References}&quot;">​</a></h1><ol><li><p><a id="Legendre_1805"></a> A. M. Legendre. <em>Mémoire sur les Opérations Trigonométriques: dont les Résultats Dépendent de la Figure de la Terre</em> (F. Didot, 1805).</p></li><li><p><a id="Gauss_1809"></a> C. F. Gauss. <em>Theoria motus corporum coelestum</em>. In: <em>Werke</em> (Königlich Preussische Akademie der Wissenschaften, 1809).</p></li><li><p><a id="Golub_Van-Loan_1996"></a> G. H. Golub and C. F. Van Loan. <em>Matrix Computations</em> (Johns Hopkins University Press, 1996).</p></li><li><p><a id="Liu_Nocedal_1989"></a> D. C. Liu and J. Nocedal. <em>On the limited memory BFGS method for large scale optimization</em>. Mathematical Programming <strong>45</strong>, 503–528 (1989).</p></li><li><p><a id="Bottou_2010"></a> L. Bottou. <em>Large-scale machine learning with stochastic gradient descent</em>. In: <em>Proceedings of COMPSTAT&#39;2010</em> (Springer, 2010); pp. 177–186.</p></li><li><p><a id="Li_Zhang_Chen_ea_2014"></a> M. Li, T. Zhang, Y. Chen and A. J. Smola. <em>Efficient mini-batch training for stochastic optimization</em>. In: <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2014); pp. 661–670.</p></li><li><p><a id="Frazier_2018"></a> P. I. Frazier. <em>A tutorial on Bayesian optimization</em>. ArXiv:1807.02811 (2018).</p></li><li><p><a id="Izmailov_Podoprikhin_Garipov_ea_2018"></a> P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov and A. G. Wilson. <em>Averaging weights leads to wider optima and better generalization</em>. ArXiv:1803.05407 (2018).</p></li><li><p><a id="Frankle_Carbin_2018"></a> J. Frankle and M. Carbin. <em>The lottery ticket hypothesis: Finding sparse, trainable neural networks</em>. ArXiv:1803.03635 (2018).</p></li><li><p><a id="Russell_Norvig_2016"></a> S. J. Russell and P. Norvig. <em>Artificial Intelligence: A Modern Approach</em> (Pearson Education Limited, 2016).</p></li><li><p><a id="Black_Scholes_1973"></a> F. Black and M. Scholes. <em>The pricing of options and corporate liabilities</em>. Journal of Political Economy <strong>81</strong>, 637–654 (1973).</p></li><li><p><a id="Naor_Reingold_1999"></a> M. Naor and O. Reingold. <em>On the construction of pseudorandom permutations: Luby–Rackoff revisited</em>. Journal of Cryptology <strong>12</strong>, 29–66 (1999).</p></li><li><p><a id="Vapnik_1992"></a> V. Vapnik. <em>Principles of risk minimization for learning theory</em>. In: <em>Advances in Neural Information Processing Systems</em> (1992); pp. 831–838.</p></li><li><p><a id="Bergstra_Breuleux_Bastien_ea_2010"></a> J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley and Y. Bengio. <em>Theano: A CPU and GPU math compiler in Python</em>. In: <em>Proc. 9th Python in Science Conference</em>, Vol. 1 (2010); pp. 3–10.</p></li><li><p><a id="Dean_Corrado_Monga_ea_2012"></a> J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker and al. <em>Large scale distributed deep networks</em>. In: <em>Proceedings of the 25th International Conference on Neural Information Processing Systems, Volume 1</em> (2012); pp. 1223–1231.</p></li><li><p><a id="Jia_Shelhamer_Donahue_ea_2014"></a> Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama and T. Darrell. <em>Caffe: Convolutional architecture for fast feature embedding</em>. In: <em>Proceedings of the 22nd ACM International Conference on Multimedia</em> (2014); pp. 675–678.</p></li><li><p><a id="Bottou_Le-Cun_1988"></a> L. Bottou and Y. Le Cun. <a href="http://leon.bottou.org/papers/bottou-lecun-88" target="_blank" rel="noreferrer"><em>SN: A simulator for connectionist models</em></a>. In: <em>Proceedings of NeuroNimes 88</em> (Nimes, France, 1988); pp. 371–382.</p></li><li><p><a id="Chen_Li_Li_ea_2015"></a> T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang and Z. Zhang. <em>MXNET: A flexible and efficient machine learning library for heterogeneous distributed systems</em>. ArXiv:1512.01274 (2015).</p></li><li><p><a id="Frostig_Johnson_Leary_2018"></a> R. Frostig, M. J. Johnson and C. Leary. <em>Compiling machine learning programs via high-level tracing</em>. In: <em>Proceedings of Systems for Machine Learning</em> (GoogleResearch, 2018).</p></li><li><p><a id="Paszke_Gross_Massa_ea_2019"></a> A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga and al. <em>PyTorch: An imperative style, high-performance deep learning library</em>. Advances in Neural Information Processing Systems <strong>32</strong>, 8026–8037 (2019).</p></li><li><p><a id="Abadi_Barham_Chen_ea_2016"></a> M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard and al. <em>TensorFlow: A system for large-scale machine learning</em>. In: <em>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</em> (2016); pp. 265–283.</p></li><li><p><a id="Deng_Dong_Socher_ea_2009"></a> J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and L. Fei-Fei. <em>Imagenet: A large-scale hierarchical image database</em>. In: <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> (IEEE, 2009); pp. 248–255.</p></li><li><p><a id="thomee2016yfcc100m"></a> B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth and L.-J. Li. <em>YFCC100M: The new data in multimedia research</em>. Communications of the ACM <strong>59</strong>, 64–73 (2016).</p></li><li><p><a id="Vapnik98"></a> V. Vapnik. <em>Statistical Learning Theory</em> (John Wiley and Sons, New York, 1998).</p></li><li><p><a id="boucheron2005theory"></a> S. Boucheron, O. Bousquet and G. Lugosi. <em>Theory of classification: A survey of some recent advances</em>. ESAIM: Probability and Statistics <strong>9</strong>, 323–375 (2005).</p></li><li><p><a id="vapnik1994measuring"></a> V. Vapnik, E. Levin and Y. Le Cun. <em>Measuring the VC-dimension of a learning machine</em>. Neural Computation <strong>6</strong>, 851–876 (1994).</p></li><li><p><a id="Scholkopf_Smola_2002"></a> B. Schölkopf and A. J. Smola. <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</em> (MIT Press, 2002).</p></li><li><p><a id="ong2005learning"></a> C. S. Ong, A. Smola and R. Williamson. <em>Learning the kernel with hyperkernels</em>. Journal of Machine Learning Research <strong>6</strong>, 1043–1071 (2005).</p></li><li><p><a id="Tsoumakas_Katakis_2007"></a> G. Tsoumakas and I. Katakis. <em>Multi-label classification: An overview</em>. International Journal of Data Warehousing and Mining <strong>3</strong>, 1–13 (2007).</p></li><li><p><a id="Huang_Xu_Yu_2015"></a> Z. Huang, W. Xu and K. Yu. <em>Bidirectional LSTM–CRF models for sequence tagging</em>. ArXiv:1508.01991 (2015).</p></li><li><p><a id="Moon_Smola_Chang_ea_2010"></a> T. Moon, A. Smola, Y. Chang and Z. Zheng. <em>Intervalrank: isotonic regression with listwise and pairwise constraints</em>. In: <em>Proceedings of the 3rd ACM International Conference on Web Search and Data Mining</em> (2010); pp. 151–160.</p></li><li><p><a id="Beutel_Murray_Faloutsos_ea_2014"></a> A. Beutel, K. Murray, C. Faloutsos and A. J. Smola. <em>CoBaFi: collaborative Bayesian filtering</em>. In: <em>Proceedings of the 23rd International Conference on World Wide Web</em> (2014); pp. 97–108.</p></li><li><p><a id="Shannon_1948"></a> C. E. Shannon. <em>A Mathematical Theory of Communication</em>. The Bell System Technical Journal <strong>27</strong>, 379–423 (1948).</p></li><li><p><a id="Yang_Moczulski_Denil_ea_2015"></a> Z. Yang, M. Moczulski, M. Denil, N. De Freitas, A. Smola, L. Song and Z. Wang. <em>Deep fried convnets</em>. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em> (2015); pp. 1476–1483.</p></li><li><p><a id="sindhwani2015structured"></a> V. Sindhwani, T. N. Sainath and S. Kumar. <em>Structured transforms for small-footprint deep learning</em>. ArXiv:1510.01722 (2015).</p></li><li><p><a id="Zhang_Tay_Zhang_ea_2021"></a> A. Zhang, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui and J. Fu. <em>Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</em>. In: <em>International Conference on Learning Representations</em> (2021).</p></li><li><p><a id="Bradley_Terry_1952"></a> R. A. Bradley and M. E. Terry. <em>Rank analysis of incomplete block designs: I. The method of paired comparisons</em>. Biometrika <strong>39</strong>, 324–345 (1952).</p></li><li><p><a id="LeCun_Bottou_Bengio_ea_1998"></a> Y. LeCun, L. Bottou, Y. Bengio and P. Haffner. <em>Gradient-based learning applied to document recognition</em>. Proceedings of the IEEE <strong>86</strong>, 2278–2324 (1998).</p></li><li><p><a id="LeCun_Jackel_Bottou_ea_1995"></a> Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker, H. Drucker, I. Guyon, U. Muller, E. Sackinger and al. <em>Comparison of learning algorithms for handwritten digit recognition</em>. In: <em>International Conference on Artificial Neural Networks</em> (1995); pp. 53–60.</p></li><li><p><a id="Scholkopf_Burges_Vapnik_1996"></a> B. Schölkopf, C. Burges and V. Vapnik. <em>Incorporating invariances in support vector learning machines</em>. In: <em>International Conference on Artificial Neural Networks</em> (Springer, 1996); pp. 47–52.</p></li><li><p><a id="Simard_LeCun_Denker_ea_1998"></a> P. Y. Simard, Y. A. LeCun, J. S. Denker and B. Victorri. <em>Transformation invariance in pattern recognition – tangent distance and tangent propagation</em>. In: <em>Neural Networks: Tricks of the Trade</em>, Vol. 529 no. 7587 (Springer, 1998); pp. 239–274.</p></li><li><p><a id="Xiao_Rasul_Vollgraf_2017"></a> H. Xiao, K. Rasul and R. Vollgraf. <em>Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</em>. ArXiv:1708.07747 (2017).</p></li><li><p><a id="dwork2015preserving"></a> C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold and A. L. Roth. <em>Preserving statistical validity in adaptive data analysis</em>. In: <em>Proceedings of the 47th Annual ACM Symposium on Theory of Computing</em> (2015); pp. 117–126.</p></li><li><p><a id="VapChe64"></a> V. Vapnik and A. Chervonenkis. <em>A note on one class of perceptrons</em>. Automation and Remote Control <strong>25</strong> (1964).</p></li><li><p><a id="VapChe68"></a> V. Vapnik and A. Chervonenkis. <em>Uniform convergence of frequencies of occurence of events to their probabilities</em>. Dokl.Ãkad.Ñauk SSSR <strong>181</strong>, 915–918 (1968).</p></li><li><p><a id="VapChe71"></a> V. Vapnik and A. Chervonenkis. <em>On the uniform convergence of relative frequencies of events to their probabilities</em>. Theory Probab. Appl. <strong>16</strong>, 264–281 (1971).</p></li><li><p><a id="VapChe74b"></a> V. Vapnik and A. Chervonenkis. <em>Ordered risk minimization</em>. Automation and Remote Control <strong>35</strong>, 1226–1235, 1403–1412 (1974).</p></li><li><p><a id="VapChe81"></a> V. Vapnik and A. Chervonenkis. <em>The necessary and sufficient conditions for the uniform convergence of averages to their expected values</em>. Teoriya Veroyatnostei i Ee Primeneniya <strong>26</strong>, 543–564 (1981).</p></li><li><p><a id="VapChe91"></a> V. Vapnik and A. Chervonenkis. <em>The necessary and sufficient conditions for consistency in the empirical risk minimization method</em>. Pattern Recognition and Image Analysis <strong>1</strong>, 283–305 (1991).</p></li><li><p><a id="Shao_Yao_Sun_ea_2020"></a> H. Shao, S. Yao, D. Sun, A. Zhang, S. Liu, D. Liu, J. Wang and T. Abdelzaher. <em>ControlVAE: Controllable variational autoencoder</em>. In: <em>Proceedings of the 37th International Conference on Machine Learning</em> (JMLR. org, 2020).</p></li><li><p><a id="Fisher_1928"></a> R. A. Fisher. <em>Statistical Methods for Research Workers.</em> (Oliver &amp; Boyd, 1925).</p></li><li><p><a id="quinlan2014c4"></a> J. R. Quinlan. <em>C4.5: Programs for Machine Learning</em>. Vol. 51 no. 4 (Elsevier, 1993); p. 66.</p></li><li><p><a id="Aronszajn_1950"></a> N. Aronszajn. <em>Theory of reproducing kernels</em>. Transactions of the American Mathematical Society <strong>68</strong>, 337–404 (1950).</p></li><li><p><a id="Wahba_1990"></a> G. Wahba. <em>Spline Models for Observational Data</em> (SIAM, 1990).</p></li><li><p><a id="Cajal_Azoulay_1894"></a> S. Ramón y Cajal and L. Azoulay. <em>Les Nouvelles Idées sur la Structure du Système Nerveux chez l&#39;Homme et chez les Vertébrés</em>. Vol. 11 no. 2 (Paris, C. Reinwald &amp; Cie, 1894); p. 125.</p></li><li><p><a id="McCulloch_Pitts_1943"></a> W. S. McCulloch and W. Pitts. <em>A logical calculus of the ideas immanent in nervous activity</em>. Bulletin of Mathematical Biophysics <strong>5</strong>, 115–133 (1943).</p></li><li><p><a id="LeCun_Bottou_Orr_ea_1998"></a> Y. LeCun, L. Bottou, G. Orr and K.-R. Muller. <em>Efficient backprop</em>. In: <em>Neural Networks: Tricks of the Trade</em> (Springer, 1998).</p></li><li><p><a id="Kalman_Kwasny_1992"></a> B. L. Kalman and S. C. Kwasny. <em>Why tanh: choosing a sigmoidal function</em>. In: <em>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</em> (IEEE, 1992); pp. 578–581.</p></li><li><p><a id="Hendrycks_Gimpel_2016"></a> D. Hendrycks and K. Gimpel. <em>Gaussian error linear units (GELUs)</em>. ArXiv:1606.08415 (2016).</p></li><li><p><a id="Ramachandran_Zoph_Le_2017"></a> P. Ramachandran, B. Zoph and Q. V. Le. <em>Searching for activation functions</em>. ArXiv:1710.05941 (2017).</p></li><li><p><a id="Ioffe_Szegedy_2015"></a> S. Ioffe and C. Szegedy. <em>Batch normalization: Accelerating deep network training by reducing internal covariate shift</em>. ArXiv:1502.03167 (2015).</p></li><li><p><a id="Xiao_Bahri_Sohl-Dickstein_ea_2018"></a> L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. Schoenholz and J. Pennington. <em>Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks</em>. In: <em>International Conference on Machine Learning</em> (2018); pp. 5393–5402.</p></li><li><p><a id="You_Gitman_Ginsburg_2017"></a> Y. You, I. Gitman and B. Ginsburg. <em>Large batch training of convolutional networks</em>. ArXiv:1708.03888 (2017).</p></li><li><p><a id="wolpert1995no"></a> D. H. Wolpert and W. G. Macready. <em>No free lunch theorems for search</em> (Technical Report SFI-TR-95-02-010, Santa Fe Institute, 1995).</p></li><li><p><a id="zhang2021understanding"></a> C. Zhang, S. Bengio, M. Hardt, B. Recht and O. Vinyals. <em>Understanding deep learning (still) requires rethinking generalization</em>. Communications of the ACM <strong>64</strong>, 107–115 (2021).</p></li><li><p><a id="nakkiran2021deep"></a> P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak and I. Sutskever. <em>Deep double descent: Where bigger models and more data hurt</em>. Journal of Statistical Mechanics: Theory and Experiment <strong>2021</strong>, 124003 (2021).</p></li><li><p><a id="Jacot_Grabriel_Hongler_2018"></a> A. Jacot, F. Gabriel and C. Hongler. <em>Neural tangent kernel: Convergence and generalization in neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em>, Vol. 31 (2018).</p></li><li><p><a id="Rolnick_Veit_Belongie_Shavit_2017"></a> D. Rolnick, A. Veit, S. Belongie and N. Shavit. <em>Deep learning is robust to massive label noise</em>. ArXiv:1705.10694 (2017).</p></li><li><p><a id="Garg_Balakrishnan_Kolter_Lipton_2021"></a> S. Garg, S. Balakrishnan, Z. Kolter and Z. Lipton. <em>RATT: Leveraging unlabeled data to guarantee generalization</em>. In: <em>International Conference on Machine Learning</em>, Vol. 31 (PMLR, 2021); pp. 3598–3609.</p></li><li><p><a id="Srivastava_Hinton_Krizhevsky_ea_2014"></a> N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov. <em>Dropout: a simple way to prevent neural networks from overfitting</em>. Journal of Machine Learning Research <strong>15</strong>, 1929–1958 (2014).</p></li><li><p><a id="Bishop_1995"></a> C. M. Bishop. <em>Training with noise is equivalent to Tikhonov regularization</em>. Neural Computation <strong>7</strong>, 108–116 (1995).</p></li><li><p><a id="Hubel_Wiesel_1959"></a> D. H. Hubel and T. N. Wiesel. <em>Receptive fields of single neurones in the cat&#39;s striate cortex</em>. Journal of Physiology <strong>148</strong>, 574–591 (1959).</p></li><li><p><a id="Hubel_Wiesel_1962"></a> D. H. Hubel and T. N. Wiesel. <em>Receptive fields, binocular interaction and functional architecture in the cat&#39;s visual cortex</em>. Journal of Physiology <strong>160</strong>, 106–154 (1962).</p></li><li><p><a id="Hubel_Wiesel_1968"></a> D. H. Hubel and T. N. Wiesel. <em>Receptive fields and functional architecture of monkey striate cortex</em>. Journal of Physiology <strong>195</strong>, 215–243 (1968).</p></li><li><p><a id="Field_1987"></a> D. J. Field. <em>Relations between the statistics of natural images and the response properties of cortical cells</em>. JOSA A <strong>4</strong>, 2379–2394 (1987).</p></li><li><p><a id="Kuzovkin_Vicente_Petton_ea_2018"></a> I. Kuzovkin, R. Vicente, M. Petton, J.-P. Lachaux, M. Baciu, P. Kahane, S. Rheims, J. R. Vidal and J. Aru. <em>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex</em>. Communications Biology <strong>1</strong>, 1–12 (2018).</p></li><li><p><a id="Alsallakh_Kokhlikyan_Miglani_ea_2020"></a> B. Alsallakh, N. Kokhlikyan, V. Miglani, J. Yuan and O. Reblitz-Richardson. <em>Mind the PAD – CNNs can develop blind spots</em>. ArXiv:2010.02178 (2020).</p></li><li><p><a id="Lin_Chen_Yan_2013"></a> M. Lin, Q. Chen and S. Yan. <em>Network in network</em>. ArXiv:1312.4400 (2013).</p></li><li><p><a id="Szegedy_Ioffe_Vanhoucke_ea_2017"></a> C. Szegedy, S. Ioffe, V. Vanhoucke and A. A. Alemi. <em>Inception-v4, Inception-ResNet and the impact of residual connections on learning</em>. In: <em>31st AAAI Conference on Artificial Intelligence</em> (2017).</p></li><li><p><a id="Xie_Girshick_Dollar_ea_2017"></a> S. Xie, R. Girshick, P. Dollár, Z. Tu and K. He. <em>Aggregated residual transformations for deep neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2017); pp. 1492–1500.</p></li><li><p><a id="Riesenhuber_Poggio_1999"></a> M. Riesenhuber and T. Poggio. <em>Hierarchical models of object recognition in cortex</em>. Nature Neuroscience <strong>2</strong>, 1019–1025 (1999).</p></li><li><p><a id="Yamaguchi_Sakamoto_Akabane_ea_1990"></a> K. Yamaguchi, K. Sakamoto, T. Akabane and Y. Fujimoto. <em>A neural network for speaker-independent isolated word recognition</em>. In: <em>First International Conference on Spoken Language Processing</em> (1990).</p></li><li><p><a id="LeCun_Boser_Denker_ea_1989"></a> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard and L. D. Jackel. <em>Backpropagation applied to handwritten zip code recognition</em>. Neural Computation <strong>1</strong>, 541–551 (1989).</p></li><li><p><a id="Zhang_Sun_Jiang_ea_2021"></a> Y. Zhang, P. Sun, Y. Jiang, D. Yu, Z. Yuan, P. Luo, W. Liu and X. Wang. <em>ByteTrack: Multi-object tracking by associating every detection box</em>. ArXiv:2110.06864 (2021).</p></li><li><p><a id="Long_Shelhamer_Darrell_2015"></a> J. Long, E. Shelhamer and T. Darrell. <em>Fully convolutional networks for semantic segmentation</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015); pp. 3431–3440.</p></li><li><p><a id="Redmon_Farhadi_2018"></a> J. Redmon and A. Farhadi. <em>YOLOv3: An incremental improvement</em>. ArXiv:1804.02767 (2018).</p></li><li><p><a id="Gatys_Ecker_Bethge_2016"></a> L. A. Gatys, A. S. Ecker and M. Bethge. <em>Image style transfer using convolutional neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp. 2414–2423.</p></li><li><p><a id="Dosovitskiy_Beyer_Kolesnikov_ea_2021"></a> A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly and al. <em>An image is worth 16 x 16 words: Transformers for image recognition at scale</em>. In: <em>International Conference on Learning Representations</em> (2021).</p></li><li><p><a id="liu2021swin"></a> Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin and B. Guo. <em>Swin transformer: Hierarchical vision transformer using shifted windows</em>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, Vol. 34 (2021); pp. 10012–10022.</p></li><li><p><a id="Krizhevsky_Sutskever_Hinton_2012"></a> A. Krizhevsky, I. Sutskever and G. E. Hinton. <em>ImageNet classification with deep convolutional neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2012); pp. 1097–1105.</p></li><li><p><a id="Simonyan_Zisserman_2014"></a> K. Simonyan and A. Zisserman. <em>Very deep convolutional networks for large-scale image recognition</em>. ArXiv:1409.1556 (2014).</p></li><li><p><a id="Szegedy_Liu_Jia_ea_2015"></a> C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke and A. Rabinovich. <em>Going deeper with convolutions</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2015); pp. 1–9.</p></li><li><p><a id="He_Zhang_Ren_ea_2016"></a> K. He, X. Zhang, S. Ren and J. Sun. <em>Deep residual learning for image recognition</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp. 770–778.</p></li><li><p><a id="Huang_Liu_Van-Der-Maaten_ea_2017"></a> G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger. <em>Densely connected convolutional networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2017); pp. 4700–4708.</p></li><li><p><a id="wu2018shift"></a> B. Wu, A. Wan, X. Yue, P. Jin, S. Zhao, N. Golmant, A. Gholaminejad, J. Gonzalez and K. Keutzer. <em>Shift: A zero flop, zero parameter alternative to spatial convolutions</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018); pp. 9127–9135.</p></li><li><p><a id="Howard_Sandler_Chu_ea_2019"></a> A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le and H. Adam. <em>Searching for MobileNetV3</em>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (2019); pp. 1314–1324.</p></li><li><p><a id="Radosavovic_Kosaraju_Girshick_ea_2020"></a> I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He and P. Dollár. <em>Designing network design spaces</em>. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (2020); pp. 10428–10436.</p></li><li><p><a id="liu2022convnet"></a> Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell and S. Xie. <em>A ConvNet for the 2020s</em>. ArXiv:2201.03545 (2022).</p></li><li><p><a id="Freund_Schapire_ea_1996"></a> Y. Freund and R. E. Schapire. <em>Experiments with a new boosting algorithm</em>. In: <em>Proceedings of the International Conference on Machine Learning</em>, Vol. 96 (Citeseer, 1996); pp. 148–156.</p></li><li><p><a id="Taskar_Guestrin_Koller_2004"></a> B. Taskar, C. Guestrin and D. Koller. <em>Max-margin Markov networks</em>. Advances in Neural Information Processing Systems <strong>16</strong>, 25 (2004).</p></li><li><p><a id="Lowe_2004"></a> D. G. Lowe. <em>Distinctive image features from scale-invariant keypoints</em>. International Journal of Computer Vision <strong>60</strong>, 91–110 (2004).</p></li><li><p><a id="Bay_Tuytelaars_Van-Gool_2006"></a> H. Bay, T. Tuytelaars and L. Van Gool. <em>SURF: Speeded up robust features</em>. In: <em>European Conference on Computer Vision</em> (Springer, 2006); pp. 404–417.</p></li><li><p><a id="Sivic_Zisserman_2003"></a> J. Sivic and A. Zisserman. <em>Video Google: A text retrieval approach to object matching in videos</em>. In: <em>Proceedings of the IEEE International Conference on Computer Vision</em>, Vol. 3 (IEEE Computer Society, 2003); pp. 1470–1470.</p></li><li><p><a id="Hartley_Zisserman_2000"></a> R. Hartley and A. Zisserman. <em>Multiple View Geometry in Computer Vision</em> (Cambridge University Press, 2000).</p></li><li><p><a id="Glorot_Bengio_2010"></a> X. Glorot and Y. Bengio. <em>Understanding the difficulty of training deep feedforward neural networks</em>. In: <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em>, Vol. 4 (2010); pp. 249–256.</p></li><li><p><a id="Kingma_Ba_2014"></a> D. P. Kingma and J. Ba. <em>Adam: A method for stochastic optimization</em>. ArXiv:1412.6980 (2014).</p></li><li><p><a id="Nair_Hinton_2010"></a> V. Nair and G. E. Hinton. <em>Rectified linear units improve restricted Boltzmann machines</em>. In: <em>ICML</em> (2010).</p></li><li><p><a id="Boyd_Vandenberghe_2004"></a> S. Boyd and L. Vandenberghe. <em>Convex Optimization</em> (Cambridge University Press, Cambridge, England, 2004).</p></li><li><p><a id="hartley2009global"></a> R. I. Hartley and F. Kahl. <em>Global optimization through rotation space search</em>. International Journal of Computer Vision <strong>82</strong>, 64–79 (2009).</p></li><li><p><a id="Dalal_Triggs_2005"></a> N. Dalal and B. Triggs. <em>Histograms of oriented gradients for human detection</em>. In: <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#39;05)</em>, Vol. 1 (IEEE, 2005); pp. 886–893.</p></li><li><p><a id="olshausen1996emergence"></a> B. A. Olshausen and D. J. Field. <em>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</em>. Nature <strong>381</strong>, 607–609 (1996).</p></li><li><p><a id="le2013building"></a> Q. V. Le. <em>Building high-level features using large scale unsupervised learning</em>. In: <em>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</em> (IEEE, 2013); pp. 8595–8598.</p></li><li><p><a id="Miller_1995"></a> G. A. Miller. <em>WordNet: a lexical database for English</em>. Communications of the ACM <strong>38</strong>, 39–41 (1995).</p></li><li><p><a id="Torralba_Fergus_Freeman_2008"></a> A. Torralba, R. Fergus and W. T. Freeman. <em>80 million tiny images: A large data set for nonparametric object and scene recognition</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>30</strong>, 1958–1970 (2008).</p></li><li><p><a id="russakovsky2015imagenet"></a> O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein and al. <em>ImageNet large scale visual recognition challenge</em>. International Journal of Computer Vision <strong>115</strong>, 211–252 (2015).</p></li><li><p><a id="schuhmann2022laion"></a> C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman and al. <em>LAION-5B: An open large-scale dataset for training next generation image-text models</em>. ArXiv:2210.08402 (2022).</p></li><li><p><a id="Fernando_2004"></a> R. Fernando. <em>GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics</em>. Vol. 2 (Addison-Wesley, 2004).</p></li><li><p><a id="Russakovsky_Deng_Huang_ea_2013"></a> O. Russakovsky, J. Deng, Z. Huang, A. C. Berg and L. Fei-Fei. <em>Detecting avocados to zucchinis: what have we done, and where are we going?</em> In: <em>International Conference on Computer Vision (ICCV)</em>, Vol. 5 no. 3 (2013); p. 1.</p></li><li><p><a id="Buslaev_Iglovikov_Khvedchenya_ea_2020"></a> A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin and A. A. Kalinin. <em>Albumentations: Fast and flexible image augmentations</em>. Information <strong>11</strong>, 125 (2020).</p></li><li><p><a id="Mead_1980"></a> C. Mead. <em>Introduction to VLSI systems</em>. IEE Proceedings I-Solid-State and Electron Devices <strong>128</strong>, 18 (1980).</p></li><li><p><a id="bommasani2021opportunities"></a> R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill and al. <em>On the opportunities and risks of foundation models</em>. ArXiv:2108.07258 (2021).</p></li><li><p><a id="lavin2016fast"></a> A. Lavin and S. Gray. <em>Fast algorithms for convolutional neural networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, No. 3 (MIT Press, 2016); pp. 4013–4021.</p></li><li><p><a id="Goyal_Bochkovskiy_Deng_ea_2021"></a> A. Goyal, A. Bochkovskiy, J. Deng and V. Koltun. <em>Non-deep networks</em>. ArXiv:2110.07641 (2021).</p></li><li><p><a id="Szegedy_Vanhoucke_Ioffe_ea_2016"></a> C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna. <em>Rethinking the Inception architecture for computer vision</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2016); pp. 2818–2826.</p></li><li><p><a id="friedman1987exploratory"></a> J. H. Friedman. <em>Exploratory projection pursuit</em>. Journal of the American Statistical Association <strong>82</strong>, 249–266 (1987).</p></li><li><p><a id="guyon2008feature"></a> I. Guyon, S. Gunn, M. Nikravesh and L. A. Zadeh. <em>Feature Extraction: Foundations and Applications</em> (Springer, 2008).</p></li><li><p><a id="Vapnik95"></a> V. Vapnik. <em>The Nature of Statistical Learning Theory</em> (Springer, New York, 1995).</p></li><li><p><a id="Novikoff62"></a> A. B. J. Novikoff. <em>On convergence proofs on perceptrons</em>. In: <em>Proceedings of the Symposium on the Mathematical Theory of Automata</em> (Polytechnic Institute of Brooklyn, 1962); pp. 615–622.</p></li><li><p><a id="Ba_Kiros_Hinton_2016"></a> J. L. Ba, J. R. Kiros and G. E. Hinton. <em>Layer normalization</em>. ArXiv:1607.06450 (2016).</p></li><li><p><a id="Duchi_Hazan_Singer_2011"></a> J. Duchi, E. Hazan and Y. Singer. <em>Adaptive subgradient methods for online learning and stochastic optimization</em>. Journal of Machine Learning Research <strong>12</strong>, 2121–2159 (2011).</p></li><li><p><a id="Zaheer_Reddi_Sachan_ea_2018"></a> M. Zaheer, S. Reddi, D. Sachan, S. Kale and S. Kumar. <em>Adaptive methods for nonconvex optimization</em>. In: <em>Advances in Neural Information Processing Systems</em> (2018); pp. 9793–9803.</p></li><li><p><a id="anil2020scalable"></a> R. Anil, V. Gupta, T. Koren, K. Regan and Y. Singer. <em>Scalable second-order optimization for deep learning</em>. ArXiv:2002.09018 (2020).</p></li><li><p><a id="Teye_Azizpour_Smith_2018"></a> M. Teye, H. Azizpour and K. Smith. <em>Bayesian uncertainty estimation for batch normalized deep networks</em>. ArXiv:1802.06455 (2018).</p></li><li><p><a id="Luo_Wang_Shao_ea_2018"></a> P. Luo, X. Wang, W. Shao and Z. Peng. <em>Towards understanding regularization in batch normalization</em>. ArXiv:1809.00846 (2018).</p></li><li><p><a id="Lipton_Steinhardt_2018"></a> Z. C. Lipton and J. Steinhardt. <em>Troubling trends in machine learning scholarship</em>. Communications of the ACM <strong>17</strong>, 45–77 (2018).</p></li><li><p><a id="Santurkar_Tsipras_Ilyas_ea_2018"></a> S. Santurkar, D. Tsipras, A. Ilyas and A. Madry. <em>How does batch normalization help optimization?</em> In: <em>Advances in Neural Information Processing Systems</em> (2018); pp. 2483–2493.</p></li><li><p><a id="wang2022removing"></a> H. Wang, A. Zhang, S. Zheng, X. Shi, M. Li and Z. Wang. <em>Removing batch normalization boosts adversarial training</em>. In: <em>International Conference on Machine Learning</em> (PMLR, 2022); pp. 23433–23445.</p></li><li><p><a id="tikhonov1977solutions"></a> A. N. Tikhonov and V. Y. Arsenin. <em>Solutions of Ill-Posed Problems</em>. Vol. 2021 no. 12 (W.H. Winston, 1977); p. 124003.</p></li><li><p><a id="morozov2012methods"></a> V. A. Morozov. <em>Methods for Solving Incorrectly Posed Problems</em> (Springer, 1984).</p></li><li><p><a id="prakash2016neural"></a> A. Prakash, S. A. Hasan, K. Lee, V. Datla, A. Qadir, J. Liu and O. Farri. <em>Neural paraphrase generation with stacked residual LSTM networks</em>. ArXiv:1610.03098 (2016).</p></li><li><p><a id="kim2017residual"></a> J. Kim, M. El-Khamy and J. Lee. <em>Residual LSTM: Design of a deep recurrent architecture for distant speech recognition</em>. ArXiv:1701.03360 (2017).</p></li><li><p><a id="Vaswani_Shazeer_Parmar_ea_2017"></a> A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser and I. Polosukhin. <em>Attention is all you need</em>. In: <em>Advances in Neural Information Processing Systems</em> (2017); pp. 5998–6008.</p></li><li><p><a id="Kipf_Welling_2016"></a> T. N. Kipf and M. Welling. <em>Semi-supervised classification with graph convolutional networks</em>. ArXiv:1609.02907 (2016).</p></li><li><p><a id="Ren_He_Girshick_ea_2015"></a> S. Ren, K. He, R. Girshick and J. Sun. <em>Faster R-CNN: Towards real-time object detection with region proposal networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2015); pp. 91–99.</p></li><li><p><a id="srivastava2015highway"></a> R. K. Srivastava, K. Greff and J. Schmidhuber. <em>Highway networks</em>. ArXiv:1505.00387 (2015).</p></li><li><p><a id="He_Zhang_Ren_ea_2016_1"></a> K. He, X. Zhang, S. Ren and J. Sun. <em>Identity mappings in deep residual networks</em>. In: <em>European Conference on Computer Vision</em> (Springer, 2016); pp. 630–645.</p></li><li><p><a id="pleiss2017memory"></a> G. Pleiss, D. Chen, G. Huang, T. Li, L. Van Der Maaten and K. Q. Weinberger. <em>Memory-efficient implementation of densenets</em>. ArXiv:1707.06990 (2017).</p></li><li><p><a id="Hu_Shen_Sun_2018"></a> J. Hu, L. Shen and G. Sun. <em>Squeeze-and-excitation networks</em>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (2018); pp. 7132–7141.</p></li><li><p><a id="zoph2016neural"></a> B. Zoph and Q. V. Le. <em>Neural architecture search with reinforcement learning</em>. ArXiv:1611.01578 (2016).</p></li><li><p><a id="liu2018darts"></a> H. Liu, K. Simonyan and Y. Yang. <em>DARTS: Differentiable architecture search</em>. ArXiv:1806.09055 (2018).</p></li><li><p><a id="tan2019efficientnet"></a> M. Tan and Q. Le. <em>EfficientNet: Rethinking model scaling for convolutional neural networks</em>. In: <em>International Conference on Machine Learning</em> (PMLR, 2019); pp. 6105–6114.</p></li><li><p><a id="graves2008novel"></a> A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke and J. Schmidhuber. <em>A novel connectionist system for unconstrained handwriting recognition</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>31</strong>, 855–868 (2008).</p></li><li><p><a id="Sutskever_Vinyals_Le_2014"></a> I. Sutskever, O. Vinyals and Q. V. Le. <em>Sequence to sequence learning with neural networks</em>. In: <em>Advances in Neural Information Processing Systems</em> (2014); pp. 3104–3112.</p></li><li><p><a id="Lipton_Kale_2016"></a> Z. C. Lipton, D. C. Kale, C. Elkan and R. Wetzel. <em>Learning to diagnose with LSTM recurrent neural networks</em>. In: <em>International Conference on Learning Representations (ICLR)</em> (2016).</p></li><li><p><a id="Lipton_Berkowitz_Elkan_2015"></a> Z. C. Lipton, J. Berkowitz and C. Elkan. <em>A critical review of recurrent neural networks for sequence learning</em>. ArXiv:1506.00019 <strong>17</strong>, 45–77 (2015).</p></li><li><p><a id="Hoyer_Janzing_Mooij_ea_2009"></a> P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters and B. Schölkopf. <em>Nonlinear causal discovery with additive noise models</em>. In: <em>Advances in Neural Information Processing Systems</em> (2009); pp. 689–696.</p></li><li><p><a id="Peters_Janzing_Scholkopf_2017"></a> J. Peters, D. Janzing and B. Schölkopf. <em>Elements of Causal Inference: Foundations and Learning Algorithms</em> (MIT Press, 2017).</p></li><li><p><a id="Wood_Gasthaus_Archambeau_ea_2011"></a> F. Wood, J. Gasthaus, C. Archambeau, L. James and Y. W. Teh. <em>The sequence memoizer</em>. Communications of the ACM <strong>54</strong>, 91–98 (2011).</p></li><li><p><a id="Bengio_Ducharme_Vincent_ea_2003"></a> Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. <em>A neural probabilistic language model</em>. Journal of Machine Learning Research <strong>3</strong>, 1137–1155 (2003).</p></li><li><p><a id="Werbos_1990"></a> P. J. Werbos. <em>Backpropagation through time: what it does and how to do it</em>. Proceedings of the IEEE <strong>78</strong>, 1550–1560 (1990).</p></li><li><p><a id="Jaeger_2002"></a> H. Jaeger. <em>Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the ``echo state network&#39;&#39; approach</em>. Vol. 31 (GMD-Forschungszentrum Informationstechnik Bonn, 2002).</p></li><li><p><a id="Tallec_Ollivier_2017"></a> C. Tallec and Y. Ollivier. <em>Unbiasing truncated backpropagation through time</em>. ArXiv:1705.08209 (2017).</p></li><li><p><a id="elman1990finding"></a> J. L. Elman. <em>Finding structure in time</em>. Cognitive Science <strong>14</strong>, 179–211 (1990).</p></li><li><p><a id="bengio1994learning"></a> Y. Bengio, P. Simard and P. Frasconi. <em>Learning long-term dependencies with gradient descent is difficult</em>. IEEE Transactions on Neural Networks <strong>5</strong>, 157–166 (1994).</p></li><li><p><a id="Hochreiter_Bengio_Frasconi_ea_2001"></a> S. Hochreiter, Y. Bengio, P. Frasconi and J. Schmidhuber. <em>Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</em>. In: <em>A Field Guide to Dynamical Recurrent Neural Networks</em> (IEEE Press, 2001).</p></li><li><p><a id="Hochreiter_Schmidhuber_1997"></a> S. Hochreiter and J. Schmidhuber. <em>Long short-term memory</em>. Neural Computation <strong>9</strong>, 1735–1780 (1997).</p></li><li><p><a id="Cho_Van-Merrienboer_Bahdanau_ea_2014"></a> K. Cho, B. Van Merriënboer, D. Bahdanau and Y. Bengio. <em>On the properties of neural machine translation: Encoder–decoder approaches</em>. ArXiv:1409.1259 (2014).</p></li><li><p><a id="Chung_Gulcehre_Cho_ea_2014"></a> J. Chung, C. Gulcehre, K. Cho and Y. Bengio. <em>Empirical evaluation of gated recurrent neural networks on sequence modeling</em>. ArXiv:1412.3555 (2014).</p></li><li><p><a id="Schuster_Paliwal_1997"></a> M. Schuster and K. K. Paliwal. <em>Bidirectional recurrent neural networks</em>. IEEE Transactions on Signal Processing <strong>45</strong>, 2673–2681 (1997).</p></li><li><p><a id="Brown_Cocke_Della-Pietra_ea_1988"></a> P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, R. L. Mercer and P. Roossin. <em>A statistical approach to language translation</em>. In: <em>COLING Budapest 1988 Volume 1: International Conference on Computational Linguistics</em> (1988).</p></li><li><p><a id="Brown_Cocke_Della-Pietra_ea_1990"></a> P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. Lafferty, R. L. Mercer and P. S. Roossin. <em>A statistical approach to machine translation</em>. Computational Linguistics <strong>16</strong>, 79–85 (1990).</p></li><li><p><a id="Cho_Van-Merrienboer_Gulcehre_ea_2014"></a> K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk and Y. Bengio. <em>Learning phrase representations using RNN encoder–decoder for statistical machine translation</em>. ArXiv:1406.1078 (2014).</p></li><li><p><a id="Papineni_Roukos_Ward_ea_2002"></a> K. Papineni, S. Roukos, T. Ward and W.-J. Zhu. <em>BLEU: a method for automatic evaluation of machine translation</em>. In: <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em> (2002); pp. 311–318.</p></li><li><p><a id="Kalchbrenner_Grefenstette_Blunsom_2014"></a> N. Kalchbrenner, E. Grefenstette and P. Blunsom. <em>A convolutional neural network for modelling sentences</em>. ArXiv:1404.2188 (2014).</p></li><li><p><a id="yang2016neural"></a> Z. Yang, Z. Hu, Y. Deng, C. Dyer and A. Smola. <em>Neural machine translation with recurrent attention modeling</em>. ArXiv:1607.05108 (2016).</p></li><li><p><a id="Bahdanau_Cho_Bengio_2014"></a> D. Bahdanau, K. Cho and Y. Bengio. <em>Neural machine translation by jointly learning to align and translate</em>. ArXiv:1409.0473 (2014).</p></li><li><p><a id="Mnih_Heess_Graves_ea_2014"></a> V. Mnih, N. Heess, A. Graves and others. <em>Recurrent models of visual attention</em>. In: <em>Advances in Neural Information Processing Systems</em> (2014); pp. 2204–2212.</p></li><li><p><a id="Nadaraya_1964"></a> E. A. Nadaraya. <em>On estimating regression</em>. Theory of Probability &amp; its Applications <strong>9</strong>, 141–142 (1964).</p></li><li><p><a id="Watson_1964"></a> G. S. Watson. <em>Smooth regression analysis</em>. Sankhyā: The Indian Journal of Statistics, Series A, 359–372 (1964).</p></li><li><p><a id="mack1982weak"></a> Y.-P. Mack and B. W. Silverman. <em>Weak and strong uniform consistency of kernel regression estimates</em>. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete <strong>61</strong>, 405–415 (1982).</p></li><li><p><a id="Silverman86"></a> B. Silverman. <em>Density Estimation for Statistical and Data Analysis</em> (Chapman and Hall, 1986).</p></li><li><p><a id="norelli2022asif"></a> A. Norelli, M. Fumero, V. Maiorca, L. Moschella, E. Rodolà and F. Locatello. <em>ASIF: Coupled data turns unimodal models to multimodal without training</em>. ArXiv:2210.01738 (2022).</p></li><li><p><a id="shoeybi2019megatron"></a> M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper and B. Catanzaro. <em>Megatron-LM: Training multi-billion parameter language models using model parallelism</em>. ArXiv:1909.08053 (2019).</p></li><li><p><a id="Graves_2013"></a> A. Graves. <em>Generating sequences with recurrent neural networks</em>. ArXiv:1308.0850 (2013).</p></li><li><p><a id="rabiner1993fundamentals"></a> L. Rabiner and B.-H. Juang. <em>Fundamentals of Speech Recognition</em> (Prentice-Hall., 1993).</p></li><li><p><a id="chan2015listen"></a> W. Chan, N. Jaitly, Q. V. Le and O. Vinyals. <em>Listen, attend and spell</em>. ArXiv:1508.01211 (2015).</p></li><li><p><a id="Lin_Feng_Santos_ea_2017"></a> Z. Lin, M. Feng, C. N. Santos, M. Yu, B. Xiang, B. Zhou and Y. Bengio. <em>A structured self-attentive sentence embedding</em>. ArXiv:1703.03130 (2017).</p></li><li><p><a id="Cheng_Dong_Lapata_2016"></a> J. Cheng, L. Dong and M. Lapata. <em>Long short-term memory-networks for machine reading</em>. In: <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em> (2016); pp. 551–561.</p></li><li><p><a id="Parikh_Tackstrom_Das_ea_2016"></a> A. P. Parikh, O. Täckström, D. Das and J. Uszkoreit. <em>A decomposable attention model for natural language inference</em>. ArXiv:1606.01933 (2016).</p></li><li><p><a id="Paulus_Xiong_Socher_2017"></a> R. Paulus, C. Xiong and R. Socher. <em>A deep reinforced model for abstractive summarization</em>. ArXiv:1705.04304 (2017).</p></li><li><p><a id="shaw2018self"></a> P. Shaw, J. Uszkoreit and A. Vaswani. <em>Self-attention with relative position representations</em>. ArXiv:1803.02155 (2018).</p></li><li><p><a id="huang2018music"></a> C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Dinculescu and D. Eck. <em>Music transformer: generating music with long-term structure</em>. In: <em>International Conference on Learning Representations</em> (2018).</p></li></ol>',2)]))}const u=n(r,[["render",t]]);export{g as __pageData,u as default};
