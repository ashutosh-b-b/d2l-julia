import{_ as e,c as l,o as t,j as i,aA as n,a}from"./chunks/framework.DjA5121Y.js";const p="/d2l-julia/previews/PR1/assets/lenet.CYUyqK95.svg",O=JSON.parse('{"title":"Convolutional Neural Networks (LeNet)","description":"","frontmatter":{},"headers":[],"relativePath":"CH6.Convolutional_Neural_Networks/CNN_6.md","filePath":"CH6.Convolutional_Neural_Networks/CNN_6.md","lastUpdated":null}'),h={name:"CH6.Convolutional_Neural_Networks/CNN_6.md"},k={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},r={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 688","aria-hidden":"true"},o={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},d={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.394ex",height:"1.581ex",role:"img",focusable:"false",viewBox:"0 -677 1500 699","aria-hidden":"true"},c={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Q={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 2222.4 688","aria-hidden":"true"},E={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},g={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.507ex",role:"img",focusable:"false",viewBox:"0 -666 2222.4 666","aria-hidden":"true"},y={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},T={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"1.131ex",height:"1.532ex",role:"img",focusable:"false",viewBox:"0 -677 500 677","aria-hidden":"true"},u={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},C={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 688","aria-hidden":"true"},m={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},F={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.028ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 2222.4 688","aria-hidden":"true"},f={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},L={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 688","aria-hidden":"true"},w={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},A={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.291ex",height:"1.557ex",role:"img",focusable:"false",viewBox:"0 -666 3222.4 688","aria-hidden":"true"};function N(B,s,x,M,v,I){return t(),l("div",null,[s[39]||(s[39]=i("h1",{id:"sec_lenet",tabindex:"-1"},[a("Convolutional Neural Networks (LeNet) "),i("a",{class:"header-anchor",href:"#sec_lenet","aria-label":'Permalink to "Convolutional Neural Networks (LeNet) {#sec_lenet}"'},"​")],-1)),i("p",null,[s[4]||(s[4]=a("We now have all the ingredients required to assemble a fully-functional CNN. In our earlier encounter with image data, we applied a linear model with softmax regression (:numref:")),s[5]||(s[5]=i("code",null,"sec_softmax_scratch",-1)),s[6]||(s[6]=a(") and an MLP (:numref:")),s[7]||(s[7]=i("code",null,"sec_mlp-implementation",-1)),s[8]||(s[8]=a(") to pictures of clothing in the Fashion-MNIST dataset. To make such data amenable we first flattened each image from a ")),i("mjx-container",k,[(t(),l("svg",r,s[0]||(s[0]=[n("",1)]))),s[1]||(s[1]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"28"),i("mo",null,"×"),i("mn",null,"28")])],-1))]),s[9]||(s[9]=a(" matrix into a fixed-length ")),i("mjx-container",o,[(t(),l("svg",d,s[2]||(s[2]=[n("",1)]))),s[3]||(s[3]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"784")])],-1))]),s[10]||(s[10]=a("-dimensional vector, and thereafter processed them in fully connected layers. Now that we have a handle on convolutional layers, we can retain the spatial structure in our images. As an additional benefit of replacing fully connected layers with convolutional layers, we will enjoy more parsimonious models that require far fewer parameters."))]),s[40]||(s[40]=n("",7)),i("p",null,[s[17]||(s[17]=a("The basic units in each convolutional block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation. Note that while ReLUs and max-pooling work better, they had not yet been discovered. Each convolutional layer uses a ")),i("mjx-container",c,[(t(),l("svg",Q,s[11]||(s[11]=[n("",1)]))),s[12]||(s[12]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"5"),i("mo",null,"×"),i("mn",null,"5")])],-1))]),s[18]||(s[18]=a(" kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number of two-dimensional feature maps, typically increasing the number of channels. The first convolutional layer has 6 output channels, while the second has 16. Each ")),i("mjx-container",E,[(t(),l("svg",g,s[13]||(s[13]=[n("",1)]))),s[14]||(s[14]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"2"),i("mo",null,"×"),i("mn",null,"2")])],-1))]),s[19]||(s[19]=a(" pooling operation (stride 2) reduces dimensionality by a factor of ")),i("mjx-container",y,[(t(),l("svg",T,s[15]||(s[15]=[i("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[i("g",{"data-mml-node":"math"},[i("g",{"data-mml-node":"mn"},[i("path",{"data-c":"34",d:"M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z",style:{"stroke-width":"3"}})])])],-1)]))),s[16]||(s[16]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"4")])],-1))]),s[20]||(s[20]=a(" via spatial downsampling. The convolutional block emits an output with shape given by (batch size, number of channel, height, width)."))]),s[41]||(s[41]=n("",3)),i("p",null,[s[23]||(s[23]=a("We have taken some liberty in the reproduction of LeNet insofar as we have replaced the Gaussian activation layer by a softmax layer. This greatly simplifies the implementation, not least due to the fact that the Gaussian decoder is rarely used nowadays. Other than that, this network matches the original LeNet-5 architecture. Let's see what happens inside the network. By passing a single-channel (black and white) ")),i("mjx-container",u,[(t(),l("svg",C,s[21]||(s[21]=[n("",1)]))),s[22]||(s[22]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"28"),i("mo",null,"×"),i("mn",null,"28")])],-1))]),s[24]||(s[24]=a(" image through the network and printing the output shape at each layer, we can inspect the model to ensure that its operations line up with what we expect from :numref:")),s[25]||(s[25]=i("code",null,"img_lenet_vert",-1)),s[26]||(s[26]=a("."))]),s[42]||(s[42]=n("",2)),i("p",null,[s[33]||(s[33]=a("Note that the height and width of the representation at each layer throughout the convolutional block is reduced (compared with the previous layer). The first convolutional layer uses two pixels of padding to compensate for the reduction in height and width that would otherwise result from using a ")),i("mjx-container",m,[(t(),l("svg",F,s[27]||(s[27]=[n("",1)]))),s[28]||(s[28]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"5"),i("mo",null,"×"),i("mn",null,"5")])],-1))]),s[34]||(s[34]=a(" kernel. As an aside, the image size of ")),i("mjx-container",f,[(t(),l("svg",L,s[29]||(s[29]=[n("",1)]))),s[30]||(s[30]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"28"),i("mo",null,"×"),i("mn",null,"28")])],-1))]),s[35]||(s[35]=a(" pixels in the original MNIST OCR dataset is a result of ")),s[36]||(s[36]=i("em",null,"trimming",-1)),s[37]||(s[37]=a(" two pixel rows (and columns) from the original scans that measured ")),i("mjx-container",w,[(t(),l("svg",A,s[31]||(s[31]=[n("",1)]))),s[32]||(s[32]=i("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[i("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[i("mn",null,"32"),i("mo",null,"×"),i("mn",null,"32")])],-1))]),s[38]||(s[38]=a(" pixels. This was done primarily to save space (a 30% reduction) at a time when megabytes mattered."))]),s[43]||(s[43]=n("",22))])}const D=e(h,[["render",N]]);export{O as __pageData,D as default};
