<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Long Short-Term Memory (LSTM) · d2l Julia</title><meta name="title" content="Long Short-Term Memory (LSTM) · d2l Julia"/><meta property="og:title" content="Long Short-Term Memory (LSTM) · d2l Julia"/><meta property="twitter:title" content="Long Short-Term Memory (LSTM) · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_3/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_5/">-</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../../CH7.ModernConvolutionalNeuralNetworks/MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li class="is-active"><a class="tocitem" href>Long Short-Term Memory (LSTM)</a><ul class="internal"><li><a class="tocitem" href="#Gated-Memory-Cell"><span>Gated Memory Cell</span></a></li><li><a class="tocitem" href="#Implementation-from-Scratch"><span>Implementation from Scratch</span></a></li><li><a class="tocitem" href="#Concise-Implementation"><span>Concise Implementation</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../MRNN_3/">-</a></li><li><a class="tocitem" href="../MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Modern Recurrent Neural Networks</a></li><li class="is-active"><a href>Long Short-Term Memory (LSTM)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Long Short-Term Memory (LSTM)</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Long-Short-Term-Memory-(LSTM)"><a class="docs-heading-anchor" href="#Long-Short-Term-Memory-(LSTM)">Long Short-Term Memory (LSTM)</a><a id="Long-Short-Term-Memory-(LSTM)-1"></a><a class="docs-heading-anchor-permalink" href="#Long-Short-Term-Memory-(LSTM)" title="Permalink"></a></h1><p>:label:<code>sec_lstm</code></p><p>Shortly after the first Elman-style RNNs were trained using backpropagation  :cite:<code>elman1990finding</code>, the problems of learning long-term dependencies (owing to vanishing and exploding gradients) became salient, with Bengio and Hochreiter  discussing the problem :cite:<code>bengio1994learning,Hochreiter.Bengio.Frasconi.ea.2001</code>. Hochreiter had articulated this problem as early  as 1991 in his Master&#39;s thesis, although the results  were not widely known because the thesis was written in German. While gradient clipping helps with exploding gradients,  handling vanishing gradients appears  to require a more elaborate solution.  One of the first and most successful techniques  for addressing vanishing gradients  came in the form of the long short-term memory (LSTM) model  due to :citet:<code>Hochreiter.Schmidhuber.1997</code>.  LSTMs resemble standard recurrent neural networks  but here each ordinary recurrent node is replaced by a <em>memory cell</em>. Each memory cell contains an <em>internal state</em>, i.e., a node with a self-connected recurrent edge of fixed weight 1, ensuring that the gradient can pass across many time steps  without vanishing or exploding.</p><p>The term &quot;long short-term memory&quot; comes from the following intuition. Simple recurrent neural networks  have <em>long-term memory</em> in the form of weights. The weights change slowly during training,  encoding general knowledge about the data. They also have <em>short-term memory</em> in the form of ephemeral activations, which pass from each node to successive nodes. The LSTM model introduces an intermediate type of storage via the memory cell. A memory cell is a composite unit,  built from simpler nodes  in a specific connectivity pattern, with the novel inclusion of multiplicative nodes.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Flux 
using Downloads
using StatsBase
using Plots
using CUDA, cuDNN</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/d2l-julia/d2lai`</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Precompiling d2lai [749b8817-cd67-416c-8a57-830ea19f3cc4] (cache misses: include_dependency fsize change (2))</pre>
</div><h2 id="Gated-Memory-Cell"><a class="docs-heading-anchor" href="#Gated-Memory-Cell">Gated Memory Cell</a><a id="Gated-Memory-Cell-1"></a><a class="docs-heading-anchor-permalink" href="#Gated-Memory-Cell" title="Permalink"></a></h2><p>Each memory cell is equipped with an <em>internal state</em> and a number of multiplicative gates that determine whether (i) a given input should impact the internal state (the <em>input gate</em>), (ii) the internal state should be flushed to <span>$0$</span> (the <em>forget gate</em>), and (iii) the internal state of a given neuron  should be allowed to impact the cell&#39;s output (the <em>output</em> gate). </p><h3 id="Gated-Hidden-State"><a class="docs-heading-anchor" href="#Gated-Hidden-State">Gated Hidden State</a><a id="Gated-Hidden-State-1"></a><a class="docs-heading-anchor-permalink" href="#Gated-Hidden-State" title="Permalink"></a></h3><p>The key distinction between vanilla RNNs and LSTMs is that the latter support gating of the hidden state. This means that we have dedicated mechanisms for when a hidden state should be <em>updated</em> and also for when it should be <em>reset</em>. These mechanisms are learned and they address the concerns listed above. For instance, if the first token is of great importance we will learn not to update the hidden state after the first observation. Likewise, we will learn to skip irrelevant temporary observations. Last, we will learn to reset the latent state whenever needed. We discuss this in detail below.</p><h3 id="Input-Gate,-Forget-Gate,-and-Output-Gate"><a class="docs-heading-anchor" href="#Input-Gate,-Forget-Gate,-and-Output-Gate">Input Gate, Forget Gate, and Output Gate</a><a id="Input-Gate,-Forget-Gate,-and-Output-Gate-1"></a><a class="docs-heading-anchor-permalink" href="#Input-Gate,-Forget-Gate,-and-Output-Gate" title="Permalink"></a></h3><p>The data feeding into the LSTM gates are the input at the current time step and the hidden state of the previous time step, as illustrated in :numref:<code>fig_lstm_0</code>. Three fully connected layers with sigmoid activation functions compute the values of the input, forget, and output gates. As a result of the sigmoid activation, all values of the three gates are in the range of <span>$(0, 1)$</span>. Additionally, we require an <em>input node</em>, typically computed with a <em>tanh</em> activation function.  Intuitively, the <em>input gate</em> determines how much of the input node&#39;s value should be added  to the current memory cell internal state. The <em>forget gate</em> determines whether to keep the current value of the memory or flush it.  And the <em>output gate</em> determines whether  the memory cell should influence the output at the current time step. </p><p><img src="../../img/lstm-0.svg" alt="Computing the input gate, the forget gate, and the output gate in an LSTM model."/> :label:<code>fig_lstm_0</code></p><p>Mathematically, suppose that there are <span>$h$</span> hidden units,  the batch size is <span>$n$</span>, and the number of inputs is <span>$d$</span>. Thus, the input is <span>$\mathbf{X}_t \in \mathbb{R}^{n \times d}$</span>  and the hidden state of the previous time step  is <span>$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$</span>.  Correspondingly, the gates at time step <span>$t$</span> are defined as follows: the input gate is <span>$\mathbf{I}_t \in \mathbb{R}^{n \times h}$</span>,  the forget gate is <span>$\mathbf{F}_t \in \mathbb{R}^{n \times h}$</span>,  and the output gate is <span>$\mathbf{O}_t \in \mathbb{R}^{n \times h}$</span>.  They are calculated as follows:</p><p class="math-container">\[
\begin{aligned}
\mathbf{I}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xi}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hi}} + \mathbf{b}_\textrm{i}),\\
\mathbf{F}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xf}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hf}} + \mathbf{b}_\textrm{f}),\\
\mathbf{O}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xo}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{ho}} + \mathbf{b}_\textrm{o}),
\end{aligned}
$$

where $\mathbf{W}_{\textrm{xi}}, \mathbf{W}_{\textrm{xf}}, \mathbf{W}_{\textrm{xo}} \in \mathbb{R}^{d \times h}$ and $\mathbf{W}_{\textrm{hi}}, \mathbf{W}_{\textrm{hf}}, \mathbf{W}_{\textrm{ho}} \in \mathbb{R}^{h \times h}$ are weight parameters 
and $\mathbf{b}_\textrm{i}, \mathbf{b}_\textrm{f}, \mathbf{b}_\textrm{o} \in \mathbb{R}^{1 \times h}$ are bias parameters.
Note that broadcasting 
(see :numref:`subsec_broadcasting`)
is triggered during the summation.
We use sigmoid functions 
(as introduced in :numref:`sec_mlp`) 
to map the input values to the interval $(0, 1)$.


### Input Node

Next we design the memory cell. 
Since we have not specified the action of the various gates yet, 
we first introduce the *input node* 
$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$.
Its computation is similar to that of the three gates described above, 
but uses a $\tanh$ function with a value range for $(-1, 1)$ as the activation function. 
This leads to the following equation at time step $t$:

$$\tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c}),\]</p><p>where <span>$\mathbf{W}_{\textrm{xc}} \in \mathbb{R}^{d \times h}$</span> and <span>$\mathbf{W}_{\textrm{hc}} \in \mathbb{R}^{h \times h}$</span> are weight parameters and <span>$\mathbf{b}_\textrm{c} \in \mathbb{R}^{1 \times h}$</span> is a bias parameter.</p><p>A quick illustration of the input node is shown in :numref:<code>fig_lstm_1</code>.</p><p><img src="../../img/lstm-1.svg" alt="Computing the input node in an LSTM model."/> :label:<code>fig_lstm_1</code></p><h3 id="Memory-Cell-Internal-State"><a class="docs-heading-anchor" href="#Memory-Cell-Internal-State">Memory Cell Internal State</a><a id="Memory-Cell-Internal-State-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-Cell-Internal-State" title="Permalink"></a></h3><p>In LSTMs, the input gate <span>$\mathbf{I}_t$</span> governs  how much we take new data into account via <span>$\tilde{\mathbf{C}}_t$</span>  and the forget gate <span>$\mathbf{F}_t$</span> addresses  how much of the old cell internal state <span>$\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$</span> we retain.  Using the Hadamard (elementwise) product operator <span>$\odot$</span> we arrive at the following update equation:</p><p class="math-container">\[\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.\]</p><p>If the forget gate is always 1 and the input gate is always 0,  the memory cell internal state <span>$\mathbf{C}_{t-1}$</span> will remain constant forever,  passing unchanged to each subsequent time step. However, input gates and forget gates give the model the flexibility of being able to learn  when to keep this value unchanged and when to perturb it in response  to subsequent inputs.  In practice, this design alleviates the vanishing gradient problem, resulting in models that are much easier to train, especially when facing datasets with long sequence lengths. </p><p>We thus arrive at the flow diagram in :numref:<code>fig_lstm_2</code>.</p><p><img src="../../img/lstm-2.svg" alt="Computing the memory cell internal state in an LSTM model."/></p><p>:label:<code>fig_lstm_2</code></p><h3 id="Hidden-State"><a class="docs-heading-anchor" href="#Hidden-State">Hidden State</a><a id="Hidden-State-1"></a><a class="docs-heading-anchor-permalink" href="#Hidden-State" title="Permalink"></a></h3><p>Last, we need to define how to compute the output of the memory cell, i.e., the hidden state <span>$\mathbf{H}_t \in \mathbb{R}^{n \times h}$</span>, as seen by other layers.  This is where the output gate comes into play. In LSTMs, we first apply <span>$\tanh$</span> to the memory cell internal state and then apply another point-wise multiplication, this time with the output gate. This ensures that the values of <span>$\mathbf{H}_t$</span>  are always in the interval <span>$(-1, 1)$</span>:</p><p class="math-container">\[\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).\]</p><p>Whenever the output gate is close to 1,  we allow the memory cell internal state to impact the subsequent layers uninhibited, whereas for output gate values close to 0, we prevent the current memory from impacting other layers of the network at the current time step.  Note that a memory cell can accrue information  across many time steps without impacting the rest of the network (as long as the output gate takes values close to 0), and then suddenly impact the network at a subsequent time step as soon as the output gate flips from values close to 0 to values close to 1. :numref:<code>fig_lstm_3</code> has a graphical illustration of the data flow.</p><p><img src="../../img/lstm-3.svg" alt="Computing the hidden state in an LSTM model."/> :label:<code>fig_lstm_3</code></p><h2 id="Implementation-from-Scratch"><a class="docs-heading-anchor" href="#Implementation-from-Scratch">Implementation from Scratch</a><a id="Implementation-from-Scratch-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-from-Scratch" title="Permalink"></a></h2><p>Now let&#39;s implement an LSTM from scratch. As same as the experiments in :numref:<code>sec_rnn-scratch</code>, we first load <em>The Time Machine</em> dataset.</p><h3 id="Initializing-Model-Parameters"><a class="docs-heading-anchor" href="#Initializing-Model-Parameters">Initializing Model Parameters</a><a id="Initializing-Model-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Initializing-Model-Parameters" title="Permalink"></a></h3><p>Next, we need to define and initialize the model parameters.  As previously, the hyperparameter <code>num_hiddens</code>  dictates the number of hidden units. We initialize weights following a Gaussian distribution with 0.01 standard deviation,  and we set the biases to 0.</p><pre><code class="language-julia hljs">struct LSTMScratch{W, A} &lt;: AbstractModel 
    w::W
    args::A
end 
Flux.@layer LSTMScratch trainable = (w,)</code></pre><pre><code class="language-julia hljs">function LSTMScratch(num_inputs::Int, num_hiddens::Int; sigma = 0.1)
    init_weights() = randn(num_hiddens, num_inputs).*sigma, randn(num_hiddens, num_hiddens).*sigma, zeros(num_hiddens)
    W_ix, W_ih, b_i = init_weights() # input gate
    W_fx, W_fh, b_f = init_weights() # forget gate
    W_cx, W_ch, b_c = init_weights() # input node
    W_ox, W_oh, b_o = init_weights()

    w = (input_gate = d2lai.construct_nt_args(;W_ix, W_ih, b_i),
         forget_gate = d2lai.construct_nt_args(; W_fx, W_fh, b_f),
         input_node = d2lai.construct_nt_args(;W_cx, W_ch, b_c),
         output_gate = d2lai.construct_nt_args(;W_ox, W_oh, b_o)
    )
    args = d2lai.construct_nt_args(; num_inputs, num_hiddens, sigma)
    LSTMScratch(w, args)
end</code></pre><pre><code class="nohighlight hljs">LSTMScratch</code></pre><pre><code class="language-julia hljs">function (m::LSTMScratch)(x, state = nothing)
    batchsize = size(x, 3)
    device = isa(x, CuArray) ? gpu : cpu 
    H, C = if isnothing(state)
        zeros(m.args.num_hiddens, batchsize), zeros(m.args.num_hiddens, batchsize) 
    else
        state
    end |&gt; device

    outputs = map(eachslice(x; dims = 2)) do x_ 
        It = sigmoid.(m.w.input_gate.W_ix*x_ + m.w.input_gate.W_ih*H .+ m.w.input_gate.b_i)
        Ft = sigmoid.(m.w.forget_gate.W_fx*x_ + m.w.forget_gate.W_fh*H .+ m.w.forget_gate.b_f)
        Ot = sigmoid.(m.w.output_gate.W_ox*x_ + m.w.output_gate.W_oh*H .+ m.w.output_gate.b_o)
        C_tilde = tanh.(m.w.input_node.W_cx*x_ + m.w.input_node.W_ch*H .+ m.w.input_node.b_c)
        C = Ft.*C + It.*C_tilde
        H = Ot.*C
        return H
    end
    outputs = stack(outputs)
    permutedims(outputs, [1,3,2]), (H,C)
end</code></pre><h3 id="Training-and-Prediction"><a class="docs-heading-anchor" href="#Training-and-Prediction">Training and Prediction</a><a id="Training-and-Prediction-1"></a><a class="docs-heading-anchor-permalink" href="#Training-and-Prediction" title="Permalink"></a></h3><p>Let&#39;s train an LSTM model by instantiating the <code>RNNLMScratch</code> class from :numref:<code>sec_rnn-scratch</code>.</p><pre><code class="language-julia hljs">data = d2lai.TimeMachine(1024, 32) |&gt; f64
num_hiddens = 32
lstm = LSTMScratch(length(data.vocab), num_hiddens)
model = RNNLMScratch(lstm, length(data.vocab)) |&gt; f64</code></pre><pre><code class="nohighlight hljs">RNNLMScratch(
  LSTMScratch(
    (W_ix = [0.08747171542425301 -0.17341179478720958 … -0.06726800252203806 0.09367294373942317; -0.1452824044577879 0.12619551779995467 … -0.09895119556199267 0.010863879061412557; … ; -0.04473087731072691 -0.09172165627921565 … -0.022918508423614457 0.12640761448798024; 0.044891806895076894 0.1253085614641499 … 0.13964047920846456 0.04759282998431221], W_ih = [0.00198311320890812 0.06285866923421234 … 0.1181754137313053 0.022546301070249433; -0.11826559391442358 -0.02407909507971443 … 0.10119098908374068 -0.09650097487588047; … ; -0.1636589338274944 0.17372744913318205 … 0.05804939516927729 -0.017811349706691525; -0.055622201558758556 -0.02674223288341316 … -0.012510227028076538 -0.07420678957704421], b_i = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),  # 1_952 parameters
    (W_fx = [-0.03990545908452857 -0.067809551088846 … -0.10866316208637067 0.09450208536139496; 0.104632485712098 -0.0713691863742404 … 0.1776967778508824 -0.08515016488721067; … ; -0.032020427141648176 0.07252298887271223 … -0.038950676291590644 -0.09071531169374465; -0.14312767584185845 0.03182870549135677 … 0.2484052388570822 0.02838694309947931], W_fh = [-0.1542728723745034 -0.08137738201212526 … 0.13508575195738584 0.04435403279988345; 0.0926025052358465 0.2115985738567155 … 0.041519734356975056 -0.048142429347691744; … ; -0.021665807358067564 0.284003578361186 … -0.04007965960538923 -0.0943554101580606; 0.10265102692019597 0.062394240951549586 … 0.0037949800148378283 0.06891733867484122], b_f = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),  # 1_952 parameters
    (W_cx = [-0.057746973035697396 0.049473263701352875 … 0.14266931815401726 -0.12835027766770862; -0.15853591183556465 0.11142973276125619 … -0.0514186117703089 -0.04167830752986392; … ; 0.03802113801933415 -0.16798656201263984 … -0.05775810377740525 0.07803486512754508; 0.0007221079892858898 0.11214275798990739 … 0.020792252262238735 0.016063651387834856], W_ch = [-0.010305367639121033 -0.026382694315963292 … 0.021271328314904577 -0.17804910157928017; 0.10761429933258931 0.0047901517084850105 … -0.05323488569625752 -0.025607971816758474; … ; 0.09385819398844707 0.07621759848114866 … 0.02868831050757416 0.09367484361681064; 0.01416586475005763 0.05216876563726765 … -0.16881776954360384 -0.04751474790122834], b_c = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),  # 1_952 parameters
    (W_ox = [-0.04526375021551397 -0.06648484271920702 … 0.10107594549007899 -0.1152532728881317; -0.18353714110467184 -0.027480361116801202 … -0.0998697352851585 -0.10317289653443074; … ; -0.006446491123335958 -0.08940384910640532 … 0.0316224204708325 0.20238208121579998; -0.09458060135995505 0.06142514360853934 … -0.08026848914011071 0.08207326271481763], W_oh = [-0.08824522502870512 0.05201644901708602 … -0.13259675059260456 -0.07490408118605714; 0.09243762383911425 -0.0026307008373316855 … 0.10233276493803499 0.08652392808485837; … ; -0.06646381760877561 -0.09401510737760418 … -0.10332148228115944 -0.10370990372253414; -0.09358430741237717 -0.14512489397013742 … 0.09465269108418406 0.2752306069916312], b_o = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),  # 1_952 parameters
  ),
  28×32 Matrix{Float64},                # 896 parameters
  28-element Vector{Float64},           # 28 parameters  (all zero)
)                   # Total: 14 arrays, 8_732 parameters, 68.977 KiB.</code></pre><pre><code class="language-julia hljs">opt = Descent(1.)
trainer = Trainer(model, data, opt; max_epochs = 100, gpu = true, board_yscale = :identity, gradient_clip_val = 1.)
m = d2lai.fit(trainer);</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 2.8577464, Val Loss: 2.8570354
    [ Info: Train Loss: 2.789548, Val Loss: 2.7877047
    [ Info: Train Loss: 2.688144, Val Loss: 2.6732988
    [ Info: Train Loss: 2.5377173, Val Loss: 2.537118
    [ Info: Train Loss: 2.4538963, Val Loss: 2.4457486
    [ Info: Train Loss: 2.3772397, Val Loss: 2.3949955
    [ Info: Train Loss: 2.3360584, Val Loss: 2.3654156
    [ Info: Train Loss: 2.2909172, Val Loss: 2.3251545
    [ Info: Train Loss: 2.2559114, Val Loss: 2.3191686
    [ Info: Train Loss: 2.2344718, Val Loss: 2.2964644
    [ Info: Train Loss: 2.1931727, Val Loss: 2.2701557
    [ Info: Train Loss: 2.168284, Val Loss: 2.2416053
    [ Info: Train Loss: 2.1241865, Val Loss: 2.224526
    [ Info: Train Loss: 2.1009102, Val Loss: 2.1999269
    [ Info: Train Loss: 2.0752017, Val Loss: 2.1708565
    [ Info: Train Loss: 2.0330355, Val Loss: 2.1414955
    [ Info: Train Loss: 2.0177925, Val Loss: 2.1233914
    [ Info: Train Loss: 1.9999425, Val Loss: 2.1197312
    [ Info: Train Loss: 1.9709194, Val Loss: 2.0945277
    [ Info: Train Loss: 1.9374853, Val Loss: 2.094332
    [ Info: Train Loss: 1.9336435, Val Loss: 2.0630207
    [ Info: Train Loss: 1.91804, Val Loss: 2.0741513
    [ Info: Train Loss: 1.9016124, Val Loss: 2.031097
    [ Info: Train Loss: 1.8670052, Val Loss: 2.032497
    [ Info: Train Loss: 1.8544912, Val Loss: 2.0302908
    [ Info: Train Loss: 1.831575, Val Loss: 2.0121994
    [ Info: Train Loss: 1.8039266, Val Loss: 2.0143247
    [ Info: Train Loss: 1.7944635, Val Loss: 2.003396
    [ Info: Train Loss: 1.7696036, Val Loss: 1.9874908
    [ Info: Train Loss: 1.751212, Val Loss: 1.9674721
    [ Info: Train Loss: 1.7603586, Val Loss: 1.9571958
    [ Info: Train Loss: 1.7435517, Val Loss: 1.9905457
    [ Info: Train Loss: 1.7058074, Val Loss: 1.961815
    [ Info: Train Loss: 1.7169164, Val Loss: 1.9375858
    [ Info: Train Loss: 1.6705436, Val Loss: 1.9560782
    [ Info: Train Loss: 1.679861, Val Loss: 1.9562875
    [ Info: Train Loss: 1.6392078, Val Loss: 1.9134141
    [ Info: Train Loss: 1.635678, Val Loss: 1.9301218
    [ Info: Train Loss: 1.6286591, Val Loss: 1.9856387
    [ Info: Train Loss: 1.6013554, Val Loss: 1.9378722
    [ Info: Train Loss: 1.617093, Val Loss: 1.9436615
    [ Info: Train Loss: 1.5867039, Val Loss: 1.9401144
    [ Info: Train Loss: 1.5967989, Val Loss: 1.9242941
    [ Info: Train Loss: 1.5880837, Val Loss: 1.9108937
    [ Info: Train Loss: 1.5574433, Val Loss: 1.9368172
    [ Info: Train Loss: 1.5503292, Val Loss: 1.9265687
    [ Info: Train Loss: 1.5365485, Val Loss: 1.9561881
    [ Info: Train Loss: 1.533664, Val Loss: 1.966977
    [ Info: Train Loss: 1.5209395, Val Loss: 1.9422178
    [ Info: Train Loss: 1.5454886, Val Loss: 1.9531256
    [ Info: Train Loss: 1.513706, Val Loss: 1.9142479
    [ Info: Train Loss: 1.5376143, Val Loss: 1.8976619
    [ Info: Train Loss: 1.5177532, Val Loss: 1.9576881
    [ Info: Train Loss: 1.4931737, Val Loss: 1.9219521
    [ Info: Train Loss: 1.4751806, Val Loss: 1.950009
    [ Info: Train Loss: 1.4812049, Val Loss: 1.9766624
    [ Info: Train Loss: 1.4989445, Val Loss: 1.9336927
    [ Info: Train Loss: 1.4475642, Val Loss: 1.9463936
    [ Info: Train Loss: 1.4587109, Val Loss: 1.9254599
    [ Info: Train Loss: 1.441813, Val Loss: 1.9885918
    [ Info: Train Loss: 1.4468588, Val Loss: 1.9261206
    [ Info: Train Loss: 1.4331188, Val Loss: 1.9755284
    [ Info: Train Loss: 1.4292498, Val Loss: 1.9617558
    [ Info: Train Loss: 1.4354277, Val Loss: 1.9441824
    [ Info: Train Loss: 1.4217302, Val Loss: 2.00037
    [ Info: Train Loss: 1.434056, Val Loss: 1.9373636
    [ Info: Train Loss: 1.4313787, Val Loss: 1.9979906
    [ Info: Train Loss: 1.3996192, Val Loss: 1.9893837
    [ Info: Train Loss: 1.3944763, Val Loss: 1.9426534
    [ Info: Train Loss: 1.404267, Val Loss: 2.0085268
    [ Info: Train Loss: 1.4112734, Val Loss: 1.9628971
    [ Info: Train Loss: 1.4130398, Val Loss: 1.9723268
    [ Info: Train Loss: 1.4022796, Val Loss: 1.9902713
    [ Info: Train Loss: 1.3957345, Val Loss: 1.9562278
    [ Info: Train Loss: 1.3724736, Val Loss: 1.9985989
    [ Info: Train Loss: 1.3800858, Val Loss: 1.9412769
    [ Info: Train Loss: 1.3721211, Val Loss: 1.9895244
    [ Info: Train Loss: 1.3690766, Val Loss: 1.9767517
    [ Info: Train Loss: 1.362259, Val Loss: 1.9564062
    [ Info: Train Loss: 1.3640676, Val Loss: 2.000539
    [ Info: Train Loss: 1.3771462, Val Loss: 2.002624
    [ Info: Train Loss: 1.357882, Val Loss: 1.9988647
    [ Info: Train Loss: 1.3579962, Val Loss: 2.026906
    [ Info: Train Loss: 1.3600618, Val Loss: 2.0099378
    [ Info: Train Loss: 1.3557875, Val Loss: 1.9709011
    [ Info: Train Loss: 1.3204275, Val Loss: 1.9751427
    [ Info: Train Loss: 1.3581452, Val Loss: 2.0131629
    [ Info: Train Loss: 1.3186963, Val Loss: 1.985362
    [ Info: Train Loss: 1.333647, Val Loss: 1.9873333
    [ Info: Train Loss: 1.3187902, Val Loss: 2.0392253
    [ Info: Train Loss: 1.322236, Val Loss: 2.0185454
    [ Info: Train Loss: 1.3284179, Val Loss: 2.0056393
    [ Info: Train Loss: 1.3288062, Val Loss: 2.0357358
    [ Info: Train Loss: 1.3189505, Val Loss: 2.0276403
    [ Info: Train Loss: 1.3097466, Val Loss: 2.020081
    [ Info: Train Loss: 1.3055674, Val Loss: 2.0383105
    [ Info: Train Loss: 1.3121166, Val Loss: 2.0296302
    [ Info: Train Loss: 1.3159087, Val Loss: 2.0466354
    [ Info: Train Loss: 1.2757726, Val Loss: 2.0666091
    [ Info: Train Loss: 1.2954105, Val Loss: 2.0443974</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip790">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip790)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip791">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip790)" d="M138.867 1423.18 L2352.76 1423.18 L2352.76 47.2441 L138.867 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip792">
    <rect x="138" y="47" width="2215" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="180.427,1423.18 180.427,47.2441 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="707.845,1423.18 707.845,47.2441 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1235.26,1423.18 1235.26,47.2441 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1762.68,1423.18 1762.68,47.2441 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2290.1,1423.18 2290.1,47.2441 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,1181.09 2352.76,1181.09 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,920.045 2352.76,920.045 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,659.006 2352.76,659.006 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,397.966 2352.76,397.966 "/>
<polyline clip-path="url(#clip792)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,136.926 2352.76,136.926 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="180.427,1423.18 180.427,1404.28 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="707.845,1423.18 707.845,1404.28 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1235.26,1423.18 1235.26,1404.28 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1762.68,1423.18 1762.68,1404.28 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2290.1,1423.18 2290.1,1404.28 "/>
<path clip-path="url(#clip790)" d="M180.427 1454.1 Q176.816 1454.1 174.988 1457.66 Q173.182 1461.2 173.182 1468.33 Q173.182 1475.44 174.988 1479.01 Q176.816 1482.55 180.427 1482.55 Q184.062 1482.55 185.867 1479.01 Q187.696 1475.44 187.696 1468.33 Q187.696 1461.2 185.867 1457.66 Q184.062 1454.1 180.427 1454.1 M180.427 1450.39 Q186.238 1450.39 189.293 1455 Q192.372 1459.58 192.372 1468.33 Q192.372 1477.06 189.293 1481.67 Q186.238 1486.25 180.427 1486.25 Q174.617 1486.25 171.539 1481.67 Q168.483 1477.06 168.483 1468.33 Q168.483 1459.58 171.539 1455 Q174.617 1450.39 180.427 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M687.116 1481.64 L703.435 1481.64 L703.435 1485.58 L681.491 1485.58 L681.491 1481.64 Q684.153 1478.89 688.737 1474.26 Q693.343 1469.61 694.524 1468.27 Q696.769 1465.74 697.648 1464.01 Q698.551 1462.25 698.551 1460.56 Q698.551 1457.8 696.607 1456.07 Q694.686 1454.33 691.584 1454.33 Q689.385 1454.33 686.931 1455.09 Q684.5 1455.86 681.723 1457.41 L681.723 1452.69 Q684.547 1451.55 687 1450.97 Q689.454 1450.39 691.491 1450.39 Q696.861 1450.39 700.056 1453.08 Q703.25 1455.77 703.25 1460.26 Q703.25 1462.39 702.44 1464.31 Q701.653 1466.2 699.547 1468.8 Q698.968 1469.47 695.866 1472.69 Q692.764 1475.88 687.116 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M713.297 1451.02 L731.653 1451.02 L731.653 1454.96 L717.579 1454.96 L717.579 1463.43 Q718.597 1463.08 719.616 1462.92 Q720.634 1462.73 721.653 1462.73 Q727.44 1462.73 730.82 1465.9 Q734.199 1469.08 734.199 1474.49 Q734.199 1480.07 730.727 1483.17 Q727.255 1486.25 720.935 1486.25 Q718.759 1486.25 716.491 1485.88 Q714.246 1485.51 711.838 1484.77 L711.838 1480.07 Q713.922 1481.2 716.144 1481.76 Q718.366 1482.32 720.843 1482.32 Q724.847 1482.32 727.185 1480.21 Q729.523 1478.1 729.523 1474.49 Q729.523 1470.88 727.185 1468.77 Q724.847 1466.67 720.843 1466.67 Q718.968 1466.67 717.093 1467.08 Q715.241 1467.5 713.297 1468.38 L713.297 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1209.96 1451.02 L1228.32 1451.02 L1228.32 1454.96 L1214.24 1454.96 L1214.24 1463.43 Q1215.26 1463.08 1216.28 1462.92 Q1217.3 1462.73 1218.32 1462.73 Q1224.11 1462.73 1227.49 1465.9 Q1230.86 1469.08 1230.86 1474.49 Q1230.86 1480.07 1227.39 1483.17 Q1223.92 1486.25 1217.6 1486.25 Q1215.43 1486.25 1213.16 1485.88 Q1210.91 1485.51 1208.5 1484.77 L1208.5 1480.07 Q1210.59 1481.2 1212.81 1481.76 Q1215.03 1482.32 1217.51 1482.32 Q1221.51 1482.32 1223.85 1480.21 Q1226.19 1478.1 1226.19 1474.49 Q1226.19 1470.88 1223.85 1468.77 Q1221.51 1466.67 1217.51 1466.67 Q1215.63 1466.67 1213.76 1467.08 Q1211.91 1467.5 1209.96 1468.38 L1209.96 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1250.08 1454.1 Q1246.47 1454.1 1244.64 1457.66 Q1242.83 1461.2 1242.83 1468.33 Q1242.83 1475.44 1244.64 1479.01 Q1246.47 1482.55 1250.08 1482.55 Q1253.71 1482.55 1255.52 1479.01 Q1257.35 1475.44 1257.35 1468.33 Q1257.35 1461.2 1255.52 1457.66 Q1253.71 1454.1 1250.08 1454.1 M1250.08 1450.39 Q1255.89 1450.39 1258.94 1455 Q1262.02 1459.58 1262.02 1468.33 Q1262.02 1477.06 1258.94 1481.67 Q1255.89 1486.25 1250.08 1486.25 Q1244.27 1486.25 1241.19 1481.67 Q1238.13 1477.06 1238.13 1468.33 Q1238.13 1459.58 1241.19 1455 Q1244.27 1450.39 1250.08 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1736.54 1451.02 L1758.76 1451.02 L1758.76 1453.01 L1746.21 1485.58 L1741.33 1485.58 L1753.13 1454.96 L1736.54 1454.96 L1736.54 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1767.92 1451.02 L1786.28 1451.02 L1786.28 1454.96 L1772.21 1454.96 L1772.21 1463.43 Q1773.22 1463.08 1774.24 1462.92 Q1775.26 1462.73 1776.28 1462.73 Q1782.07 1462.73 1785.45 1465.9 Q1788.83 1469.08 1788.83 1474.49 Q1788.83 1480.07 1785.35 1483.17 Q1781.88 1486.25 1775.56 1486.25 Q1773.39 1486.25 1771.12 1485.88 Q1768.87 1485.51 1766.47 1484.77 L1766.47 1480.07 Q1768.55 1481.2 1770.77 1481.76 Q1772.99 1482.32 1775.47 1482.32 Q1779.47 1482.32 1781.81 1480.21 Q1784.15 1478.1 1784.15 1474.49 Q1784.15 1470.88 1781.81 1468.77 Q1779.47 1466.67 1775.47 1466.67 Q1773.6 1466.67 1771.72 1467.08 Q1769.87 1467.5 1767.92 1468.38 L1767.92 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2249.71 1481.64 L2257.34 1481.64 L2257.34 1455.28 L2249.03 1456.95 L2249.03 1452.69 L2257.3 1451.02 L2261.97 1451.02 L2261.97 1481.64 L2269.61 1481.64 L2269.61 1485.58 L2249.71 1485.58 L2249.71 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2289.06 1454.1 Q2285.45 1454.1 2283.62 1457.66 Q2281.81 1461.2 2281.81 1468.33 Q2281.81 1475.44 2283.62 1479.01 Q2285.45 1482.55 2289.06 1482.55 Q2292.69 1482.55 2294.5 1479.01 Q2296.33 1475.44 2296.33 1468.33 Q2296.33 1461.2 2294.5 1457.66 Q2292.69 1454.1 2289.06 1454.1 M2289.06 1450.39 Q2294.87 1450.39 2297.92 1455 Q2301 1459.58 2301 1468.33 Q2301 1477.06 2297.92 1481.67 Q2294.87 1486.25 2289.06 1486.25 Q2283.25 1486.25 2280.17 1481.67 Q2277.11 1477.06 2277.11 1468.33 Q2277.11 1459.58 2280.17 1455 Q2283.25 1450.39 2289.06 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2319.22 1454.1 Q2315.61 1454.1 2313.78 1457.66 Q2311.97 1461.2 2311.97 1468.33 Q2311.97 1475.44 2313.78 1479.01 Q2315.61 1482.55 2319.22 1482.55 Q2322.85 1482.55 2324.66 1479.01 Q2326.49 1475.44 2326.49 1468.33 Q2326.49 1461.2 2324.66 1457.66 Q2322.85 1454.1 2319.22 1454.1 M2319.22 1450.39 Q2325.03 1450.39 2328.08 1455 Q2331.16 1459.58 2331.16 1468.33 Q2331.16 1477.06 2328.08 1481.67 Q2325.03 1486.25 2319.22 1486.25 Q2313.41 1486.25 2310.33 1481.67 Q2307.27 1477.06 2307.27 1468.33 Q2307.27 1459.58 2310.33 1455 Q2313.41 1450.39 2319.22 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1166 1548.76 L1166 1551.62 L1139.07 1551.62 Q1139.46 1557.67 1142.7 1560.85 Q1145.98 1564 1151.81 1564 Q1155.18 1564 1158.33 1563.17 Q1161.51 1562.35 1164.63 1560.69 L1164.63 1566.23 Q1161.48 1567.57 1158.17 1568.27 Q1154.86 1568.97 1151.46 1568.97 Q1142.93 1568.97 1137.93 1564 Q1132.96 1559.04 1132.96 1550.57 Q1132.96 1541.82 1137.67 1536.69 Q1142.42 1531.54 1150.44 1531.54 Q1157.63 1531.54 1161.8 1536.18 Q1166 1540.8 1166 1548.76 M1160.14 1547.04 Q1160.08 1542.23 1157.44 1539.37 Q1154.83 1536.5 1150.5 1536.5 Q1145.6 1536.5 1142.64 1539.27 Q1139.71 1542.04 1139.27 1547.07 L1160.14 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1181.28 1562.7 L1181.28 1581.6 L1175.39 1581.6 L1175.39 1532.4 L1181.28 1532.4 L1181.28 1537.81 Q1183.13 1534.62 1185.93 1533.1 Q1188.76 1531.54 1192.67 1531.54 Q1199.17 1531.54 1203.21 1536.69 Q1207.28 1541.85 1207.28 1550.25 Q1207.28 1558.65 1203.21 1563.81 Q1199.17 1568.97 1192.67 1568.97 Q1188.76 1568.97 1185.93 1567.44 Q1183.13 1565.88 1181.28 1562.7 M1201.2 1550.25 Q1201.2 1543.79 1198.53 1540.13 Q1195.89 1536.44 1191.24 1536.44 Q1186.59 1536.44 1183.92 1540.13 Q1181.28 1543.79 1181.28 1550.25 Q1181.28 1556.71 1183.92 1560.4 Q1186.59 1564.07 1191.24 1564.07 Q1195.89 1564.07 1198.53 1560.4 Q1201.2 1556.71 1201.2 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1230.8 1536.5 Q1226.09 1536.5 1223.36 1540.19 Q1220.62 1543.85 1220.62 1550.25 Q1220.62 1556.65 1223.32 1560.34 Q1226.06 1564 1230.8 1564 Q1235.48 1564 1238.22 1560.31 Q1240.96 1556.62 1240.96 1550.25 Q1240.96 1543.92 1238.22 1540.23 Q1235.48 1536.5 1230.8 1536.5 M1230.8 1531.54 Q1238.44 1531.54 1242.8 1536.5 Q1247.16 1541.47 1247.16 1550.25 Q1247.16 1559 1242.8 1564 Q1238.44 1568.97 1230.8 1568.97 Q1223.13 1568.97 1218.77 1564 Q1214.44 1559 1214.44 1550.25 Q1214.44 1541.47 1218.77 1536.5 Q1223.13 1531.54 1230.8 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1282.53 1533.76 L1282.53 1539.24 Q1280.04 1537.87 1277.53 1537.2 Q1275.05 1536.5 1272.5 1536.5 Q1266.8 1536.5 1263.65 1540.13 Q1260.5 1543.73 1260.5 1550.25 Q1260.5 1556.78 1263.65 1560.4 Q1266.8 1564 1272.5 1564 Q1275.05 1564 1277.53 1563.33 Q1280.04 1562.63 1282.53 1561.26 L1282.53 1566.68 Q1280.07 1567.82 1277.43 1568.39 Q1274.82 1568.97 1271.86 1568.97 Q1263.81 1568.97 1259.07 1563.91 Q1254.33 1558.85 1254.33 1550.25 Q1254.33 1541.53 1259.1 1536.53 Q1263.91 1531.54 1272.24 1531.54 Q1274.95 1531.54 1277.53 1532.11 Q1280.11 1532.65 1282.53 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1322.34 1546.53 L1322.34 1568.04 L1316.49 1568.04 L1316.49 1546.72 Q1316.49 1541.66 1314.51 1539.14 Q1312.54 1536.63 1308.59 1536.63 Q1303.85 1536.63 1301.11 1539.65 Q1298.38 1542.68 1298.38 1547.9 L1298.38 1568.04 L1292.49 1568.04 L1292.49 1518.52 L1298.38 1518.52 L1298.38 1537.93 Q1300.48 1534.72 1303.31 1533.13 Q1306.17 1531.54 1309.9 1531.54 Q1316.04 1531.54 1319.19 1535.36 Q1322.34 1539.14 1322.34 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M1356.75 1533.45 L1356.75 1538.98 Q1354.27 1537.71 1351.59 1537.07 Q1348.92 1536.44 1346.06 1536.44 Q1341.69 1536.44 1339.5 1537.77 Q1337.33 1539.11 1337.33 1541.79 Q1337.33 1543.82 1338.89 1545 Q1340.45 1546.15 1345.16 1547.2 L1347.17 1547.64 Q1353.41 1548.98 1356.02 1551.43 Q1358.66 1553.85 1358.66 1558.21 Q1358.66 1563.17 1354.71 1566.07 Q1350.8 1568.97 1343.92 1568.97 Q1341.06 1568.97 1337.94 1568.39 Q1334.85 1567.85 1331.41 1566.74 L1331.41 1560.69 Q1334.66 1562.38 1337.81 1563.24 Q1340.96 1564.07 1344.05 1564.07 Q1348.19 1564.07 1350.42 1562.66 Q1352.64 1561.23 1352.64 1558.65 Q1352.64 1556.27 1351.02 1554.99 Q1349.43 1553.72 1343.99 1552.54 L1341.95 1552.07 Q1336.51 1550.92 1334.09 1548.56 Q1331.67 1546.18 1331.67 1542.04 Q1331.67 1537.01 1335.23 1534.27 Q1338.8 1531.54 1345.36 1531.54 Q1348.6 1531.54 1351.47 1532.01 Q1354.33 1532.49 1356.75 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,1423.18 138.867,47.2441 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,1181.09 157.764,1181.09 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,920.045 157.764,920.045 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,659.006 157.764,659.006 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,397.966 157.764,397.966 "/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,136.926 157.764,136.926 "/>
<path clip-path="url(#clip790)" d="M91.3391 1179.22 Q88.191 1179.22 86.3392 1181.37 Q84.5105 1183.53 84.5105 1187.28 Q84.5105 1191 86.3392 1193.18 Q88.191 1195.33 91.3391 1195.33 Q94.4873 1195.33 96.316 1193.18 Q98.1678 1191 98.1678 1187.28 Q98.1678 1183.53 96.316 1181.37 Q94.4873 1179.22 91.3391 1179.22 M100.621 1164.57 L100.621 1168.83 Q98.8622 1167.99 97.0567 1167.56 Q95.2743 1167.12 93.515 1167.12 Q88.8854 1167.12 86.4318 1170.24 Q84.0012 1173.37 83.654 1179.68 Q85.0197 1177.67 87.0799 1176.61 Q89.1401 1175.52 91.6169 1175.52 Q96.8252 1175.52 99.8345 1178.69 Q102.867 1181.84 102.867 1187.28 Q102.867 1192.6 99.7187 1195.82 Q96.5706 1199.04 91.3391 1199.04 Q85.3438 1199.04 82.1725 1194.45 Q79.0012 1189.85 79.0012 1181.12 Q79.0012 1172.93 82.8901 1168.06 Q86.779 1163.18 93.3299 1163.18 Q95.0891 1163.18 96.8715 1163.53 Q98.6771 1163.87 100.621 1164.57 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M81.2466 936.608 L81.2466 932.349 Q83.0058 933.182 84.8114 933.622 Q86.6169 934.061 88.353 934.061 Q92.9826 934.061 95.4132 930.96 Q97.8669 927.835 98.2141 921.492 Q96.8715 923.483 94.8113 924.548 Q92.7512 925.612 90.2512 925.612 Q85.066 925.612 82.0336 922.487 Q79.0244 919.339 79.0244 913.9 Q79.0244 908.576 82.1725 905.358 Q85.3206 902.14 90.5521 902.14 Q96.5474 902.14 99.6956 906.747 Q102.867 911.33 102.867 920.08 Q102.867 928.251 98.978 933.136 Q95.1123 937.997 88.5614 937.997 Q86.8021 937.997 84.9966 937.649 Q83.191 937.302 81.2466 936.608 M90.5521 921.955 Q93.7002 921.955 95.5289 919.802 Q97.3808 917.65 97.3808 913.9 Q97.3808 910.173 95.5289 908.02 Q93.7002 905.844 90.5521 905.844 Q87.404 905.844 85.5521 908.02 Q83.7234 910.173 83.7234 913.9 Q83.7234 917.65 85.5521 919.802 Q87.404 921.955 90.5521 921.955 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M53.168 672.35 L60.8069 672.35 L60.8069 645.985 L52.4967 647.651 L52.4967 643.392 L60.7606 641.726 L65.4365 641.726 L65.4365 672.35 L73.0753 672.35 L73.0753 676.286 L53.168 676.286 L53.168 672.35 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M86.5475 672.35 L102.867 672.35 L102.867 676.286 L80.9225 676.286 L80.9225 672.35 Q83.5845 669.596 88.1679 664.966 Q92.7743 660.313 93.9549 658.971 Q96.2002 656.448 97.0798 654.712 Q97.9826 652.952 97.9826 651.263 Q97.9826 648.508 96.0382 646.772 Q94.1169 645.036 91.0151 645.036 Q88.816 645.036 86.3623 645.8 Q83.9318 646.564 81.154 648.114 L81.154 643.392 Q83.9781 642.258 86.4318 641.679 Q88.8854 641.101 90.9225 641.101 Q96.2928 641.101 99.4872 643.786 Q102.682 646.471 102.682 650.962 Q102.682 653.091 101.871 655.013 Q101.084 656.911 98.978 659.503 Q98.3993 660.175 95.2974 663.392 Q92.1956 666.587 86.5475 672.35 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M52.5662 411.311 L60.205 411.311 L60.205 384.945 L51.8949 386.612 L51.8949 382.352 L60.1587 380.686 L64.8346 380.686 L64.8346 411.311 L72.4735 411.311 L72.4735 415.246 L52.5662 415.246 L52.5662 411.311 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M81.9642 380.686 L100.321 380.686 L100.321 384.621 L86.2466 384.621 L86.2466 393.093 Q87.2651 392.746 88.2836 392.584 Q89.3021 392.399 90.3206 392.399 Q96.1076 392.399 99.4872 395.57 Q102.867 398.741 102.867 404.158 Q102.867 409.737 99.3946 412.838 Q95.9224 415.917 89.603 415.917 Q87.4271 415.917 85.1586 415.547 Q82.9133 415.176 80.5059 414.436 L80.5059 409.737 Q82.5892 410.871 84.8114 411.426 Q87.0336 411.982 89.5104 411.982 Q93.515 411.982 95.853 409.875 Q98.1909 407.769 98.1909 404.158 Q98.1909 400.547 95.853 398.44 Q93.515 396.334 89.5104 396.334 Q87.6354 396.334 85.7605 396.751 Q83.9086 397.167 81.9642 398.047 L81.9642 380.686 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M51.6634 150.271 L59.3023 150.271 L59.3023 123.905 L50.9921 125.572 L50.9921 121.313 L59.256 119.646 L63.9319 119.646 L63.9319 150.271 L71.5707 150.271 L71.5707 154.206 L51.6634 154.206 L51.6634 150.271 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M91.0151 137.794 Q87.6817 137.794 85.7605 139.576 Q83.8623 141.359 83.8623 144.484 Q83.8623 147.609 85.7605 149.391 Q87.6817 151.174 91.0151 151.174 Q94.3484 151.174 96.2697 149.391 Q98.1909 147.586 98.1909 144.484 Q98.1909 141.359 96.2697 139.576 Q94.3715 137.794 91.0151 137.794 M86.3392 135.803 Q83.3299 135.063 81.6401 133.002 Q79.9735 130.942 79.9735 127.979 Q79.9735 123.836 82.9133 121.428 Q85.8762 119.021 91.0151 119.021 Q96.1771 119.021 99.1169 121.428 Q102.057 123.836 102.057 127.979 Q102.057 130.942 100.367 133.002 Q98.7002 135.063 95.7141 135.803 Q99.0937 136.59 100.969 138.882 Q102.867 141.174 102.867 144.484 Q102.867 149.507 99.7882 152.192 Q96.7326 154.877 91.0151 154.877 Q85.2975 154.877 82.2188 152.192 Q79.1633 149.507 79.1633 144.484 Q79.1633 141.174 81.0614 138.882 Q82.9595 136.59 86.3392 135.803 M84.6262 128.419 Q84.6262 131.104 86.2929 132.609 Q87.9827 134.114 91.0151 134.114 Q94.0243 134.114 95.7141 132.609 Q97.4271 131.104 97.4271 128.419 Q97.4271 125.734 95.7141 124.229 Q94.0243 122.725 91.0151 122.725 Q87.9827 122.725 86.2929 124.229 Q84.6262 125.734 84.6262 128.419 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip792)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="201.524,86.1857 222.621,237.284 243.718,372.559 264.814,534.898 285.911,673.101 307.008,743.979 328.104,791.35 349.201,830.119 370.298,857.054 391.395,886.387 412.491,914.313 433.588,937.857 454.685,963.965 475.781,984.536 496.878,1006.11 517.975,1022.38 539.071,1039.77 560.168,1054.94 581.265,1071.58 602.362,1086.15 623.458,1098.68 644.555,1110.37 665.652,1123.61 686.748,1135.69 707.845,1146.12 728.942,1159.5 750.039,1167.91 771.135,1176.09 792.232,1187.13 813.329,1196.52 834.425,1202.75 855.522,1210.55 876.619,1217.22 897.716,1224.44 918.812,1232.48 939.909,1238.46 961.006,1245.43 982.102,1250.84 1003.2,1256.21 1024.3,1261.67 1045.39,1266.57 1066.49,1271.34 1087.59,1276.39 1108.68,1279.99 1129.78,1284.74 1150.88,1287.66 1171.97,1291.86 1193.07,1296.52 1214.17,1299.05 1235.26,1302.15 1256.36,1305.3 1277.46,1308.09 1298.55,1311.46 1319.65,1314.7 1340.75,1317.65 1361.84,1319.06 1382.94,1321.3 1404.04,1324.04 1425.13,1326.84 1446.23,1329.28 1467.33,1331.19 1488.42,1333.41 1509.52,1334.78 1530.62,1337.58 1551.71,1338.93 1572.81,1340.44 1593.91,1342.97 1615,1344.46 1636.1,1346.82 1657.2,1348.44 1678.29,1349.96 1699.39,1351.18 1720.49,1352.97 1741.58,1354.64 1762.68,1355.99 1783.78,1358.14 1804.87,1358.8 1825.97,1360.45 1847.07,1361.51 1868.16,1363.44 1889.26,1364.14 1910.36,1365.24 1931.45,1366.17 1952.55,1368.46 1973.65,1368.88 1994.74,1370.66 2015.84,1371.07 2036.94,1373.26 2058.03,1373.95 2079.13,1375.2 2100.23,1375.64 2121.32,1377.12 2142.42,1377.57 2163.52,1379.02 2184.62,1380.03 2205.71,1380.75 2226.81,1381.49 2247.91,1382.54 2269,1384.24 2290.1,1383.83 "/>
<polyline clip-path="url(#clip792)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="201.524,215.791 222.621,321.796 243.718,477.198 264.814,643.716 285.911,730.568 307.008,785.208 328.104,817.716 349.201,844.903 370.298,860.7 391.395,878.735 412.491,902.184 433.588,917.964 454.685,936.838 475.781,961 496.878,976.691 517.975,991.399 539.071,1003.58 560.168,1008.68 581.265,1026.95 602.362,1029.43 623.458,1045.89 644.555,1041.87 665.652,1059.67 686.748,1061.84 707.845,1066.47 728.942,1077.74 750.039,1076.84 771.135,1088.05 792.232,1090.35 813.329,1097.44 834.425,1104.6 855.522,1096.45 876.619,1104.53 897.716,1116.23 918.812,1117.35 939.909,1110.5 961.006,1123.39 982.102,1121.27 1003.2,1112.09 1024.3,1120.44 1045.39,1116.35 1066.49,1122.23 1087.59,1129.95 1108.68,1129.97 1129.78,1121.24 1150.88,1126.11 1171.97,1116.4 1193.07,1115.18 1214.17,1120.36 1235.26,1120.14 1256.36,1126.07 1277.46,1132.52 1298.55,1117.43 1319.65,1125.1 1340.75,1114.99 1361.84,1104.59 1382.94,1118.47 1404.04,1114.1 1425.13,1122.66 1446.23,1096.3 1467.33,1119.52 1488.42,1101.64 1509.52,1115.64 1530.62,1114.91 1551.71,1094.54 1572.81,1114.53 1593.91,1094.55 1615,1095.34 1636.1,1116.23 1657.2,1093.09 1678.29,1108.59 1699.39,1105.76 1720.49,1099.27 1741.58,1113.63 1762.68,1093.4 1783.78,1116.4 1804.87,1091.84 1825.97,1103.48 1847.07,1108.35 1868.16,1090.84 1889.26,1089.34 1910.36,1082.98 1931.45,1082.5 1952.55,1078.52 1973.65,1103.91 1994.74,1099.13 2015.84,1088.78 2036.94,1092.74 2058.03,1097.39 2079.13,1073.18 2100.23,1077.48 2121.32,1089.52 2142.42,1067.36 2163.52,1071.12 2184.62,1075.85 2205.71,1075.46 2226.81,1062.06 2247.91,1071.57 2269,1054.85 2290.1,1064.41 "/>
<path clip-path="url(#clip790)" d="M1855.88 248.629 L2278.96 248.629 L2278.96 93.1086 L1855.88 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip790)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1855.88,248.629 2278.96,248.629 2278.96,93.1086 1855.88,93.1086 1855.88,248.629 "/>
<polyline clip-path="url(#clip790)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1880.48,144.949 2028.07,144.949 "/>
<path clip-path="url(#clip790)" d="M2060.08 128.942 L2060.08 136.303 L2068.85 136.303 L2068.85 139.613 L2060.08 139.613 L2060.08 153.687 Q2060.08 156.858 2060.94 157.761 Q2061.82 158.664 2064.48 158.664 L2068.85 158.664 L2068.85 162.229 L2064.48 162.229 Q2059.55 162.229 2057.67 160.4 Q2055.8 158.548 2055.8 153.687 L2055.8 139.613 L2052.67 139.613 L2052.67 136.303 L2055.8 136.303 L2055.8 128.942 L2060.08 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2089.48 140.284 Q2088.76 139.868 2087.9 139.682 Q2087.07 139.474 2086.05 139.474 Q2082.44 139.474 2080.5 141.835 Q2078.57 144.173 2078.57 148.571 L2078.57 162.229 L2074.29 162.229 L2074.29 136.303 L2078.57 136.303 L2078.57 140.331 Q2079.92 137.969 2082.07 136.835 Q2084.22 135.678 2087.3 135.678 Q2087.74 135.678 2088.27 135.747 Q2088.81 135.794 2089.45 135.909 L2089.48 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2105.73 149.196 Q2100.57 149.196 2098.57 150.377 Q2096.58 151.557 2096.58 154.405 Q2096.58 156.673 2098.07 158.016 Q2099.57 159.335 2102.14 159.335 Q2105.68 159.335 2107.81 156.835 Q2109.96 154.312 2109.96 150.145 L2109.96 149.196 L2105.73 149.196 M2114.22 147.437 L2114.22 162.229 L2109.96 162.229 L2109.96 158.293 Q2108.51 160.655 2106.33 161.789 Q2104.15 162.9 2101.01 162.9 Q2097.02 162.9 2094.66 160.678 Q2092.32 158.432 2092.32 154.682 Q2092.32 150.307 2095.24 148.085 Q2098.18 145.863 2103.99 145.863 L2109.96 145.863 L2109.96 145.446 Q2109.96 142.507 2108.02 140.909 Q2106.1 139.289 2102.6 139.289 Q2100.38 139.289 2098.27 139.821 Q2096.17 140.354 2094.22 141.419 L2094.22 137.483 Q2096.56 136.581 2098.76 136.141 Q2100.96 135.678 2103.04 135.678 Q2108.67 135.678 2111.44 138.594 Q2114.22 141.511 2114.22 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2123 136.303 L2127.26 136.303 L2127.26 162.229 L2123 162.229 L2123 136.303 M2123 126.21 L2127.26 126.21 L2127.26 131.604 L2123 131.604 L2123 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2157.72 146.581 L2157.72 162.229 L2153.46 162.229 L2153.46 146.719 Q2153.46 143.039 2152.02 141.21 Q2150.59 139.382 2147.72 139.382 Q2144.27 139.382 2142.28 141.581 Q2140.29 143.78 2140.29 147.576 L2140.29 162.229 L2136 162.229 L2136 136.303 L2140.29 136.303 L2140.29 140.331 Q2141.82 137.993 2143.88 136.835 Q2145.96 135.678 2148.67 135.678 Q2153.13 135.678 2155.43 138.456 Q2157.72 141.21 2157.72 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2185.91 170.099 L2185.91 173.409 L2161.28 173.409 L2161.28 170.099 L2185.91 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2194.04 158.34 L2194.04 172.09 L2189.75 172.09 L2189.75 136.303 L2194.04 136.303 L2194.04 140.238 Q2195.38 137.923 2197.42 136.812 Q2199.48 135.678 2202.32 135.678 Q2207.05 135.678 2209.99 139.428 Q2212.95 143.178 2212.95 149.289 Q2212.95 155.4 2209.99 159.15 Q2207.05 162.9 2202.32 162.9 Q2199.48 162.9 2197.42 161.789 Q2195.38 160.655 2194.04 158.34 M2208.53 149.289 Q2208.53 144.59 2206.58 141.928 Q2204.66 139.243 2201.28 139.243 Q2197.9 139.243 2195.96 141.928 Q2194.04 144.59 2194.04 149.289 Q2194.04 153.988 2195.96 156.673 Q2197.9 159.335 2201.28 159.335 Q2204.66 159.335 2206.58 156.673 Q2208.53 153.988 2208.53 149.289 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2224.13 158.34 L2224.13 172.09 L2219.85 172.09 L2219.85 136.303 L2224.13 136.303 L2224.13 140.238 Q2225.47 137.923 2227.51 136.812 Q2229.57 135.678 2232.42 135.678 Q2237.14 135.678 2240.08 139.428 Q2243.04 143.178 2243.04 149.289 Q2243.04 155.4 2240.08 159.15 Q2237.14 162.9 2232.42 162.9 Q2229.57 162.9 2227.51 161.789 Q2225.47 160.655 2224.13 158.34 M2238.62 149.289 Q2238.62 144.59 2236.68 141.928 Q2234.75 139.243 2231.37 139.243 Q2228 139.243 2226.05 141.928 Q2224.13 144.59 2224.13 149.289 Q2224.13 153.988 2226.05 156.673 Q2228 159.335 2231.37 159.335 Q2234.75 159.335 2236.68 156.673 Q2238.62 153.988 2238.62 149.289 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2250.1 126.21 L2254.36 126.21 L2254.36 162.229 L2250.1 162.229 L2250.1 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip790)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1880.48,196.789 2028.07,196.789 "/>
<path clip-path="url(#clip790)" d="M2052.67 188.143 L2057.19 188.143 L2065.29 209.902 L2073.39 188.143 L2077.9 188.143 L2068.18 214.069 L2062.39 214.069 L2052.67 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2095.57 201.036 Q2090.4 201.036 2088.41 202.217 Q2086.42 203.397 2086.42 206.245 Q2086.42 208.513 2087.9 209.856 Q2089.41 211.175 2091.98 211.175 Q2095.52 211.175 2097.65 208.675 Q2099.8 206.152 2099.8 201.985 L2099.8 201.036 L2095.57 201.036 M2104.06 199.277 L2104.06 214.069 L2099.8 214.069 L2099.8 210.133 Q2098.34 212.495 2096.17 213.629 Q2093.99 214.74 2090.84 214.74 Q2086.86 214.74 2084.5 212.518 Q2082.16 210.272 2082.16 206.522 Q2082.16 202.147 2085.08 199.925 Q2088.02 197.703 2093.83 197.703 L2099.8 197.703 L2099.8 197.286 Q2099.8 194.347 2097.86 192.749 Q2095.94 191.129 2092.44 191.129 Q2090.22 191.129 2088.11 191.661 Q2086.01 192.194 2084.06 193.259 L2084.06 189.323 Q2086.4 188.421 2088.6 187.981 Q2090.8 187.518 2092.88 187.518 Q2098.51 187.518 2101.28 190.434 Q2104.06 193.351 2104.06 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2112.83 178.05 L2117.09 178.05 L2117.09 214.069 L2112.83 214.069 L2112.83 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2145.7 221.939 L2145.7 225.249 L2121.07 225.249 L2121.07 221.939 L2145.7 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2153.83 210.18 L2153.83 223.93 L2149.55 223.93 L2149.55 188.143 L2153.83 188.143 L2153.83 192.078 Q2155.17 189.763 2157.21 188.652 Q2159.27 187.518 2162.12 187.518 Q2166.84 187.518 2169.78 191.268 Q2172.74 195.018 2172.74 201.129 Q2172.74 207.24 2169.78 210.99 Q2166.84 214.74 2162.12 214.74 Q2159.27 214.74 2157.21 213.629 Q2155.17 212.495 2153.83 210.18 M2168.32 201.129 Q2168.32 196.43 2166.38 193.768 Q2164.45 191.083 2161.07 191.083 Q2157.69 191.083 2155.75 193.768 Q2153.83 196.43 2153.83 201.129 Q2153.83 205.828 2155.75 208.513 Q2157.69 211.175 2161.07 211.175 Q2164.45 211.175 2166.38 208.513 Q2168.32 205.828 2168.32 201.129 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2183.92 210.18 L2183.92 223.93 L2179.64 223.93 L2179.64 188.143 L2183.92 188.143 L2183.92 192.078 Q2185.26 189.763 2187.3 188.652 Q2189.36 187.518 2192.21 187.518 Q2196.93 187.518 2199.87 191.268 Q2202.83 195.018 2202.83 201.129 Q2202.83 207.24 2199.87 210.99 Q2196.93 214.74 2192.21 214.74 Q2189.36 214.74 2187.3 213.629 Q2185.26 212.495 2183.92 210.18 M2198.41 201.129 Q2198.41 196.43 2196.47 193.768 Q2194.55 191.083 2191.17 191.083 Q2187.79 191.083 2185.84 193.768 Q2183.92 196.43 2183.92 201.129 Q2183.92 205.828 2185.84 208.513 Q2187.79 211.175 2191.17 211.175 Q2194.55 211.175 2196.47 208.513 Q2198.41 205.828 2198.41 201.129 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip790)" d="M2209.89 178.05 L2214.15 178.05 L2214.15 214.069 L2209.89 214.069 L2209.89 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="language-julia hljs">prefix = &quot;it has&quot;
d2lai.prediction(prefix, m[1], data.vocab, 20)</code></pre><pre><code class="nohighlight hljs">&quot;it has and some that is al&quot;</code></pre><h2 id="Concise-Implementation"><a class="docs-heading-anchor" href="#Concise-Implementation">Concise Implementation</a><a id="Concise-Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Concise-Implementation" title="Permalink"></a></h2><p>Using high-level APIs, we can directly instantiate an LSTM model. This encapsulates all the configuration details  that we made explicit above.  The code is significantly faster.</p><pre><code class="language-julia hljs">lstm_concise = LSTM(length(data.vocab) =&gt; num_hiddens; return_state = true)
model = RNNModelConcise(lstm_concise, num_hiddens, length(data.vocab)) |&gt; f64
opt = Descent(1.)
trainer = Trainer(model, data, opt; max_epochs = 100, gpu = true, board_yscale = :identity, gradient_clip_val = 1.)
m = d2lai.fit(trainer);</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 2.8427107, Val Loss: 2.8369534
    [ Info: Train Loss: 2.76487, Val Loss: 2.7668047
    [ Info: Train Loss: 2.6583545, Val Loss: 2.6500497
    [ Info: Train Loss: 2.5179682, Val Loss: 2.5197139
    [ Info: Train Loss: 2.431327, Val Loss: 2.4431098
    [ Info: Train Loss: 2.3572307, Val Loss: 2.391342
    [ Info: Train Loss: 2.3227513, Val Loss: 2.3556664
    [ Info: Train Loss: 2.2947817, Val Loss: 2.3331885
    [ Info: Train Loss: 2.2521603, Val Loss: 2.2913945
    [ Info: Train Loss: 2.2128947, Val Loss: 2.2687695
    [ Info: Train Loss: 2.1813984, Val Loss: 2.2456841
    [ Info: Train Loss: 2.1330736, Val Loss: 2.2201614
    [ Info: Train Loss: 2.107634, Val Loss: 2.193372
    [ Info: Train Loss: 2.067568, Val Loss: 2.15843
    [ Info: Train Loss: 2.0382428, Val Loss: 2.1482017
    [ Info: Train Loss: 2.0414839, Val Loss: 2.1229594
    [ Info: Train Loss: 1.9991355, Val Loss: 2.112071
    [ Info: Train Loss: 1.9926142, Val Loss: 2.0838833
    [ Info: Train Loss: 1.9585189, Val Loss: 2.0609097
    [ Info: Train Loss: 1.9337728, Val Loss: 2.049715
    [ Info: Train Loss: 1.8995695, Val Loss: 2.041491
    [ Info: Train Loss: 1.87231, Val Loss: 2.0025415
    [ Info: Train Loss: 1.8709, Val Loss: 2.007211
    [ Info: Train Loss: 1.8434366, Val Loss: 1.9976096
    [ Info: Train Loss: 1.8393289, Val Loss: 2.0047002
    [ Info: Train Loss: 1.8241904, Val Loss: 1.9923693
    [ Info: Train Loss: 1.8032255, Val Loss: 1.994213
    [ Info: Train Loss: 1.7696857, Val Loss: 1.9697965
    [ Info: Train Loss: 1.7758065, Val Loss: 1.9684645
    [ Info: Train Loss: 1.7555944, Val Loss: 1.9513144
    [ Info: Train Loss: 1.7414378, Val Loss: 1.9620013
    [ Info: Train Loss: 1.7296154, Val Loss: 1.9514403
    [ Info: Train Loss: 1.71898, Val Loss: 1.9280633
    [ Info: Train Loss: 1.709265, Val Loss: 1.9368947
    [ Info: Train Loss: 1.699403, Val Loss: 1.9414907
    [ Info: Train Loss: 1.6675162, Val Loss: 1.9390253
    [ Info: Train Loss: 1.676482, Val Loss: 1.9241304
    [ Info: Train Loss: 1.6794927, Val Loss: 1.9257721
    [ Info: Train Loss: 1.6586671, Val Loss: 1.9032117
    [ Info: Train Loss: 1.6457124, Val Loss: 1.9131901
    [ Info: Train Loss: 1.6280394, Val Loss: 1.9078672
    [ Info: Train Loss: 1.6233665, Val Loss: 1.900517
    [ Info: Train Loss: 1.6127323, Val Loss: 1.8871064
    [ Info: Train Loss: 1.604043, Val Loss: 1.905726
    [ Info: Train Loss: 1.6070231, Val Loss: 1.8944578
    [ Info: Train Loss: 1.5934694, Val Loss: 1.8868828
    [ Info: Train Loss: 1.5822835, Val Loss: 1.8936948
    [ Info: Train Loss: 1.5772934, Val Loss: 1.897214
    [ Info: Train Loss: 1.5587626, Val Loss: 1.8838716
    [ Info: Train Loss: 1.5576969, Val Loss: 1.9061204
    [ Info: Train Loss: 1.5581809, Val Loss: 1.8890705
    [ Info: Train Loss: 1.5537266, Val Loss: 1.8935368
    [ Info: Train Loss: 1.5498881, Val Loss: 1.9078213
    [ Info: Train Loss: 1.5434625, Val Loss: 1.9040521
    [ Info: Train Loss: 1.517248, Val Loss: 1.9126192
    [ Info: Train Loss: 1.5158942, Val Loss: 1.9081383
    [ Info: Train Loss: 1.5224184, Val Loss: 1.9113809
    [ Info: Train Loss: 1.5104607, Val Loss: 1.8958948
    [ Info: Train Loss: 1.508267, Val Loss: 1.9130298
    [ Info: Train Loss: 1.4817677, Val Loss: 1.9148769
    [ Info: Train Loss: 1.501207, Val Loss: 1.9040751
    [ Info: Train Loss: 1.4847144, Val Loss: 1.8919543
    [ Info: Train Loss: 1.5002887, Val Loss: 1.9020813
    [ Info: Train Loss: 1.4769074, Val Loss: 1.9049815
    [ Info: Train Loss: 1.4697545, Val Loss: 1.8896354
    [ Info: Train Loss: 1.4565266, Val Loss: 1.9191732
    [ Info: Train Loss: 1.4550897, Val Loss: 1.9008762
    [ Info: Train Loss: 1.4565283, Val Loss: 1.9195735
    [ Info: Train Loss: 1.4566759, Val Loss: 1.9123726
    [ Info: Train Loss: 1.4473749, Val Loss: 1.9118538
    [ Info: Train Loss: 1.4360607, Val Loss: 1.9222747
    [ Info: Train Loss: 1.4182155, Val Loss: 1.9195168
    [ Info: Train Loss: 1.4331613, Val Loss: 1.9151908
    [ Info: Train Loss: 1.4168733, Val Loss: 1.9315561
    [ Info: Train Loss: 1.4212273, Val Loss: 1.9126766
    [ Info: Train Loss: 1.4102596, Val Loss: 1.9049134
    [ Info: Train Loss: 1.4269556, Val Loss: 1.9387856
    [ Info: Train Loss: 1.4188406, Val Loss: 1.9388701
    [ Info: Train Loss: 1.3939968, Val Loss: 1.9117727
    [ Info: Train Loss: 1.4007958, Val Loss: 1.9260702
    [ Info: Train Loss: 1.4008294, Val Loss: 1.9048063
    [ Info: Train Loss: 1.4162264, Val Loss: 1.9182342
    [ Info: Train Loss: 1.3896556, Val Loss: 1.9164705
    [ Info: Train Loss: 1.3912114, Val Loss: 1.935259
    [ Info: Train Loss: 1.4087954, Val Loss: 1.9066732
    [ Info: Train Loss: 1.3773736, Val Loss: 1.933785
    [ Info: Train Loss: 1.3843387, Val Loss: 1.9180298
    [ Info: Train Loss: 1.3781508, Val Loss: 1.915618
    [ Info: Train Loss: 1.3738914, Val Loss: 1.9218712
    [ Info: Train Loss: 1.3691036, Val Loss: 1.9033444
    [ Info: Train Loss: 1.3789481, Val Loss: 1.9576943
    [ Info: Train Loss: 1.3722394, Val Loss: 1.9286522
    [ Info: Train Loss: 1.3392986, Val Loss: 1.9551451
    [ Info: Train Loss: 1.35313, Val Loss: 1.9495249
    [ Info: Train Loss: 1.36132, Val Loss: 1.9364525
    [ Info: Train Loss: 1.358688, Val Loss: 1.9494243
    [ Info: Train Loss: 1.3130082, Val Loss: 1.9404734
    [ Info: Train Loss: 1.338081, Val Loss: 1.9542209
    [ Info: Train Loss: 1.3178873, Val Loss: 1.9131705
    [ Info: Train Loss: 1.3307419, Val Loss: 1.9667456</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip970">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip970)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip971">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip970)" d="M138.867 1423.18 L2352.76 1423.18 L2352.76 47.2441 L138.867 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip972">
    <rect x="138" y="47" width="2215" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="180.427,1423.18 180.427,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="707.845,1423.18 707.845,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1235.26,1423.18 1235.26,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1762.68,1423.18 1762.68,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2290.1,1423.18 2290.1,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,1190.07 2352.76,1190.07 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,924.043 2352.76,924.043 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,658.017 2352.76,658.017 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,391.991 2352.76,391.991 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="138.867,125.965 2352.76,125.965 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="180.427,1423.18 180.427,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="707.845,1423.18 707.845,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1235.26,1423.18 1235.26,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1762.68,1423.18 1762.68,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2290.1,1423.18 2290.1,1404.28 "/>
<path clip-path="url(#clip970)" d="M180.427 1454.1 Q176.816 1454.1 174.988 1457.66 Q173.182 1461.2 173.182 1468.33 Q173.182 1475.44 174.988 1479.01 Q176.816 1482.55 180.427 1482.55 Q184.062 1482.55 185.867 1479.01 Q187.696 1475.44 187.696 1468.33 Q187.696 1461.2 185.867 1457.66 Q184.062 1454.1 180.427 1454.1 M180.427 1450.39 Q186.238 1450.39 189.293 1455 Q192.372 1459.58 192.372 1468.33 Q192.372 1477.06 189.293 1481.67 Q186.238 1486.25 180.427 1486.25 Q174.617 1486.25 171.539 1481.67 Q168.483 1477.06 168.483 1468.33 Q168.483 1459.58 171.539 1455 Q174.617 1450.39 180.427 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M687.116 1481.64 L703.435 1481.64 L703.435 1485.58 L681.491 1485.58 L681.491 1481.64 Q684.153 1478.89 688.737 1474.26 Q693.343 1469.61 694.524 1468.27 Q696.769 1465.74 697.648 1464.01 Q698.551 1462.25 698.551 1460.56 Q698.551 1457.8 696.607 1456.07 Q694.686 1454.33 691.584 1454.33 Q689.385 1454.33 686.931 1455.09 Q684.5 1455.86 681.723 1457.41 L681.723 1452.69 Q684.547 1451.55 687 1450.97 Q689.454 1450.39 691.491 1450.39 Q696.861 1450.39 700.056 1453.08 Q703.25 1455.77 703.25 1460.26 Q703.25 1462.39 702.44 1464.31 Q701.653 1466.2 699.547 1468.8 Q698.968 1469.47 695.866 1472.69 Q692.764 1475.88 687.116 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M713.297 1451.02 L731.653 1451.02 L731.653 1454.96 L717.579 1454.96 L717.579 1463.43 Q718.597 1463.08 719.616 1462.92 Q720.634 1462.73 721.653 1462.73 Q727.44 1462.73 730.82 1465.9 Q734.199 1469.08 734.199 1474.49 Q734.199 1480.07 730.727 1483.17 Q727.255 1486.25 720.935 1486.25 Q718.759 1486.25 716.491 1485.88 Q714.246 1485.51 711.838 1484.77 L711.838 1480.07 Q713.922 1481.2 716.144 1481.76 Q718.366 1482.32 720.843 1482.32 Q724.847 1482.32 727.185 1480.21 Q729.523 1478.1 729.523 1474.49 Q729.523 1470.88 727.185 1468.77 Q724.847 1466.67 720.843 1466.67 Q718.968 1466.67 717.093 1467.08 Q715.241 1467.5 713.297 1468.38 L713.297 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1209.96 1451.02 L1228.32 1451.02 L1228.32 1454.96 L1214.24 1454.96 L1214.24 1463.43 Q1215.26 1463.08 1216.28 1462.92 Q1217.3 1462.73 1218.32 1462.73 Q1224.11 1462.73 1227.49 1465.9 Q1230.86 1469.08 1230.86 1474.49 Q1230.86 1480.07 1227.39 1483.17 Q1223.92 1486.25 1217.6 1486.25 Q1215.43 1486.25 1213.16 1485.88 Q1210.91 1485.51 1208.5 1484.77 L1208.5 1480.07 Q1210.59 1481.2 1212.81 1481.76 Q1215.03 1482.32 1217.51 1482.32 Q1221.51 1482.32 1223.85 1480.21 Q1226.19 1478.1 1226.19 1474.49 Q1226.19 1470.88 1223.85 1468.77 Q1221.51 1466.67 1217.51 1466.67 Q1215.63 1466.67 1213.76 1467.08 Q1211.91 1467.5 1209.96 1468.38 L1209.96 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1250.08 1454.1 Q1246.47 1454.1 1244.64 1457.66 Q1242.83 1461.2 1242.83 1468.33 Q1242.83 1475.44 1244.64 1479.01 Q1246.47 1482.55 1250.08 1482.55 Q1253.71 1482.55 1255.52 1479.01 Q1257.35 1475.44 1257.35 1468.33 Q1257.35 1461.2 1255.52 1457.66 Q1253.71 1454.1 1250.08 1454.1 M1250.08 1450.39 Q1255.89 1450.39 1258.94 1455 Q1262.02 1459.58 1262.02 1468.33 Q1262.02 1477.06 1258.94 1481.67 Q1255.89 1486.25 1250.08 1486.25 Q1244.27 1486.25 1241.19 1481.67 Q1238.13 1477.06 1238.13 1468.33 Q1238.13 1459.58 1241.19 1455 Q1244.27 1450.39 1250.08 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1736.54 1451.02 L1758.76 1451.02 L1758.76 1453.01 L1746.21 1485.58 L1741.33 1485.58 L1753.13 1454.96 L1736.54 1454.96 L1736.54 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1767.92 1451.02 L1786.28 1451.02 L1786.28 1454.96 L1772.21 1454.96 L1772.21 1463.43 Q1773.22 1463.08 1774.24 1462.92 Q1775.26 1462.73 1776.28 1462.73 Q1782.07 1462.73 1785.45 1465.9 Q1788.83 1469.08 1788.83 1474.49 Q1788.83 1480.07 1785.35 1483.17 Q1781.88 1486.25 1775.56 1486.25 Q1773.39 1486.25 1771.12 1485.88 Q1768.87 1485.51 1766.47 1484.77 L1766.47 1480.07 Q1768.55 1481.2 1770.77 1481.76 Q1772.99 1482.32 1775.47 1482.32 Q1779.47 1482.32 1781.81 1480.21 Q1784.15 1478.1 1784.15 1474.49 Q1784.15 1470.88 1781.81 1468.77 Q1779.47 1466.67 1775.47 1466.67 Q1773.6 1466.67 1771.72 1467.08 Q1769.87 1467.5 1767.92 1468.38 L1767.92 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2249.71 1481.64 L2257.34 1481.64 L2257.34 1455.28 L2249.03 1456.95 L2249.03 1452.69 L2257.3 1451.02 L2261.97 1451.02 L2261.97 1481.64 L2269.61 1481.64 L2269.61 1485.58 L2249.71 1485.58 L2249.71 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2289.06 1454.1 Q2285.45 1454.1 2283.62 1457.66 Q2281.81 1461.2 2281.81 1468.33 Q2281.81 1475.44 2283.62 1479.01 Q2285.45 1482.55 2289.06 1482.55 Q2292.69 1482.55 2294.5 1479.01 Q2296.33 1475.44 2296.33 1468.33 Q2296.33 1461.2 2294.5 1457.66 Q2292.69 1454.1 2289.06 1454.1 M2289.06 1450.39 Q2294.87 1450.39 2297.92 1455 Q2301 1459.58 2301 1468.33 Q2301 1477.06 2297.92 1481.67 Q2294.87 1486.25 2289.06 1486.25 Q2283.25 1486.25 2280.17 1481.67 Q2277.11 1477.06 2277.11 1468.33 Q2277.11 1459.58 2280.17 1455 Q2283.25 1450.39 2289.06 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2319.22 1454.1 Q2315.61 1454.1 2313.78 1457.66 Q2311.97 1461.2 2311.97 1468.33 Q2311.97 1475.44 2313.78 1479.01 Q2315.61 1482.55 2319.22 1482.55 Q2322.85 1482.55 2324.66 1479.01 Q2326.49 1475.44 2326.49 1468.33 Q2326.49 1461.2 2324.66 1457.66 Q2322.85 1454.1 2319.22 1454.1 M2319.22 1450.39 Q2325.03 1450.39 2328.08 1455 Q2331.16 1459.58 2331.16 1468.33 Q2331.16 1477.06 2328.08 1481.67 Q2325.03 1486.25 2319.22 1486.25 Q2313.41 1486.25 2310.33 1481.67 Q2307.27 1477.06 2307.27 1468.33 Q2307.27 1459.58 2310.33 1455 Q2313.41 1450.39 2319.22 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1166 1548.76 L1166 1551.62 L1139.07 1551.62 Q1139.46 1557.67 1142.7 1560.85 Q1145.98 1564 1151.81 1564 Q1155.18 1564 1158.33 1563.17 Q1161.51 1562.35 1164.63 1560.69 L1164.63 1566.23 Q1161.48 1567.57 1158.17 1568.27 Q1154.86 1568.97 1151.46 1568.97 Q1142.93 1568.97 1137.93 1564 Q1132.96 1559.04 1132.96 1550.57 Q1132.96 1541.82 1137.67 1536.69 Q1142.42 1531.54 1150.44 1531.54 Q1157.63 1531.54 1161.8 1536.18 Q1166 1540.8 1166 1548.76 M1160.14 1547.04 Q1160.08 1542.23 1157.44 1539.37 Q1154.83 1536.5 1150.5 1536.5 Q1145.6 1536.5 1142.64 1539.27 Q1139.71 1542.04 1139.27 1547.07 L1160.14 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1181.28 1562.7 L1181.28 1581.6 L1175.39 1581.6 L1175.39 1532.4 L1181.28 1532.4 L1181.28 1537.81 Q1183.13 1534.62 1185.93 1533.1 Q1188.76 1531.54 1192.67 1531.54 Q1199.17 1531.54 1203.21 1536.69 Q1207.28 1541.85 1207.28 1550.25 Q1207.28 1558.65 1203.21 1563.81 Q1199.17 1568.97 1192.67 1568.97 Q1188.76 1568.97 1185.93 1567.44 Q1183.13 1565.88 1181.28 1562.7 M1201.2 1550.25 Q1201.2 1543.79 1198.53 1540.13 Q1195.89 1536.44 1191.24 1536.44 Q1186.59 1536.44 1183.92 1540.13 Q1181.28 1543.79 1181.28 1550.25 Q1181.28 1556.71 1183.92 1560.4 Q1186.59 1564.07 1191.24 1564.07 Q1195.89 1564.07 1198.53 1560.4 Q1201.2 1556.71 1201.2 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1230.8 1536.5 Q1226.09 1536.5 1223.36 1540.19 Q1220.62 1543.85 1220.62 1550.25 Q1220.62 1556.65 1223.32 1560.34 Q1226.06 1564 1230.8 1564 Q1235.48 1564 1238.22 1560.31 Q1240.96 1556.62 1240.96 1550.25 Q1240.96 1543.92 1238.22 1540.23 Q1235.48 1536.5 1230.8 1536.5 M1230.8 1531.54 Q1238.44 1531.54 1242.8 1536.5 Q1247.16 1541.47 1247.16 1550.25 Q1247.16 1559 1242.8 1564 Q1238.44 1568.97 1230.8 1568.97 Q1223.13 1568.97 1218.77 1564 Q1214.44 1559 1214.44 1550.25 Q1214.44 1541.47 1218.77 1536.5 Q1223.13 1531.54 1230.8 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1282.53 1533.76 L1282.53 1539.24 Q1280.04 1537.87 1277.53 1537.2 Q1275.05 1536.5 1272.5 1536.5 Q1266.8 1536.5 1263.65 1540.13 Q1260.5 1543.73 1260.5 1550.25 Q1260.5 1556.78 1263.65 1560.4 Q1266.8 1564 1272.5 1564 Q1275.05 1564 1277.53 1563.33 Q1280.04 1562.63 1282.53 1561.26 L1282.53 1566.68 Q1280.07 1567.82 1277.43 1568.39 Q1274.82 1568.97 1271.86 1568.97 Q1263.81 1568.97 1259.07 1563.91 Q1254.33 1558.85 1254.33 1550.25 Q1254.33 1541.53 1259.1 1536.53 Q1263.91 1531.54 1272.24 1531.54 Q1274.95 1531.54 1277.53 1532.11 Q1280.11 1532.65 1282.53 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1322.34 1546.53 L1322.34 1568.04 L1316.49 1568.04 L1316.49 1546.72 Q1316.49 1541.66 1314.51 1539.14 Q1312.54 1536.63 1308.59 1536.63 Q1303.85 1536.63 1301.11 1539.65 Q1298.38 1542.68 1298.38 1547.9 L1298.38 1568.04 L1292.49 1568.04 L1292.49 1518.52 L1298.38 1518.52 L1298.38 1537.93 Q1300.48 1534.72 1303.31 1533.13 Q1306.17 1531.54 1309.9 1531.54 Q1316.04 1531.54 1319.19 1535.36 Q1322.34 1539.14 1322.34 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1356.75 1533.45 L1356.75 1538.98 Q1354.27 1537.71 1351.59 1537.07 Q1348.92 1536.44 1346.06 1536.44 Q1341.69 1536.44 1339.5 1537.77 Q1337.33 1539.11 1337.33 1541.79 Q1337.33 1543.82 1338.89 1545 Q1340.45 1546.15 1345.16 1547.2 L1347.17 1547.64 Q1353.41 1548.98 1356.02 1551.43 Q1358.66 1553.85 1358.66 1558.21 Q1358.66 1563.17 1354.71 1566.07 Q1350.8 1568.97 1343.92 1568.97 Q1341.06 1568.97 1337.94 1568.39 Q1334.85 1567.85 1331.41 1566.74 L1331.41 1560.69 Q1334.66 1562.38 1337.81 1563.24 Q1340.96 1564.07 1344.05 1564.07 Q1348.19 1564.07 1350.42 1562.66 Q1352.64 1561.23 1352.64 1558.65 Q1352.64 1556.27 1351.02 1554.99 Q1349.43 1553.72 1343.99 1552.54 L1341.95 1552.07 Q1336.51 1550.92 1334.09 1548.56 Q1331.67 1546.18 1331.67 1542.04 Q1331.67 1537.01 1335.23 1534.27 Q1338.8 1531.54 1345.36 1531.54 Q1348.6 1531.54 1351.47 1532.01 Q1354.33 1532.49 1356.75 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,1423.18 138.867,47.2441 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,1190.07 157.764,1190.07 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,924.043 157.764,924.043 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,658.017 157.764,658.017 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,391.991 157.764,391.991 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="138.867,125.965 157.764,125.965 "/>
<path clip-path="url(#clip970)" d="M91.3391 1188.21 Q88.191 1188.21 86.3392 1190.36 Q84.5105 1192.51 84.5105 1196.26 Q84.5105 1199.99 86.3392 1202.16 Q88.191 1204.32 91.3391 1204.32 Q94.4873 1204.32 96.316 1202.16 Q98.1678 1199.99 98.1678 1196.26 Q98.1678 1192.51 96.316 1190.36 Q94.4873 1188.21 91.3391 1188.21 M100.621 1173.55 L100.621 1177.81 Q98.8622 1176.98 97.0567 1176.54 Q95.2743 1176.1 93.515 1176.1 Q88.8854 1176.1 86.4318 1179.22 Q84.0012 1182.35 83.654 1188.67 Q85.0197 1186.65 87.0799 1185.59 Q89.1401 1184.5 91.6169 1184.5 Q96.8252 1184.5 99.8345 1187.67 Q102.867 1190.82 102.867 1196.26 Q102.867 1201.58 99.7187 1204.8 Q96.5706 1208.02 91.3391 1208.02 Q85.3438 1208.02 82.1725 1203.44 Q79.0012 1198.83 79.0012 1190.1 Q79.0012 1181.91 82.8901 1177.05 Q86.779 1172.16 93.3299 1172.16 Q95.0891 1172.16 96.8715 1172.51 Q98.6771 1172.86 100.621 1173.55 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M81.2466 940.605 L81.2466 936.346 Q83.0058 937.179 84.8114 937.619 Q86.6169 938.059 88.353 938.059 Q92.9826 938.059 95.4132 934.957 Q97.8669 931.832 98.2141 925.489 Q96.8715 927.48 94.8113 928.545 Q92.7512 929.61 90.2512 929.61 Q85.066 929.61 82.0336 926.485 Q79.0244 923.337 79.0244 917.897 Q79.0244 912.573 82.1725 909.355 Q85.3206 906.138 90.5521 906.138 Q96.5474 906.138 99.6956 910.744 Q102.867 915.328 102.867 924.077 Q102.867 932.249 98.978 937.133 Q95.1123 941.994 88.5614 941.994 Q86.8021 941.994 84.9966 941.647 Q83.191 941.3 81.2466 940.605 M90.5521 925.952 Q93.7002 925.952 95.5289 923.8 Q97.3808 921.647 97.3808 917.897 Q97.3808 914.17 95.5289 912.017 Q93.7002 909.841 90.5521 909.841 Q87.404 909.841 85.5521 912.017 Q83.7234 914.17 83.7234 917.897 Q83.7234 921.647 85.5521 923.8 Q87.404 925.952 90.5521 925.952 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M53.168 671.362 L60.8069 671.362 L60.8069 644.996 L52.4967 646.663 L52.4967 642.404 L60.7606 640.737 L65.4365 640.737 L65.4365 671.362 L73.0753 671.362 L73.0753 675.297 L53.168 675.297 L53.168 671.362 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M86.5475 671.362 L102.867 671.362 L102.867 675.297 L80.9225 675.297 L80.9225 671.362 Q83.5845 668.607 88.1679 663.978 Q92.7743 659.325 93.9549 657.982 Q96.2002 655.459 97.0798 653.723 Q97.9826 651.964 97.9826 650.274 Q97.9826 647.519 96.0382 645.783 Q94.1169 644.047 91.0151 644.047 Q88.816 644.047 86.3623 644.811 Q83.9318 645.575 81.154 647.126 L81.154 642.404 Q83.9781 641.269 86.4318 640.691 Q88.8854 640.112 90.9225 640.112 Q96.2928 640.112 99.4872 642.797 Q102.682 645.482 102.682 649.973 Q102.682 652.103 101.871 654.024 Q101.084 655.922 98.978 658.515 Q98.3993 659.186 95.2974 662.404 Q92.1956 665.598 86.5475 671.362 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M52.5662 405.336 L60.205 405.336 L60.205 378.97 L51.8949 380.637 L51.8949 376.378 L60.1587 374.711 L64.8346 374.711 L64.8346 405.336 L72.4735 405.336 L72.4735 409.271 L52.5662 409.271 L52.5662 405.336 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M81.9642 374.711 L100.321 374.711 L100.321 378.646 L86.2466 378.646 L86.2466 387.119 Q87.2651 386.771 88.2836 386.609 Q89.3021 386.424 90.3206 386.424 Q96.1076 386.424 99.4872 389.595 Q102.867 392.767 102.867 398.183 Q102.867 403.762 99.3946 406.864 Q95.9224 409.943 89.603 409.943 Q87.4271 409.943 85.1586 409.572 Q82.9133 409.202 80.5059 408.461 L80.5059 403.762 Q82.5892 404.896 84.8114 405.452 Q87.0336 406.007 89.5104 406.007 Q93.515 406.007 95.853 403.901 Q98.1909 401.794 98.1909 398.183 Q98.1909 394.572 95.853 392.466 Q93.515 390.359 89.5104 390.359 Q87.6354 390.359 85.7605 390.776 Q83.9086 391.193 81.9642 392.072 L81.9642 374.711 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M51.6634 139.31 L59.3023 139.31 L59.3023 112.945 L50.9921 114.611 L50.9921 110.352 L59.256 108.685 L63.9319 108.685 L63.9319 139.31 L71.5707 139.31 L71.5707 143.245 L51.6634 143.245 L51.6634 139.31 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M91.0151 126.834 Q87.6817 126.834 85.7605 128.616 Q83.8623 130.398 83.8623 133.523 Q83.8623 136.648 85.7605 138.431 Q87.6817 140.213 91.0151 140.213 Q94.3484 140.213 96.2697 138.431 Q98.1909 136.625 98.1909 133.523 Q98.1909 130.398 96.2697 128.616 Q94.3715 126.834 91.0151 126.834 M86.3392 124.843 Q83.3299 124.102 81.6401 122.042 Q79.9735 119.982 79.9735 117.019 Q79.9735 112.875 82.9133 110.468 Q85.8762 108.06 91.0151 108.06 Q96.1771 108.06 99.1169 110.468 Q102.057 112.875 102.057 117.019 Q102.057 119.982 100.367 122.042 Q98.7002 124.102 95.7141 124.843 Q99.0937 125.63 100.969 127.921 Q102.867 130.213 102.867 133.523 Q102.867 138.546 99.7882 141.232 Q96.7326 143.917 91.0151 143.917 Q85.2975 143.917 82.2188 141.232 Q79.1633 138.546 79.1633 133.523 Q79.1633 130.213 81.0614 127.921 Q82.9595 125.63 86.3392 124.843 M84.6262 117.459 Q84.6262 120.144 86.2929 121.648 Q87.9827 123.153 91.0151 123.153 Q94.0243 123.153 95.7141 121.648 Q97.4271 120.144 97.4271 117.459 Q97.4271 114.773 95.7141 113.269 Q94.0243 111.764 91.0151 111.764 Q87.9827 111.764 86.2929 113.269 Q84.6262 114.773 84.6262 117.459 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip972)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="201.524,86.1857 222.621,250.224 243.718,396.093 264.814,561.144 285.911,683.737 307.008,750.524 328.104,801.063 349.201,834.296 370.298,870.251 391.395,897.538 412.491,924.724 433.588,954.151 454.685,976.772 475.781,1002.35 496.878,1020.83 517.975,1042.57 539.071,1058.87 560.168,1076.6 581.265,1094.52 602.362,1106.7 623.458,1120.46 644.555,1134.98 665.652,1143.47 686.748,1156.22 707.845,1166.59 728.942,1174.41 750.039,1183.23 771.135,1193.75 792.232,1198.36 813.329,1208 834.425,1214.79 855.522,1218.71 876.619,1226.84 897.716,1231.36 918.812,1237.92 939.909,1242.41 961.006,1247.07 982.102,1251.82 1003.2,1256.87 1024.3,1260.86 1045.39,1265.19 1066.49,1270.37 1087.59,1273.38 1108.68,1277.86 1129.78,1279.72 1150.88,1285.84 1171.97,1287.63 1193.07,1291.76 1214.17,1294.9 1235.26,1297.72 1256.36,1301.28 1277.46,1304.4 1298.55,1306.59 1319.65,1308.77 1340.75,1313.28 1361.84,1315.28 1382.94,1317.39 1404.04,1319.51 1425.13,1320.77 1446.23,1324.87 1467.33,1326.8 1488.42,1328.19 1509.52,1330.46 1530.62,1333.36 1551.71,1334.07 1572.81,1337.45 1593.91,1339.34 1615,1340.76 1636.1,1342.11 1657.2,1344.03 1678.29,1346.6 1699.39,1347.95 1720.49,1349.27 1741.58,1351.28 1762.68,1353.03 1783.78,1354.48 1804.87,1354.98 1825.97,1357.96 1847.07,1359.6 1868.16,1360.47 1889.26,1361.38 1910.36,1362.73 1931.45,1364.3 1952.55,1366.16 1973.65,1367.04 1994.74,1368.27 2015.84,1370.1 2036.94,1370.5 2058.03,1371.96 2079.13,1373.3 2100.23,1374.45 2121.32,1375.31 2142.42,1376.72 2163.52,1378.09 2184.62,1379.63 2205.71,1379.9 2226.81,1381.37 2247.91,1382.51 2269,1383.93 2290.1,1384.24 "/>
<polyline clip-path="url(#clip972)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="201.524,237.889 222.621,345.67 243.718,504.092 264.814,656.417 285.911,747.932 307.008,807.538 328.104,827.366 349.201,845.982 370.298,874.558 391.395,897.994 412.491,910.676 433.588,934.177 454.685,953.902 475.781,981.243 496.878,989.064 517.975,1014.81 539.071,1021.67 560.168,1036.38 581.265,1048.18 602.362,1051.74 623.458,1067.84 644.555,1077.62 665.652,1077.65 686.748,1085.8 707.845,1095.77 728.942,1088.67 750.039,1093.82 771.135,1101.14 792.232,1104.4 813.329,1120.47 834.425,1119.99 855.522,1122.48 876.619,1129.76 897.716,1127.88 918.812,1128.12 939.909,1130.54 961.006,1140.17 982.102,1132.63 1003.2,1139.64 1024.3,1132.8 1045.39,1137.63 1066.49,1144.23 1087.59,1147.26 1108.68,1145.04 1129.78,1149.14 1150.88,1142.51 1171.97,1141.77 1193.07,1154.74 1214.17,1148.49 1235.26,1147.18 1256.36,1154.04 1277.46,1157.94 1298.55,1142.48 1319.65,1150.87 1340.75,1145.55 1361.84,1134.74 1382.94,1148.73 1404.04,1152.64 1425.13,1151.68 1446.23,1145.24 1467.33,1152.57 1488.42,1159.19 1509.52,1148.34 1530.62,1147.1 1551.71,1149.62 1572.81,1145.29 1593.91,1139.96 1615,1147.33 1636.1,1141.66 1657.2,1155.48 1678.29,1151.01 1699.39,1148.94 1720.49,1140.73 1741.58,1136 1762.68,1140.48 1783.78,1151.53 1804.87,1146.53 1825.97,1139.6 1847.07,1140.82 1868.16,1144.61 1889.26,1148.68 1910.36,1132.79 1931.45,1137.6 1952.55,1144.27 1973.65,1134.74 1994.74,1144.38 2015.84,1147.39 2036.94,1137.48 2058.03,1143.44 2079.13,1140.73 2100.23,1130.31 2121.32,1133.18 2142.42,1129.18 2163.52,1125.39 2184.62,1132.05 2205.71,1133.16 2226.81,1127.54 2247.91,1118.91 2269,1141.41 2290.1,1119.14 "/>
<path clip-path="url(#clip970)" d="M1855.88 248.629 L2278.96 248.629 L2278.96 93.1086 L1855.88 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1855.88,248.629 2278.96,248.629 2278.96,93.1086 1855.88,93.1086 1855.88,248.629 "/>
<polyline clip-path="url(#clip970)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1880.48,144.949 2028.07,144.949 "/>
<path clip-path="url(#clip970)" d="M2060.08 128.942 L2060.08 136.303 L2068.85 136.303 L2068.85 139.613 L2060.08 139.613 L2060.08 153.687 Q2060.08 156.858 2060.94 157.761 Q2061.82 158.664 2064.48 158.664 L2068.85 158.664 L2068.85 162.229 L2064.48 162.229 Q2059.55 162.229 2057.67 160.4 Q2055.8 158.548 2055.8 153.687 L2055.8 139.613 L2052.67 139.613 L2052.67 136.303 L2055.8 136.303 L2055.8 128.942 L2060.08 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2089.48 140.284 Q2088.76 139.868 2087.9 139.682 Q2087.07 139.474 2086.05 139.474 Q2082.44 139.474 2080.5 141.835 Q2078.57 144.173 2078.57 148.571 L2078.57 162.229 L2074.29 162.229 L2074.29 136.303 L2078.57 136.303 L2078.57 140.331 Q2079.92 137.969 2082.07 136.835 Q2084.22 135.678 2087.3 135.678 Q2087.74 135.678 2088.27 135.747 Q2088.81 135.794 2089.45 135.909 L2089.48 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2105.73 149.196 Q2100.57 149.196 2098.57 150.377 Q2096.58 151.557 2096.58 154.405 Q2096.58 156.673 2098.07 158.016 Q2099.57 159.335 2102.14 159.335 Q2105.68 159.335 2107.81 156.835 Q2109.96 154.312 2109.96 150.145 L2109.96 149.196 L2105.73 149.196 M2114.22 147.437 L2114.22 162.229 L2109.96 162.229 L2109.96 158.293 Q2108.51 160.655 2106.33 161.789 Q2104.15 162.9 2101.01 162.9 Q2097.02 162.9 2094.66 160.678 Q2092.32 158.432 2092.32 154.682 Q2092.32 150.307 2095.24 148.085 Q2098.18 145.863 2103.99 145.863 L2109.96 145.863 L2109.96 145.446 Q2109.96 142.507 2108.02 140.909 Q2106.1 139.289 2102.6 139.289 Q2100.38 139.289 2098.27 139.821 Q2096.17 140.354 2094.22 141.419 L2094.22 137.483 Q2096.56 136.581 2098.76 136.141 Q2100.96 135.678 2103.04 135.678 Q2108.67 135.678 2111.44 138.594 Q2114.22 141.511 2114.22 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2123 136.303 L2127.26 136.303 L2127.26 162.229 L2123 162.229 L2123 136.303 M2123 126.21 L2127.26 126.21 L2127.26 131.604 L2123 131.604 L2123 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2157.72 146.581 L2157.72 162.229 L2153.46 162.229 L2153.46 146.719 Q2153.46 143.039 2152.02 141.21 Q2150.59 139.382 2147.72 139.382 Q2144.27 139.382 2142.28 141.581 Q2140.29 143.78 2140.29 147.576 L2140.29 162.229 L2136 162.229 L2136 136.303 L2140.29 136.303 L2140.29 140.331 Q2141.82 137.993 2143.88 136.835 Q2145.96 135.678 2148.67 135.678 Q2153.13 135.678 2155.43 138.456 Q2157.72 141.21 2157.72 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2185.91 170.099 L2185.91 173.409 L2161.28 173.409 L2161.28 170.099 L2185.91 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2194.04 158.34 L2194.04 172.09 L2189.75 172.09 L2189.75 136.303 L2194.04 136.303 L2194.04 140.238 Q2195.38 137.923 2197.42 136.812 Q2199.48 135.678 2202.32 135.678 Q2207.05 135.678 2209.99 139.428 Q2212.95 143.178 2212.95 149.289 Q2212.95 155.4 2209.99 159.15 Q2207.05 162.9 2202.32 162.9 Q2199.48 162.9 2197.42 161.789 Q2195.38 160.655 2194.04 158.34 M2208.53 149.289 Q2208.53 144.59 2206.58 141.928 Q2204.66 139.243 2201.28 139.243 Q2197.9 139.243 2195.96 141.928 Q2194.04 144.59 2194.04 149.289 Q2194.04 153.988 2195.96 156.673 Q2197.9 159.335 2201.28 159.335 Q2204.66 159.335 2206.58 156.673 Q2208.53 153.988 2208.53 149.289 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2224.13 158.34 L2224.13 172.09 L2219.85 172.09 L2219.85 136.303 L2224.13 136.303 L2224.13 140.238 Q2225.47 137.923 2227.51 136.812 Q2229.57 135.678 2232.42 135.678 Q2237.14 135.678 2240.08 139.428 Q2243.04 143.178 2243.04 149.289 Q2243.04 155.4 2240.08 159.15 Q2237.14 162.9 2232.42 162.9 Q2229.57 162.9 2227.51 161.789 Q2225.47 160.655 2224.13 158.34 M2238.62 149.289 Q2238.62 144.59 2236.68 141.928 Q2234.75 139.243 2231.37 139.243 Q2228 139.243 2226.05 141.928 Q2224.13 144.59 2224.13 149.289 Q2224.13 153.988 2226.05 156.673 Q2228 159.335 2231.37 159.335 Q2234.75 159.335 2236.68 156.673 Q2238.62 153.988 2238.62 149.289 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2250.1 126.21 L2254.36 126.21 L2254.36 162.229 L2250.1 162.229 L2250.1 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip970)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1880.48,196.789 2028.07,196.789 "/>
<path clip-path="url(#clip970)" d="M2052.67 188.143 L2057.19 188.143 L2065.29 209.902 L2073.39 188.143 L2077.9 188.143 L2068.18 214.069 L2062.39 214.069 L2052.67 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2095.57 201.036 Q2090.4 201.036 2088.41 202.217 Q2086.42 203.397 2086.42 206.245 Q2086.42 208.513 2087.9 209.856 Q2089.41 211.175 2091.98 211.175 Q2095.52 211.175 2097.65 208.675 Q2099.8 206.152 2099.8 201.985 L2099.8 201.036 L2095.57 201.036 M2104.06 199.277 L2104.06 214.069 L2099.8 214.069 L2099.8 210.133 Q2098.34 212.495 2096.17 213.629 Q2093.99 214.74 2090.84 214.74 Q2086.86 214.74 2084.5 212.518 Q2082.16 210.272 2082.16 206.522 Q2082.16 202.147 2085.08 199.925 Q2088.02 197.703 2093.83 197.703 L2099.8 197.703 L2099.8 197.286 Q2099.8 194.347 2097.86 192.749 Q2095.94 191.129 2092.44 191.129 Q2090.22 191.129 2088.11 191.661 Q2086.01 192.194 2084.06 193.259 L2084.06 189.323 Q2086.4 188.421 2088.6 187.981 Q2090.8 187.518 2092.88 187.518 Q2098.51 187.518 2101.28 190.434 Q2104.06 193.351 2104.06 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2112.83 178.05 L2117.09 178.05 L2117.09 214.069 L2112.83 214.069 L2112.83 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2145.7 221.939 L2145.7 225.249 L2121.07 225.249 L2121.07 221.939 L2145.7 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2153.83 210.18 L2153.83 223.93 L2149.55 223.93 L2149.55 188.143 L2153.83 188.143 L2153.83 192.078 Q2155.17 189.763 2157.21 188.652 Q2159.27 187.518 2162.12 187.518 Q2166.84 187.518 2169.78 191.268 Q2172.74 195.018 2172.74 201.129 Q2172.74 207.24 2169.78 210.99 Q2166.84 214.74 2162.12 214.74 Q2159.27 214.74 2157.21 213.629 Q2155.17 212.495 2153.83 210.18 M2168.32 201.129 Q2168.32 196.43 2166.38 193.768 Q2164.45 191.083 2161.07 191.083 Q2157.69 191.083 2155.75 193.768 Q2153.83 196.43 2153.83 201.129 Q2153.83 205.828 2155.75 208.513 Q2157.69 211.175 2161.07 211.175 Q2164.45 211.175 2166.38 208.513 Q2168.32 205.828 2168.32 201.129 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2183.92 210.18 L2183.92 223.93 L2179.64 223.93 L2179.64 188.143 L2183.92 188.143 L2183.92 192.078 Q2185.26 189.763 2187.3 188.652 Q2189.36 187.518 2192.21 187.518 Q2196.93 187.518 2199.87 191.268 Q2202.83 195.018 2202.83 201.129 Q2202.83 207.24 2199.87 210.99 Q2196.93 214.74 2192.21 214.74 Q2189.36 214.74 2187.3 213.629 Q2185.26 212.495 2183.92 210.18 M2198.41 201.129 Q2198.41 196.43 2196.47 193.768 Q2194.55 191.083 2191.17 191.083 Q2187.79 191.083 2185.84 193.768 Q2183.92 196.43 2183.92 201.129 Q2183.92 205.828 2185.84 208.513 Q2187.79 211.175 2191.17 211.175 Q2194.55 211.175 2196.47 208.513 Q2198.41 205.828 2198.41 201.129 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2209.89 178.05 L2214.15 178.05 L2214.15 214.069 L2209.89 214.069 L2209.89 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="language-julia hljs">prefix = &quot;it has&quot;
d2lai.prediction(prefix, m[1], data.vocab, 20; state = (zeros(num_hiddens), zeros(num_hiddens)))</code></pre><pre><code class="nohighlight hljs">&quot;it has of the then the tim&quot;</code></pre><p>LSTMs are the prototypical latent variable autoregressive model with nontrivial state control. Many variants thereof have been proposed over the years, e.g., multiple layers, residual connections, different types of regularization. However, training LSTMs and other sequence models (such as GRUs) is quite costly because of the long range dependency of the sequence. Later we will encounter alternative models such as Transformers that can be used in some cases.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>While LSTMs were published in 1997,  they rose to great prominence  with some victories in prediction competitions in the mid-2000s, and became the dominant models for sequence learning from 2011  until the rise of Transformer models, starting in 2017. Even Tranformers owe some of their key ideas  to architecture design innovations introduced by the LSTM.</p><p>LSTMs have three types of gates:  input gates, forget gates, and output gates  that control the flow of information. The hidden layer output of LSTM includes the hidden state and the memory cell internal state.  Only the hidden state is passed into the output layer while  the memory cell internal state remains entirely internal. LSTMs can alleviate vanishing and exploding gradients.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Adjust the hyperparameters and analyze their influence on running time, perplexity, and the output sequence.</li><li>How would you need to change the model to generate proper words rather than just sequences of characters?</li><li>Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost.</li><li>Since the candidate memory cell ensures that the value range is between <span>$-1$</span> and <span>$1$</span> by  using the <span>$\tanh$</span> function, why does the hidden state need to use the <span>$\tanh$</span> function again to ensure that the output value range is between <span>$-1$</span> and <span>$1$</span>?</li><li>Implement an LSTM model for time series prediction rather than character sequence prediction.</li></ol><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MRNN7/">« Sequence-to-Sequence Learning for Machine Translation</a><a class="docs-footer-nextpage" href="../MRNN_2/">Gated Recurrent Units (GRU) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
