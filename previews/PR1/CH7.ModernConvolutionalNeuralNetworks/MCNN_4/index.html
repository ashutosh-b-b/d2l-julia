<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multi-Branch Networks  (GoogLeNet) · d2l Julia</title><meta name="title" content="Multi-Branch Networks  (GoogLeNet) · d2l Julia"/><meta property="og:title" content="Multi-Branch Networks  (GoogLeNet) · d2l Julia"/><meta property="twitter:title" content="Multi-Branch Networks  (GoogLeNet) · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../MCNN_3/">-</a></li><li class="is-active"><a class="tocitem" href>Multi-Branch Networks  (GoogLeNet)</a><ul class="internal"><li><a class="tocitem" href="#**Inception-Blocks**"><span><strong>Inception Blocks</strong></span></a></li><li><a class="tocitem" href="#**GoogLeNet-Model**"><span><strong>GoogLeNet Model</strong></span></a></li><li><a class="tocitem" href="#Discussion"><span>Discussion</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../MCNN_5/">-</a></li><li><a class="tocitem" href="../MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li><a class="tocitem" href="../MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Modern Convolutional Neural Networks</a></li><li class="is-active"><a href>Multi-Branch Networks  (GoogLeNet)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multi-Branch Networks  (GoogLeNet)</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Multi-Branch-Networks-(GoogLeNet)"><a class="docs-heading-anchor" href="#Multi-Branch-Networks-(GoogLeNet)">Multi-Branch Networks  (GoogLeNet)</a><a id="Multi-Branch-Networks-(GoogLeNet)-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Branch-Networks-(GoogLeNet)" title="Permalink"></a></h1><p>:label:<code>sec_googlenet</code></p><p>In 2014, <em>GoogLeNet</em> won the ImageNet Challenge :cite:<code>Szegedy.Liu.Jia.ea.2015</code>, using a structure that combined the strengths of NiN :cite:<code>Lin.Chen.Yan.2013</code>, repeated blocks :cite:<code>Simonyan.Zisserman.2014</code>, and a cocktail of convolution kernels. It was arguably also the first network that exhibited a clear distinction among the stem (data ingest), body (data processing), and head (prediction) in a CNN. This design pattern has persisted ever since in the design of deep networks: the <em>stem</em> is given by the first two or three convolutions that operate on the image. They extract low-level features from the underlying images. This is followed by a <em>body</em> of convolutional blocks. Finally, the <em>head</em> maps the features obtained so far to the required classification, segmentation, detection, or tracking problem at hand.</p><p>The key contribution in GoogLeNet was the design of the network body. It solved the problem of selecting convolution kernels in an ingenious way. While other works tried to identify which convolution, ranging from <span>$1 \times 1$</span> to <span>$11 \times 11$</span> would be best, it simply <em>concatenated</em> multi-branch convolutions. In what follows we introduce a slightly simplified version of GoogLeNet: the original design included a number of tricks for stabilizing training through intermediate loss functions, applied to multiple layers of the network.  They are no longer necessary due to the availability of improved training algorithms.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Flux 
using CUDA, cuDNN</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/d2l-julia/d2lai`</code></pre><h2 id="**Inception-Blocks**"><a class="docs-heading-anchor" href="#**Inception-Blocks**"><strong>Inception Blocks</strong></a><a id="**Inception-Blocks**-1"></a><a class="docs-heading-anchor-permalink" href="#**Inception-Blocks**" title="Permalink"></a></h2><p>The basic convolutional block in GoogLeNet is called an <em>Inception block</em>, stemming from the meme &quot;we need to go deeper&quot; from the movie <em>Inception</em>.</p><p><img src="../../img/inception.svg" alt="Structure of the Inception block."/> :label:<code>fig_inception</code></p><p>As depicted in :numref:<code>fig_inception</code>, the inception block consists of four parallel branches. The first three branches use convolutional layers with window sizes of <span>$1\times 1$</span>, <span>$3\times 3$</span>, and <span>$5\times 5$</span> to extract information from different spatial sizes. The middle two branches also add a <span>$1\times 1$</span> convolution of the input to reduce the number of channels, reducing the model&#39;s complexity. The fourth branch uses a <span>$3\times 3$</span> max-pooling layer, followed by a <span>$1\times 1$</span> convolutional layer to change the number of channels. The four branches all use appropriate padding to give the input and output the same height and width. Finally, the outputs along each branch are concatenated along the channel dimension and comprise the block&#39;s output. The commonly-tuned hyperparameters of the Inception block are the number of output channels per layer, i.e., how to allocate capacity among convolutions of different size.</p><pre><code class="language-julia hljs">struct InceptionBlock{N1, N2, N3, N4} &lt;: AbstractModel 
    net1::N1
    net2::N2
    net3::N3 
    net4::N4
end 

function InceptionBlock(channel_in::Int, channel1::Int, channel2::Tuple, channel3::Tuple, channel4::Int)
    net1 = Chain(
        Conv((1,1), channel_in =&gt; channel1, relu)
    )

    net2 = Chain(
        Conv((1,1), channel_in =&gt; channel2[1], relu),
        Conv((3,3), channel2[1] =&gt; channel2[2], relu, pad = 1)
    )

    net3 = Chain(
        Conv((1,1), channel_in =&gt; channel3[1], relu),
        Conv((5,5), channel3[1] =&gt; channel3[2], relu, pad = 2)
    )

    net4 = Chain(
        MaxPool((3,3), pad = 1, stride = 1),
        Conv((1,1), channel_in =&gt; channel4)
    )
    InceptionBlock(net1, net2, net3, net4)
end

function (ib::InceptionBlock)(x)
    n1 = ib.net1(x)
    n2 = ib.net2(x)
    n3 = ib.net3(x)
    n4 = ib.net4(x)
    cat(n1, n2, n3, n4; dims = 3)
end

Flux.@layer InceptionBlock</code></pre><p>To gain some intuition for why this network works so well, consider the combination of the filters. They explore the image in a variety of filter sizes. This means that details at different extents can be recognized efficiently by filters of different sizes. At the same time, we can allocate different amounts of parameters for different filters.</p><h2 id="**GoogLeNet-Model**"><a class="docs-heading-anchor" href="#**GoogLeNet-Model**"><strong>GoogLeNet Model</strong></a><a id="**GoogLeNet-Model**-1"></a><a class="docs-heading-anchor-permalink" href="#**GoogLeNet-Model**" title="Permalink"></a></h2><p>As shown in :numref:<code>fig_inception_full</code>, GoogLeNet uses a stack of a total of 9 inception blocks, arranged into three groups with max-pooling in between, and global average pooling in its head to generate its estimates. Max-pooling between inception blocks reduces the dimensionality. At its stem, the first module is similar to AlexNet and LeNet.</p><p><img src="../../img/inception-full-90.svg" alt="The GoogLeNet architecture."/> :label:<code>fig_inception_full</code></p><p>We can now implement GoogLeNet piece by piece. Let&#39;s begin with the stem. The first module uses a 64-channel <span>$7\times 7$</span> convolutional layer.</p><pre><code class="language-julia hljs">function b1(channel_in = 1)
        Chain(
        Conv((7,7), channel_in =&gt; 64, pad = 3, stride = 2),
        MaxPool((3,3), stride = 2, pad = 1),        
    )
end</code></pre><pre><code class="nohighlight hljs">b1 (generic function with 2 methods)</code></pre><p>The second module uses two convolutional layers: first, a 64-channel <span>$1\times 1$</span> convolutional layer, followed by a <span>$3\times 3$</span> convolutional layer that triples the number of channels. This corresponds to the second branch in the Inception block and concludes the design of the body. At this point we have 192 channels.</p><pre><code class="language-julia hljs">function b2(channel_in = 64)
    Chain(
        Conv((1,1), channel_in =&gt; 64, relu),
        Conv((3,3), 64 =&gt; 192, relu),
        MaxPool((3,3), pad = 1, stride = 2)
        
    )
end</code></pre><pre><code class="nohighlight hljs">b2 (generic function with 2 methods)</code></pre><p>The third module connects two complete Inception blocks in series. The number of output channels of the first Inception block is <span>$64+128+32+32=256$</span>. This amounts to  a ratio of the number of output channels among the four branches of <span>$2:4:1:1$</span>. To achieve this, we first reduce the input dimensions by <span>$\frac{1}{2}$</span> and by <span>$\frac{1}{12}$</span> in the second and third branch respectively to arrive at <span>$96 = 192/2$</span> and <span>$16 = 192/12$</span> channels respectively.</p><p>The number of output channels of the second Inception block is increased to <span>$128+192+96+64=480$</span>, yielding a ratio of <span>$128:192:96:64 = 4:6:3:2$</span>. As before, we need to reduce the number of intermediate dimensions in the second and third channel. A scale of <span>$\frac{1}{2}$</span> and <span>$\frac{1}{8}$</span> respectively suffices, yielding <span>$128$</span> and <span>$32$</span> channels respectively. This is captured by the arguments of the following <code>Inception</code> block constructors.</p><pre><code class="language-julia hljs">function b3(channel_in = 192)
    Chain(
        InceptionBlock(channel_in, 64, (96, 128), (16, 32), 32),
        InceptionBlock(256, 128, (128, 192), (32, 96), 64),
        MaxPool((3,3), stride = 2, pad = 1),
    )
end</code></pre><pre><code class="nohighlight hljs">b3 (generic function with 2 methods)</code></pre><p>The fourth module is more complicated. It connects five Inception blocks in series, and they have <span>$192+208+48+64=512$</span>, <span>$160+224+64+64=512$</span>, <span>$128+256+64+64=512$</span>, <span>$112+288+64+64=528$</span>, and <span>$256+320+128+128=832$</span> output channels, respectively. The number of channels assigned to these branches is similar to that in the third module: the second branch with the <span>$3\times 3$</span> convolutional layer outputs the largest number of channels, followed by the first branch with only the <span>$1\times 1$</span> convolutional layer, the third branch with the <span>$5\times 5$</span> convolutional layer, and the fourth branch with the <span>$3\times 3$</span> max-pooling layer. The second and third branches will first reduce the number of channels according to the ratio. These ratios are slightly different in different Inception blocks.</p><pre><code class="language-julia hljs">function b4(channel_in = 480)
    Chain(
        InceptionBlock(channel_in, 192, (96, 208), (16, 48), 64),
        InceptionBlock(512, 160, (112, 224), (24, 64), 64),
        InceptionBlock(512, 128, (128, 256), (24, 64), 64),
        InceptionBlock(512, 112, (144, 288), (32, 64), 64),
        InceptionBlock(528, 256, (160, 320), (32, 128), 128),
        MaxPool((3,3), stride = 2, pad = 1)
    )
end</code></pre><pre><code class="nohighlight hljs">b4 (generic function with 2 methods)</code></pre><p>The fifth module has two Inception blocks with <span>$256+320+128+128=832$</span> and <span>$384+384+128+128=1024$</span> output channels. The number of channels assigned to each branch is the same as that in the third and fourth modules, but differs in specific values. It should be noted that the fifth block is followed by the output layer. This block uses the global average pooling layer to change the height and width of each channel to 1, just as in NiN. Finally, we turn the output into a two-dimensional array followed by a fully connected layer whose number of outputs is the number of label classes.</p><pre><code class="language-julia hljs">function b5(channel_in = 832)
    Chain(
        InceptionBlock(channel_in, 256, (160, 320), (32, 128), 128),
        InceptionBlock(832, 384, (192, 384), (48, 128), 128),
        GlobalMeanPool(),
        Flux.flatten,
    )
end</code></pre><pre><code class="nohighlight hljs">b5 (generic function with 2 methods)</code></pre><pre><code class="language-julia hljs">struct GoogLeNet{N} &lt;: AbstractClassifier 
    net::N 
end

function GoogLeNet(num_classes::Int = 10)
    net = Flux.@autosize (96, 96, 1, 1) Chain(
        b1(),
        b2(),
        b3(),
        b4(),
        b5(),
        Dense(_ =&gt; num_classes),
        softmax
    )
    GoogLeNet(net)
end

Flux.@layer GoogLeNet 

(gn::GoogLeNet)(x) = gn.net(x)</code></pre><pre><code class="language-julia hljs">model = GoogLeNet()
model.net</code></pre><pre><code class="nohighlight hljs">Chain(
  Chain(
    Conv((7, 7), 1 =&gt; 64, pad=3, stride=2),  # 3_200 parameters
    MaxPool((3, 3), pad=1, stride=2),
  ),
  Chain(
    Conv((1, 1), 64 =&gt; 64, relu),       # 4_160 parameters
    Conv((3, 3), 64 =&gt; 192, relu),      # 110_784 parameters
    MaxPool((3, 3), pad=1, stride=2),
  ),
  Chain(
    InceptionBlock(
      Chain(
        Conv((1, 1), 192 =&gt; 64, relu),  # 12_352 parameters
      ),
      Chain(
        Conv((1, 1), 192 =&gt; 96, relu),  # 18_528 parameters
        Conv((3, 3), 96 =&gt; 128, relu, pad=1),  # 110_720 parameters
      ),
      Chain(
        Conv((1, 1), 192 =&gt; 16, relu),  # 3_088 parameters
        Conv((5, 5), 16 =&gt; 32, relu, pad=2),  # 12_832 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 192 =&gt; 32),        # 6_176 parameters
      ),
    ),
    InceptionBlock(
      Chain(
        Conv((1, 1), 256 =&gt; 128, relu),  # 32_896 parameters
      ),
      Chain(
        Conv((1, 1), 256 =&gt; 128, relu),  # 32_896 parameters
        Conv((3, 3), 128 =&gt; 192, relu, pad=1),  # 221_376 parameters
      ),
      Chain(
        Conv((1, 1), 256 =&gt; 32, relu),  # 8_224 parameters
        Conv((5, 5), 32 =&gt; 96, relu, pad=2),  # 76_896 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 256 =&gt; 64),        # 16_448 parameters
      ),
    ),
    MaxPool((3, 3), pad=1, stride=2),
  ),
  Chain(
    InceptionBlock(
      Chain(
        Conv((1, 1), 480 =&gt; 192, relu),  # 92_352 parameters
      ),
      Chain(
        Conv((1, 1), 480 =&gt; 96, relu),  # 46_176 parameters
        Conv((3, 3), 96 =&gt; 208, relu, pad=1),  # 179_920 parameters
      ),
      Chain(
        Conv((1, 1), 480 =&gt; 16, relu),  # 7_696 parameters
        Conv((5, 5), 16 =&gt; 48, relu, pad=2),  # 19_248 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 480 =&gt; 64),        # 30_784 parameters
      ),
    ),
    InceptionBlock(
      Chain(
        Conv((1, 1), 512 =&gt; 160, relu),  # 82_080 parameters
      ),
      Chain(
        Conv((1, 1), 512 =&gt; 112, relu),  # 57_456 parameters
        Conv((3, 3), 112 =&gt; 224, relu, pad=1),  # 226_016 parameters
      ),
      Chain(
        Conv((1, 1), 512 =&gt; 24, relu),  # 12_312 parameters
        Conv((5, 5), 24 =&gt; 64, relu, pad=2),  # 38_464 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 512 =&gt; 64),        # 32_832 parameters
      ),
    ),
    InceptionBlock(
      Chain(
        Conv((1, 1), 512 =&gt; 128, relu),  # 65_664 parameters
      ),
      Chain(
        Conv((1, 1), 512 =&gt; 128, relu),  # 65_664 parameters
        Conv((3, 3), 128 =&gt; 256, relu, pad=1),  # 295_168 parameters
      ),
      Chain(
        Conv((1, 1), 512 =&gt; 24, relu),  # 12_312 parameters
        Conv((5, 5), 24 =&gt; 64, relu, pad=2),  # 38_464 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 512 =&gt; 64),        # 32_832 parameters
      ),
    ),
    InceptionBlock(
      Chain(
        Conv((1, 1), 512 =&gt; 112, relu),  # 57_456 parameters
      ),
      Chain(
        Conv((1, 1), 512 =&gt; 144, relu),  # 73_872 parameters
        Conv((3, 3), 144 =&gt; 288, relu, pad=1),  # 373_536 parameters
      ),
      Chain(
        Conv((1, 1), 512 =&gt; 32, relu),  # 16_416 parameters
        Conv((5, 5), 32 =&gt; 64, relu, pad=2),  # 51_264 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 512 =&gt; 64),        # 32_832 parameters
      ),
    ),
    InceptionBlock(
      Chain(
        Conv((1, 1), 528 =&gt; 256, relu),  # 135_424 parameters
      ),
      Chain(
        Conv((1, 1), 528 =&gt; 160, relu),  # 84_640 parameters
        Conv((3, 3), 160 =&gt; 320, relu, pad=1),  # 461_120 parameters
      ),
      Chain(
        Conv((1, 1), 528 =&gt; 32, relu),  # 16_928 parameters
        Conv((5, 5), 32 =&gt; 128, relu, pad=2),  # 102_528 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 528 =&gt; 128),       # 67_712 parameters
      ),
    ),
    MaxPool((3, 3), pad=1, stride=2),
  ),
  Chain(
    InceptionBlock(
      Chain(
        Conv((1, 1), 832 =&gt; 256, relu),  # 213_248 parameters
      ),
      Chain(
        Conv((1, 1), 832 =&gt; 160, relu),  # 133_280 parameters
        Conv((3, 3), 160 =&gt; 320, relu, pad=1),  # 461_120 parameters
      ),
      Chain(
        Conv((1, 1), 832 =&gt; 32, relu),  # 26_656 parameters
        Conv((5, 5), 32 =&gt; 128, relu, pad=2),  # 102_528 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 832 =&gt; 128),       # 106_624 parameters
      ),
    ),
    InceptionBlock(
      Chain(
        Conv((1, 1), 832 =&gt; 384, relu),  # 319_872 parameters
      ),
      Chain(
        Conv((1, 1), 832 =&gt; 192, relu),  # 159_936 parameters
        Conv((3, 3), 192 =&gt; 384, relu, pad=1),  # 663_936 parameters
      ),
      Chain(
        Conv((1, 1), 832 =&gt; 48, relu),  # 39_984 parameters
        Conv((5, 5), 48 =&gt; 128, relu, pad=2),  # 153_728 parameters
      ),
      Chain(
        MaxPool((3, 3), pad=1, stride=1),
        Conv((1, 1), 832 =&gt; 128),       # 106_624 parameters
      ),
    ),
    GlobalMeanPool(),
    Flux.flatten,
  ),
  Dense(1024 =&gt; 10),                    # 10_250 parameters
  NNlib.softmax,
)                   # Total: 116 arrays, 5_977_530 parameters, 22.814 MiB.</code></pre><pre><code class="language-julia hljs">data = d2lai.FashionMNISTData(batchsize = 128, resize = (96, 96))
opt = Descent(0.01)
trainer = Trainer(model, data, opt; max_epochs = 20, gpu = true, board_yscale = :identity)
d2lai.fit(trainer);</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 1.9313097, Val Loss: 1.971195, Val Acc: 0.5
    [ Info: Train Loss: 0.6317029, Val Loss: 0.5401543, Val Acc: 0.75
    [ Info: Train Loss: 0.350438, Val Loss: 0.3720257, Val Acc: 0.8125
    [ Info: Train Loss: 0.41230914, Val Loss: 0.45135814, Val Acc: 0.8125
    [ Info: Train Loss: 0.48061773, Val Loss: 0.36486176, Val Acc: 0.875
    [ Info: Train Loss: 0.41049662, Val Loss: 0.47697634, Val Acc: 0.8125
    [ Info: Train Loss: 0.24872112, Val Loss: 0.38211867, Val Acc: 0.8125
    [ Info: Train Loss: 0.4497632, Val Loss: 0.4631205, Val Acc: 0.8125
    [ Info: Train Loss: 0.3283075, Val Loss: 0.43611112, Val Acc: 0.75
    [ Info: Train Loss: 0.37559816, Val Loss: 0.2629991, Val Acc: 0.8125
    [ Info: Train Loss: 0.27002808, Val Loss: 0.274528, Val Acc: 0.9375
    [ Info: Train Loss: 0.17974333, Val Loss: 0.4215269, Val Acc: 0.75
    [ Info: Train Loss: 0.31861034, Val Loss: 0.3670521, Val Acc: 0.75
    [ Info: Train Loss: 0.3217962, Val Loss: 0.35750878, Val Acc: 0.75
    [ Info: Train Loss: 0.18585555, Val Loss: 0.30299282, Val Acc: 0.8125
    [ Info: Train Loss: 0.23938179, Val Loss: 0.16103698, Val Acc: 1.0
    [ Info: Train Loss: 0.22150195, Val Loss: 0.2737314, Val Acc: 0.8125
    [ Info: Train Loss: 0.13171202, Val Loss: 0.19645679, Val Acc: 1.0
    [ Info: Train Loss: 0.15079072, Val Loss: 0.23048335, Val Acc: 0.875
    [ Info: Train Loss: 0.2121826, Val Loss: 0.18417183, Val Acc: 0.9375</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip570">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip570)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip571">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip570)" d="M155.765 1423.18 L2352.76 1423.18 L2352.76 47.2441 L155.765 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip572">
    <rect x="155" y="47" width="2198" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="654.288,1423.18 654.288,47.2441 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1199.72,1423.18 1199.72,47.2441 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1745.15,1423.18 1745.15,47.2441 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2290.58,1423.18 2290.58,47.2441 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="155.765,1193.39 2352.76,1193.39 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="155.765,879.865 2352.76,879.865 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="155.765,566.337 2352.76,566.337 "/>
<polyline clip-path="url(#clip572)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="155.765,252.809 2352.76,252.809 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="155.765,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="654.288,1423.18 654.288,1404.28 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1199.72,1423.18 1199.72,1404.28 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1745.15,1423.18 1745.15,1404.28 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2290.58,1423.18 2290.58,1404.28 "/>
<path clip-path="url(#clip570)" d="M644.566 1451.02 L662.922 1451.02 L662.922 1454.96 L648.848 1454.96 L648.848 1463.43 Q649.866 1463.08 650.885 1462.92 Q651.903 1462.73 652.922 1462.73 Q658.709 1462.73 662.089 1465.9 Q665.468 1469.08 665.468 1474.49 Q665.468 1480.07 661.996 1483.17 Q658.524 1486.25 652.204 1486.25 Q650.028 1486.25 647.76 1485.88 Q645.515 1485.51 643.107 1484.77 L643.107 1480.07 Q645.191 1481.2 647.413 1481.76 Q649.635 1482.32 652.112 1482.32 Q656.116 1482.32 658.454 1480.21 Q660.792 1478.1 660.792 1474.49 Q660.792 1470.88 658.454 1468.77 Q656.116 1466.67 652.112 1466.67 Q650.237 1466.67 648.362 1467.08 Q646.51 1467.5 644.566 1468.38 L644.566 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1174.41 1481.64 L1182.04 1481.64 L1182.04 1455.28 L1173.73 1456.95 L1173.73 1452.69 L1182 1451.02 L1186.67 1451.02 L1186.67 1481.64 L1194.31 1481.64 L1194.31 1485.58 L1174.41 1485.58 L1174.41 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1213.76 1454.1 Q1210.15 1454.1 1208.32 1457.66 Q1206.51 1461.2 1206.51 1468.33 Q1206.51 1475.44 1208.32 1479.01 Q1210.15 1482.55 1213.76 1482.55 Q1217.39 1482.55 1219.2 1479.01 Q1221.03 1475.44 1221.03 1468.33 Q1221.03 1461.2 1219.2 1457.66 Q1217.39 1454.1 1213.76 1454.1 M1213.76 1450.39 Q1219.57 1450.39 1222.62 1455 Q1225.7 1459.58 1225.7 1468.33 Q1225.7 1477.06 1222.62 1481.67 Q1219.57 1486.25 1213.76 1486.25 Q1207.95 1486.25 1204.87 1481.67 Q1201.81 1477.06 1201.81 1468.33 Q1201.81 1459.58 1204.87 1455 Q1207.95 1450.39 1213.76 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1720.33 1481.64 L1727.97 1481.64 L1727.97 1455.28 L1719.66 1456.95 L1719.66 1452.69 L1727.93 1451.02 L1732.6 1451.02 L1732.6 1481.64 L1740.24 1481.64 L1740.24 1485.58 L1720.33 1485.58 L1720.33 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1749.73 1451.02 L1768.09 1451.02 L1768.09 1454.96 L1754.01 1454.96 L1754.01 1463.43 Q1755.03 1463.08 1756.05 1462.92 Q1757.07 1462.73 1758.09 1462.73 Q1763.87 1462.73 1767.25 1465.9 Q1770.63 1469.08 1770.63 1474.49 Q1770.63 1480.07 1767.16 1483.17 Q1763.69 1486.25 1757.37 1486.25 Q1755.19 1486.25 1752.92 1485.88 Q1750.68 1485.51 1748.27 1484.77 L1748.27 1480.07 Q1750.36 1481.2 1752.58 1481.76 Q1754.8 1482.32 1757.28 1482.32 Q1761.28 1482.32 1763.62 1480.21 Q1765.96 1478.1 1765.96 1474.49 Q1765.96 1470.88 1763.62 1468.77 Q1761.28 1466.67 1757.28 1466.67 Q1755.4 1466.67 1753.53 1467.08 Q1751.67 1467.5 1749.73 1468.38 L1749.73 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2269.35 1481.64 L2285.67 1481.64 L2285.67 1485.58 L2263.73 1485.58 L2263.73 1481.64 Q2266.39 1478.89 2270.97 1474.26 Q2275.58 1469.61 2276.76 1468.27 Q2279 1465.74 2279.88 1464.01 Q2280.79 1462.25 2280.79 1460.56 Q2280.79 1457.8 2278.84 1456.07 Q2276.92 1454.33 2273.82 1454.33 Q2271.62 1454.33 2269.16 1455.09 Q2266.73 1455.86 2263.96 1457.41 L2263.96 1452.69 Q2266.78 1451.55 2269.23 1450.97 Q2271.69 1450.39 2273.73 1450.39 Q2279.1 1450.39 2282.29 1453.08 Q2285.48 1455.77 2285.48 1460.26 Q2285.48 1462.39 2284.67 1464.31 Q2283.89 1466.2 2281.78 1468.8 Q2281.2 1469.47 2278.1 1472.69 Q2275 1475.88 2269.35 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2305.48 1454.1 Q2301.87 1454.1 2300.04 1457.66 Q2298.24 1461.2 2298.24 1468.33 Q2298.24 1475.44 2300.04 1479.01 Q2301.87 1482.55 2305.48 1482.55 Q2309.12 1482.55 2310.92 1479.01 Q2312.75 1475.44 2312.75 1468.33 Q2312.75 1461.2 2310.92 1457.66 Q2309.12 1454.1 2305.48 1454.1 M2305.48 1450.39 Q2311.29 1450.39 2314.35 1455 Q2317.43 1459.58 2317.43 1468.33 Q2317.43 1477.06 2314.35 1481.67 Q2311.29 1486.25 2305.48 1486.25 Q2299.67 1486.25 2296.6 1481.67 Q2293.54 1477.06 2293.54 1468.33 Q2293.54 1459.58 2296.6 1455 Q2299.67 1450.39 2305.48 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1174.45 1548.76 L1174.45 1551.62 L1147.52 1551.62 Q1147.91 1557.67 1151.15 1560.85 Q1154.43 1564 1160.25 1564 Q1163.63 1564 1166.78 1563.17 Q1169.96 1562.35 1173.08 1560.69 L1173.08 1566.23 Q1169.93 1567.57 1166.62 1568.27 Q1163.31 1568.97 1159.9 1568.97 Q1151.37 1568.97 1146.38 1564 Q1141.41 1559.04 1141.41 1550.57 Q1141.41 1541.82 1146.12 1536.69 Q1150.87 1531.54 1158.89 1531.54 Q1166.08 1531.54 1170.25 1536.18 Q1174.45 1540.8 1174.45 1548.76 M1168.59 1547.04 Q1168.53 1542.23 1165.89 1539.37 Q1163.28 1536.5 1158.95 1536.5 Q1154.05 1536.5 1151.09 1539.27 Q1148.16 1542.04 1147.71 1547.07 L1168.59 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1189.73 1562.7 L1189.73 1581.6 L1183.84 1581.6 L1183.84 1532.4 L1189.73 1532.4 L1189.73 1537.81 Q1191.57 1534.62 1194.38 1533.1 Q1197.21 1531.54 1201.12 1531.54 Q1207.62 1531.54 1211.66 1536.69 Q1215.73 1541.85 1215.73 1550.25 Q1215.73 1558.65 1211.66 1563.81 Q1207.62 1568.97 1201.12 1568.97 Q1197.21 1568.97 1194.38 1567.44 Q1191.57 1565.88 1189.73 1562.7 M1209.65 1550.25 Q1209.65 1543.79 1206.98 1540.13 Q1204.34 1536.44 1199.69 1536.44 Q1195.04 1536.44 1192.37 1540.13 Q1189.73 1543.79 1189.73 1550.25 Q1189.73 1556.71 1192.37 1560.4 Q1195.04 1564.07 1199.69 1564.07 Q1204.34 1564.07 1206.98 1560.4 Q1209.65 1556.71 1209.65 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1239.25 1536.5 Q1234.54 1536.5 1231.81 1540.19 Q1229.07 1543.85 1229.07 1550.25 Q1229.07 1556.65 1231.77 1560.34 Q1234.51 1564 1239.25 1564 Q1243.93 1564 1246.67 1560.31 Q1249.41 1556.62 1249.41 1550.25 Q1249.41 1543.92 1246.67 1540.23 Q1243.93 1536.5 1239.25 1536.5 M1239.25 1531.54 Q1246.89 1531.54 1251.25 1536.5 Q1255.61 1541.47 1255.61 1550.25 Q1255.61 1559 1251.25 1564 Q1246.89 1568.97 1239.25 1568.97 Q1231.58 1568.97 1227.22 1564 Q1222.89 1559 1222.89 1550.25 Q1222.89 1541.47 1227.22 1536.5 Q1231.58 1531.54 1239.25 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1290.97 1533.76 L1290.97 1539.24 Q1288.49 1537.87 1285.98 1537.2 Q1283.49 1536.5 1280.95 1536.5 Q1275.25 1536.5 1272.1 1540.13 Q1268.95 1543.73 1268.95 1550.25 Q1268.95 1556.78 1272.1 1560.4 Q1275.25 1564 1280.95 1564 Q1283.49 1564 1285.98 1563.33 Q1288.49 1562.63 1290.97 1561.26 L1290.97 1566.68 Q1288.52 1567.82 1285.88 1568.39 Q1283.27 1568.97 1280.31 1568.97 Q1272.26 1568.97 1267.52 1563.91 Q1262.77 1558.85 1262.77 1550.25 Q1262.77 1541.53 1267.55 1536.53 Q1272.35 1531.54 1280.69 1531.54 Q1283.4 1531.54 1285.98 1532.11 Q1288.56 1532.65 1290.97 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1330.79 1546.53 L1330.79 1568.04 L1324.94 1568.04 L1324.94 1546.72 Q1324.94 1541.66 1322.96 1539.14 Q1320.99 1536.63 1317.04 1536.63 Q1312.3 1536.63 1309.56 1539.65 Q1306.83 1542.68 1306.83 1547.9 L1306.83 1568.04 L1300.94 1568.04 L1300.94 1518.52 L1306.83 1518.52 L1306.83 1537.93 Q1308.93 1534.72 1311.76 1533.13 Q1314.62 1531.54 1318.35 1531.54 Q1324.49 1531.54 1327.64 1535.36 Q1330.79 1539.14 1330.79 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M1365.2 1533.45 L1365.2 1538.98 Q1362.72 1537.71 1360.04 1537.07 Q1357.37 1536.44 1354.5 1536.44 Q1350.14 1536.44 1347.95 1537.77 Q1345.78 1539.11 1345.78 1541.79 Q1345.78 1543.82 1347.34 1545 Q1348.9 1546.15 1353.61 1547.2 L1355.62 1547.64 Q1361.86 1548.98 1364.47 1551.43 Q1367.11 1553.85 1367.11 1558.21 Q1367.11 1563.17 1363.16 1566.07 Q1359.25 1568.97 1352.37 1568.97 Q1349.51 1568.97 1346.39 1568.39 Q1343.3 1567.85 1339.86 1566.74 L1339.86 1560.69 Q1343.11 1562.38 1346.26 1563.24 Q1349.41 1564.07 1352.5 1564.07 Q1356.64 1564.07 1358.86 1562.66 Q1361.09 1561.23 1361.09 1558.65 Q1361.09 1556.27 1359.47 1554.99 Q1357.88 1553.72 1352.44 1552.54 L1350.4 1552.07 Q1344.96 1550.92 1342.54 1548.56 Q1340.12 1546.18 1340.12 1542.04 Q1340.12 1537.01 1343.68 1534.27 Q1347.25 1531.54 1353.8 1531.54 Q1357.05 1531.54 1359.92 1532.01 Q1362.78 1532.49 1365.2 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="155.765,1423.18 155.765,47.2441 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="155.765,1193.39 174.663,1193.39 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="155.765,879.865 174.663,879.865 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="155.765,566.337 174.663,566.337 "/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="155.765,252.809 174.663,252.809 "/>
<path clip-path="url(#clip570)" d="M63.5847 1179.19 Q59.9736 1179.19 58.1449 1182.76 Q56.3393 1186.3 56.3393 1193.43 Q56.3393 1200.53 58.1449 1204.1 Q59.9736 1207.64 63.5847 1207.64 Q67.2189 1207.64 69.0244 1204.1 Q70.8531 1200.53 70.8531 1193.43 Q70.8531 1186.3 69.0244 1182.76 Q67.2189 1179.19 63.5847 1179.19 M63.5847 1175.49 Q69.3948 1175.49 72.4503 1180.1 Q75.529 1184.68 75.529 1193.43 Q75.529 1202.16 72.4503 1206.76 Q69.3948 1211.34 63.5847 1211.34 Q57.7745 1211.34 54.6958 1206.76 Q51.6403 1202.16 51.6403 1193.43 Q51.6403 1184.68 54.6958 1180.1 Q57.7745 1175.49 63.5847 1175.49 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M83.7466 1204.79 L88.6308 1204.79 L88.6308 1210.67 L83.7466 1210.67 L83.7466 1204.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M98.8622 1176.11 L117.219 1176.11 L117.219 1180.05 L103.145 1180.05 L103.145 1188.52 Q104.163 1188.17 105.182 1188.01 Q106.2 1187.83 107.219 1187.83 Q113.006 1187.83 116.385 1191 Q119.765 1194.17 119.765 1199.59 Q119.765 1205.16 116.293 1208.27 Q112.82 1211.34 106.501 1211.34 Q104.325 1211.34 102.057 1210.97 Q99.8113 1210.6 97.4039 1209.86 L97.4039 1205.16 Q99.4872 1206.3 101.709 1206.85 Q103.932 1207.41 106.408 1207.41 Q110.413 1207.41 112.751 1205.3 Q115.089 1203.2 115.089 1199.59 Q115.089 1195.97 112.751 1193.87 Q110.413 1191.76 106.408 1191.76 Q104.534 1191.76 102.659 1192.18 Q100.807 1192.59 98.8622 1193.47 L98.8622 1176.11 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M53.3995 893.21 L61.0384 893.21 L61.0384 866.845 L52.7282 868.511 L52.7282 864.252 L60.9921 862.585 L65.668 862.585 L65.668 893.21 L73.3068 893.21 L73.3068 897.145 L53.3995 897.145 L53.3995 893.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M82.7512 891.266 L87.6354 891.266 L87.6354 897.145 L82.7512 897.145 L82.7512 891.266 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M107.821 865.664 Q104.209 865.664 102.381 869.229 Q100.575 872.771 100.575 879.9 Q100.575 887.007 102.381 890.571 Q104.209 894.113 107.821 894.113 Q111.455 894.113 113.26 890.571 Q115.089 887.007 115.089 879.9 Q115.089 872.771 113.26 869.229 Q111.455 865.664 107.821 865.664 M107.821 861.96 Q113.631 861.96 116.686 866.567 Q119.765 871.15 119.765 879.9 Q119.765 888.627 116.686 893.233 Q113.631 897.817 107.821 897.817 Q102.01 897.817 98.9317 893.233 Q95.8761 888.627 95.8761 879.9 Q95.8761 871.15 98.9317 866.567 Q102.01 861.96 107.821 861.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M54.3949 579.682 L62.0337 579.682 L62.0337 553.317 L53.7236 554.983 L53.7236 550.724 L61.9874 549.057 L66.6633 549.057 L66.6633 579.682 L74.3022 579.682 L74.3022 583.617 L54.3949 583.617 L54.3949 579.682 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M83.7466 577.738 L88.6308 577.738 L88.6308 583.617 L83.7466 583.617 L83.7466 577.738 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M98.8622 549.057 L117.219 549.057 L117.219 552.992 L103.145 552.992 L103.145 561.465 Q104.163 561.117 105.182 560.955 Q106.2 560.77 107.219 560.77 Q113.006 560.77 116.385 563.942 Q119.765 567.113 119.765 572.529 Q119.765 578.108 116.293 581.21 Q112.82 584.289 106.501 584.289 Q104.325 584.289 102.057 583.918 Q99.8113 583.548 97.4039 582.807 L97.4039 578.108 Q99.4872 579.242 101.709 579.798 Q103.932 580.353 106.408 580.353 Q110.413 580.353 112.751 578.247 Q115.089 576.141 115.089 572.529 Q115.089 568.918 112.751 566.812 Q110.413 564.705 106.408 564.705 Q104.534 564.705 102.659 565.122 Q100.807 565.539 98.8622 566.418 L98.8622 549.057 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M56.6171 266.154 L72.9365 266.154 L72.9365 270.089 L50.9921 270.089 L50.9921 266.154 Q53.6541 263.399 58.2375 258.77 Q62.8439 254.117 64.0245 252.775 Q66.2698 250.251 67.1494 248.515 Q68.0522 246.756 68.0522 245.066 Q68.0522 242.312 66.1078 240.575 Q64.1865 238.839 61.0847 238.839 Q58.8856 238.839 56.4319 239.603 Q54.0014 240.367 51.2236 241.918 L51.2236 237.196 Q54.0477 236.062 56.5014 235.483 Q58.955 234.904 60.9921 234.904 Q66.3624 234.904 69.5568 237.589 Q72.7513 240.275 72.7513 244.765 Q72.7513 246.895 71.9411 248.816 Q71.1541 250.714 69.0476 253.307 Q68.4689 253.978 65.367 257.196 Q62.2652 260.39 56.6171 266.154 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M82.7512 264.21 L87.6354 264.21 L87.6354 270.089 L82.7512 270.089 L82.7512 264.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M107.821 238.608 Q104.209 238.608 102.381 242.173 Q100.575 245.714 100.575 252.844 Q100.575 259.95 102.381 263.515 Q104.209 267.057 107.821 267.057 Q111.455 267.057 113.26 263.515 Q115.089 259.95 115.089 252.844 Q115.089 245.714 113.26 242.173 Q111.455 238.608 107.821 238.608 M107.821 234.904 Q113.631 234.904 116.686 239.511 Q119.765 244.094 119.765 252.844 Q119.765 261.571 116.686 266.177 Q113.631 270.761 107.821 270.761 Q102.01 270.761 98.9317 266.177 Q95.8761 261.571 95.8761 252.844 Q95.8761 244.094 98.9317 239.511 Q102.01 234.904 107.821 234.904 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip572)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="217.944,86.1857 327.03,907.016 436.116,1128.99 545.202,1188.9 654.288,1226.1 763.374,1201.75 872.46,1268.46 981.546,1289.63 1090.63,1305.13 1199.72,1314.45 1308.8,1327.83 1417.89,1337.36 1526.98,1346.03 1636.06,1352.56 1745.15,1360.21 1854.23,1365 1963.32,1370.41 2072.41,1375.61 2181.49,1381.07 2290.58,1384.24 "/>
<polyline clip-path="url(#clip572)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="217.944,289.861 327.03,1035.9 436.116,1118.76 545.202,1163.85 654.288,1151.47 763.374,1153.67 872.46,1217.51 981.546,1184.87 1090.63,1244.22 1199.72,1210.66 1308.8,1258.91 1417.89,1238.95 1526.98,1235.04 1636.06,1261.23 1745.15,1275.19 1854.23,1279.21 1963.32,1283.64 2072.41,1304.49 2181.49,1302.25 2290.58,1282.08 "/>
<polyline clip-path="url(#clip572)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="217.944,1249.64 327.03,1051.39 436.116,1024.04 545.202,1005.38 654.288,1010.03 763.374,1007.36 872.46,988.819 981.546,997.5 1090.63,975.673 1199.72,994.214 1308.8,970.216 1417.89,983.982 1526.98,986.09 1636.06,973.316 1745.15,964.387 1854.23,965.565 1963.32,962.526 2072.41,954.217 2181.49,955.457 2290.58,962.898 "/>
<path clip-path="url(#clip570)" d="M1837.79 300.469 L2279.52 300.469 L2279.52 93.1086 L1837.79 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip570)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1837.79,300.469 2279.52,300.469 2279.52,93.1086 1837.79,93.1086 1837.79,300.469 "/>
<polyline clip-path="url(#clip570)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1862.2,144.949 2008.67,144.949 "/>
<path clip-path="url(#clip570)" d="M2040.48 128.942 L2040.48 136.303 L2049.26 136.303 L2049.26 139.613 L2040.48 139.613 L2040.48 153.687 Q2040.48 156.858 2041.34 157.761 Q2042.22 158.664 2044.88 158.664 L2049.26 158.664 L2049.26 162.229 L2044.88 162.229 Q2039.95 162.229 2038.08 160.4 Q2036.2 158.548 2036.2 153.687 L2036.2 139.613 L2033.08 139.613 L2033.08 136.303 L2036.2 136.303 L2036.2 128.942 L2040.48 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2069.88 140.284 Q2069.16 139.868 2068.31 139.682 Q2067.47 139.474 2066.46 139.474 Q2062.84 139.474 2060.9 141.835 Q2058.98 144.173 2058.98 148.571 L2058.98 162.229 L2054.7 162.229 L2054.7 136.303 L2058.98 136.303 L2058.98 140.331 Q2060.32 137.969 2062.47 136.835 Q2064.63 135.678 2067.71 135.678 Q2068.15 135.678 2068.68 135.747 Q2069.21 135.794 2069.86 135.909 L2069.88 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2086.13 149.196 Q2080.97 149.196 2078.98 150.377 Q2076.99 151.557 2076.99 154.405 Q2076.99 156.673 2078.47 158.016 Q2079.97 159.335 2082.54 159.335 Q2086.08 159.335 2088.21 156.835 Q2090.37 154.312 2090.37 150.145 L2090.37 149.196 L2086.13 149.196 M2094.63 147.437 L2094.63 162.229 L2090.37 162.229 L2090.37 158.293 Q2088.91 160.655 2086.73 161.789 Q2084.56 162.9 2081.41 162.9 Q2077.43 162.9 2075.07 160.678 Q2072.73 158.432 2072.73 154.682 Q2072.73 150.307 2075.65 148.085 Q2078.59 145.863 2084.4 145.863 L2090.37 145.863 L2090.37 145.446 Q2090.37 142.507 2088.42 140.909 Q2086.5 139.289 2083.01 139.289 Q2080.78 139.289 2078.68 139.821 Q2076.57 140.354 2074.63 141.419 L2074.63 137.483 Q2076.96 136.581 2079.16 136.141 Q2081.36 135.678 2083.45 135.678 Q2089.07 135.678 2091.85 138.594 Q2094.63 141.511 2094.63 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2103.4 136.303 L2107.66 136.303 L2107.66 162.229 L2103.4 162.229 L2103.4 136.303 M2103.4 126.21 L2107.66 126.21 L2107.66 131.604 L2103.4 131.604 L2103.4 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2138.12 146.581 L2138.12 162.229 L2133.86 162.229 L2133.86 146.719 Q2133.86 143.039 2132.43 141.21 Q2130.99 139.382 2128.12 139.382 Q2124.67 139.382 2122.68 141.581 Q2120.69 143.78 2120.69 147.576 L2120.69 162.229 L2116.41 162.229 L2116.41 136.303 L2120.69 136.303 L2120.69 140.331 Q2122.22 137.993 2124.28 136.835 Q2126.36 135.678 2129.07 135.678 Q2133.54 135.678 2135.83 138.456 Q2138.12 141.21 2138.12 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2166.32 170.099 L2166.32 173.409 L2141.69 173.409 L2141.69 170.099 L2166.32 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2170.32 126.21 L2174.58 126.21 L2174.58 162.229 L2170.32 162.229 L2170.32 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2193.54 139.289 Q2190.11 139.289 2188.12 141.974 Q2186.13 144.636 2186.13 149.289 Q2186.13 153.942 2188.1 156.627 Q2190.09 159.289 2193.54 159.289 Q2196.94 159.289 2198.93 156.604 Q2200.92 153.918 2200.92 149.289 Q2200.92 144.682 2198.93 141.997 Q2196.94 139.289 2193.54 139.289 M2193.54 135.678 Q2199.09 135.678 2202.26 139.289 Q2205.44 142.9 2205.44 149.289 Q2205.44 155.655 2202.26 159.289 Q2199.09 162.9 2193.54 162.9 Q2187.96 162.9 2184.79 159.289 Q2181.64 155.655 2181.64 149.289 Q2181.64 142.9 2184.79 139.289 Q2187.96 135.678 2193.54 135.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2229.02 137.067 L2229.02 141.094 Q2227.22 140.169 2225.27 139.706 Q2223.33 139.243 2221.25 139.243 Q2218.08 139.243 2216.48 140.215 Q2214.9 141.187 2214.9 143.131 Q2214.9 144.613 2216.04 145.469 Q2217.17 146.303 2220.6 147.067 L2222.06 147.391 Q2226.59 148.363 2228.49 150.145 Q2230.41 151.905 2230.41 155.076 Q2230.41 158.687 2227.54 160.793 Q2224.7 162.9 2219.7 162.9 Q2217.61 162.9 2215.34 162.483 Q2213.1 162.09 2210.6 161.28 L2210.6 156.881 Q2212.96 158.108 2215.25 158.733 Q2217.54 159.335 2219.79 159.335 Q2222.8 159.335 2224.42 158.317 Q2226.04 157.275 2226.04 155.4 Q2226.04 153.664 2224.86 152.738 Q2223.7 151.812 2219.74 150.956 L2218.26 150.608 Q2214.3 149.775 2212.54 148.062 Q2210.78 146.326 2210.78 143.317 Q2210.78 139.659 2213.38 137.669 Q2215.97 135.678 2220.74 135.678 Q2223.1 135.678 2225.18 136.025 Q2227.26 136.372 2229.02 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2253.72 137.067 L2253.72 141.094 Q2251.92 140.169 2249.97 139.706 Q2248.03 139.243 2245.95 139.243 Q2242.77 139.243 2241.18 140.215 Q2239.6 141.187 2239.6 143.131 Q2239.6 144.613 2240.74 145.469 Q2241.87 146.303 2245.3 147.067 L2246.76 147.391 Q2251.29 148.363 2253.19 150.145 Q2255.11 151.905 2255.11 155.076 Q2255.11 158.687 2252.24 160.793 Q2249.39 162.9 2244.39 162.9 Q2242.31 162.9 2240.04 162.483 Q2237.8 162.09 2235.3 161.28 L2235.3 156.881 Q2237.66 158.108 2239.95 158.733 Q2242.24 159.335 2244.49 159.335 Q2247.5 159.335 2249.12 158.317 Q2250.74 157.275 2250.74 155.4 Q2250.74 153.664 2249.56 152.738 Q2248.4 151.812 2244.44 150.956 L2242.96 150.608 Q2239 149.775 2237.24 148.062 Q2235.48 146.326 2235.48 143.317 Q2235.48 139.659 2238.07 137.669 Q2240.67 135.678 2245.44 135.678 Q2247.8 135.678 2249.88 136.025 Q2251.96 136.372 2253.72 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip570)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1862.2,196.789 2008.67,196.789 "/>
<path clip-path="url(#clip570)" d="M2033.08 188.143 L2037.59 188.143 L2045.69 209.902 L2053.79 188.143 L2058.31 188.143 L2048.59 214.069 L2042.8 214.069 L2033.08 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2075.97 201.036 Q2070.81 201.036 2068.82 202.217 Q2066.83 203.397 2066.83 206.245 Q2066.83 208.513 2068.31 209.856 Q2069.81 211.175 2072.38 211.175 Q2075.92 211.175 2078.05 208.675 Q2080.21 206.152 2080.21 201.985 L2080.21 201.036 L2075.97 201.036 M2084.46 199.277 L2084.46 214.069 L2080.21 214.069 L2080.21 210.133 Q2078.75 212.495 2076.57 213.629 Q2074.4 214.74 2071.25 214.74 Q2067.27 214.74 2064.9 212.518 Q2062.57 210.272 2062.57 206.522 Q2062.57 202.147 2065.48 199.925 Q2068.42 197.703 2074.23 197.703 L2080.21 197.703 L2080.21 197.286 Q2080.21 194.347 2078.26 192.749 Q2076.34 191.129 2072.84 191.129 Q2070.62 191.129 2068.52 191.661 Q2066.41 192.194 2064.46 193.259 L2064.46 189.323 Q2066.8 188.421 2069 187.981 Q2071.2 187.518 2073.28 187.518 Q2078.91 187.518 2081.69 190.434 Q2084.46 193.351 2084.46 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2093.24 178.05 L2097.5 178.05 L2097.5 214.069 L2093.24 214.069 L2093.24 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2126.11 221.939 L2126.11 225.249 L2101.48 225.249 L2101.48 221.939 L2126.11 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2130.11 178.05 L2134.37 178.05 L2134.37 214.069 L2130.11 214.069 L2130.11 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2153.33 191.129 Q2149.9 191.129 2147.91 193.814 Q2145.92 196.476 2145.92 201.129 Q2145.92 205.782 2147.89 208.467 Q2149.88 211.129 2153.33 211.129 Q2156.73 211.129 2158.72 208.444 Q2160.71 205.758 2160.71 201.129 Q2160.71 196.522 2158.72 193.837 Q2156.73 191.129 2153.33 191.129 M2153.33 187.518 Q2158.89 187.518 2162.06 191.129 Q2165.23 194.74 2165.23 201.129 Q2165.23 207.495 2162.06 211.129 Q2158.89 214.74 2153.33 214.74 Q2147.75 214.74 2144.58 211.129 Q2141.43 207.495 2141.43 201.129 Q2141.43 194.74 2144.58 191.129 Q2147.75 187.518 2153.33 187.518 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2188.82 188.907 L2188.82 192.934 Q2187.01 192.009 2185.07 191.546 Q2183.12 191.083 2181.04 191.083 Q2177.87 191.083 2176.27 192.055 Q2174.7 193.027 2174.7 194.971 Q2174.7 196.453 2175.83 197.309 Q2176.96 198.143 2180.39 198.907 L2181.85 199.231 Q2186.39 200.203 2188.28 201.985 Q2190.2 203.745 2190.2 206.916 Q2190.2 210.527 2187.33 212.633 Q2184.49 214.74 2179.49 214.74 Q2177.4 214.74 2175.14 214.323 Q2172.89 213.93 2170.39 213.12 L2170.39 208.721 Q2172.75 209.948 2175.04 210.573 Q2177.33 211.175 2179.58 211.175 Q2182.59 211.175 2184.21 210.157 Q2185.83 209.115 2185.83 207.24 Q2185.83 205.504 2184.65 204.578 Q2183.49 203.652 2179.53 202.796 L2178.05 202.448 Q2174.09 201.615 2172.33 199.902 Q2170.58 198.166 2170.58 195.157 Q2170.58 191.499 2173.17 189.509 Q2175.76 187.518 2180.53 187.518 Q2182.89 187.518 2184.97 187.865 Q2187.06 188.212 2188.82 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2213.51 188.907 L2213.51 192.934 Q2211.71 192.009 2209.76 191.546 Q2207.82 191.083 2205.74 191.083 Q2202.57 191.083 2200.97 192.055 Q2199.39 193.027 2199.39 194.971 Q2199.39 196.453 2200.53 197.309 Q2201.66 198.143 2205.09 198.907 L2206.55 199.231 Q2211.08 200.203 2212.98 201.985 Q2214.9 203.745 2214.9 206.916 Q2214.9 210.527 2212.03 212.633 Q2209.19 214.74 2204.19 214.74 Q2202.1 214.74 2199.83 214.323 Q2197.59 213.93 2195.09 213.12 L2195.09 208.721 Q2197.45 209.948 2199.74 210.573 Q2202.03 211.175 2204.28 211.175 Q2207.29 211.175 2208.91 210.157 Q2210.53 209.115 2210.53 207.24 Q2210.53 205.504 2209.35 204.578 Q2208.19 203.652 2204.23 202.796 L2202.75 202.448 Q2198.79 201.615 2197.03 199.902 Q2195.27 198.166 2195.27 195.157 Q2195.27 191.499 2197.87 189.509 Q2200.46 187.518 2205.23 187.518 Q2207.59 187.518 2209.67 187.865 Q2211.76 188.212 2213.51 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip570)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1862.2,248.629 2008.67,248.629 "/>
<path clip-path="url(#clip570)" d="M2033.08 239.983 L2037.59 239.983 L2045.69 261.742 L2053.79 239.983 L2058.31 239.983 L2048.59 265.909 L2042.8 265.909 L2033.08 239.983 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2075.97 252.876 Q2070.81 252.876 2068.82 254.057 Q2066.83 255.237 2066.83 258.085 Q2066.83 260.353 2068.31 261.696 Q2069.81 263.015 2072.38 263.015 Q2075.92 263.015 2078.05 260.515 Q2080.21 257.992 2080.21 253.825 L2080.21 252.876 L2075.97 252.876 M2084.46 251.117 L2084.46 265.909 L2080.21 265.909 L2080.21 261.973 Q2078.75 264.335 2076.57 265.469 Q2074.4 266.58 2071.25 266.58 Q2067.27 266.58 2064.9 264.358 Q2062.57 262.112 2062.57 258.362 Q2062.57 253.987 2065.48 251.765 Q2068.42 249.543 2074.23 249.543 L2080.21 249.543 L2080.21 249.126 Q2080.21 246.187 2078.26 244.589 Q2076.34 242.969 2072.84 242.969 Q2070.62 242.969 2068.52 243.501 Q2066.41 244.034 2064.46 245.099 L2064.46 241.163 Q2066.8 240.261 2069 239.821 Q2071.2 239.358 2073.28 239.358 Q2078.91 239.358 2081.69 242.274 Q2084.46 245.191 2084.46 251.117 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2093.24 229.89 L2097.5 229.89 L2097.5 265.909 L2093.24 265.909 L2093.24 229.89 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2126.11 273.779 L2126.11 277.089 L2101.48 277.089 L2101.48 273.779 L2126.11 273.779 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2141.89 252.876 Q2136.73 252.876 2134.74 254.057 Q2132.75 255.237 2132.75 258.085 Q2132.75 260.353 2134.23 261.696 Q2135.74 263.015 2138.31 263.015 Q2141.85 263.015 2143.98 260.515 Q2146.13 257.992 2146.13 253.825 L2146.13 252.876 L2141.89 252.876 M2150.39 251.117 L2150.39 265.909 L2146.13 265.909 L2146.13 261.973 Q2144.67 264.335 2142.5 265.469 Q2140.32 266.58 2137.17 266.58 Q2133.19 266.58 2130.83 264.358 Q2128.49 262.112 2128.49 258.362 Q2128.49 253.987 2131.41 251.765 Q2134.35 249.543 2140.16 249.543 L2146.13 249.543 L2146.13 249.126 Q2146.13 246.187 2144.19 244.589 Q2142.27 242.969 2138.77 242.969 Q2136.55 242.969 2134.44 243.501 Q2132.33 244.034 2130.39 245.099 L2130.39 241.163 Q2132.73 240.261 2134.93 239.821 Q2137.13 239.358 2139.21 239.358 Q2144.83 239.358 2147.61 242.274 Q2150.39 245.191 2150.39 251.117 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2177.82 240.978 L2177.82 244.96 Q2176.02 243.964 2174.19 243.478 Q2172.38 242.969 2170.53 242.969 Q2166.39 242.969 2164.09 245.608 Q2161.8 248.224 2161.8 252.969 Q2161.8 257.714 2164.09 260.353 Q2166.39 262.969 2170.53 262.969 Q2172.38 262.969 2174.19 262.483 Q2176.02 261.973 2177.82 260.978 L2177.82 264.913 Q2176.04 265.747 2174.12 266.163 Q2172.22 266.58 2170.07 266.58 Q2164.21 266.58 2160.76 262.899 Q2157.31 259.219 2157.31 252.969 Q2157.31 246.626 2160.78 242.992 Q2164.28 239.358 2170.34 239.358 Q2172.31 239.358 2174.19 239.775 Q2176.06 240.168 2177.82 240.978 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip570)" d="M2203.89 240.978 L2203.89 244.96 Q2202.08 243.964 2200.25 243.478 Q2198.45 242.969 2196.59 242.969 Q2192.45 242.969 2190.16 245.608 Q2187.87 248.224 2187.87 252.969 Q2187.87 257.714 2190.16 260.353 Q2192.45 262.969 2196.59 262.969 Q2198.45 262.969 2200.25 262.483 Q2202.08 261.973 2203.89 260.978 L2203.89 264.913 Q2202.1 265.747 2200.18 266.163 Q2198.28 266.58 2196.13 266.58 Q2190.27 266.58 2186.83 262.899 Q2183.38 259.219 2183.38 252.969 Q2183.38 246.626 2186.85 242.992 Q2190.34 239.358 2196.41 239.358 Q2198.38 239.358 2200.25 239.775 Q2202.13 240.168 2203.89 240.978 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><h2 id="Discussion"><a class="docs-heading-anchor" href="#Discussion">Discussion</a><a id="Discussion-1"></a><a class="docs-heading-anchor-permalink" href="#Discussion" title="Permalink"></a></h2><p>A key feature of GoogLeNet is that it is actually <em>cheaper</em> to compute than its predecessors while simultaneously providing improved accuracy. This marks the beginning of a much more deliberate network design that trades off the cost of evaluating a network with a reduction in errors. It also marks the beginning of experimentation at a block level with network design hyperparameters, even though it was entirely manual at the time. We will revisit this topic in :numref:<code>sec_cnn-design</code> when discussing strategies for network structure exploration. </p><p>Over the following sections we will encounter a number of design choices (e.g., batch normalization, residual connections, and channel grouping) that allow us to improve networks significantly. For now, you can be proud to have implemented what is arguably the first truly modern CNN.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>GoogLeNet was so successful that it went through a number of iterations, progressively improving speed and accuracy. Try to implement and run some of them. They include the following:<ol><li>Add a batch normalization layer :cite:<code>Ioffe.Szegedy.2015</code>, as described later in :numref:<code>sec_batch_norm</code>.</li><li>Make adjustments to the Inception block (width, choice and order of convolutions), as described in :citet:<code>Szegedy.Vanhoucke.Ioffe.ea.2016</code>.</li><li>Use label smoothing for model regularization, as described in :citet:<code>Szegedy.Vanhoucke.Ioffe.ea.2016</code>.</li><li>Make further adjustments to the Inception block by adding residual connection :cite:<code>Szegedy.Ioffe.Vanhoucke.ea.2017</code>, as described later in :numref:<code>sec_resnet</code>.</li></ol></li><li>What is the minimum image size needed for GoogLeNet to work?</li><li>Can you design a variant of GoogLeNet that works on Fashion-MNIST&#39;s native resolution of <span>$28 \times 28$</span> pixels? How would you need to change the stem, the body, and the head of the network, if anything at all?</li><li>Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do the latter two network architectures significantly reduce the model parameter size?</li><li>Compare the amount of computation needed in GoogLeNet and AlexNet. How does this affect the design of an accelerator chip, e.g., in terms of memory size, memory bandwidth, cache size, the amount of computation, and the benefit of specialized operations?</li></ol><h3 id="1."><a class="docs-heading-anchor" href="#1.">1.</a><a id="1.-1"></a><a class="docs-heading-anchor-permalink" href="#1." title="Permalink"></a></h3><pre><code class="language-julia hljs">function InceptionBlockFactorized1(channel_in::Int, channel1::Int, channel2::Tuple, channel3::Tuple, channel4::Int, reduce = false)
    r = reduce ? 2 : 1
    net1 = Chain(
        Conv((1,1), channel_in =&gt; channel1, relu, stride = r)
    )

    net2 = Chain(
        Conv((1,1), channel_in =&gt; channel2[1], relu),
        Conv((3,3), channel2[1] =&gt; channel2[2], relu, pad = 1, stride = r)
    )

    net3 = Chain(
        Conv((1,1), channel_in =&gt; channel3[1], relu),
        Conv((3,3), channel3[1] =&gt; channel3[2], relu, pad = 1, stride = 1),
        Conv((3,3), channel3[2] =&gt; channel3[3], relu, pad = 1, stride = r)
    )

    net4 = Chain(
        MaxPool((3,3), pad = 1, stride = r),
        Conv((1,1), channel_in =&gt; channel4)
    )
    InceptionBlock(net1, net2, net3, net4)
end

function InceptionBlockFactorized2(channel_in::Int, channel1::Int, channel2::Tuple, channel3::Tuple, channel4::Int, reduce = false)
    r = reduce ? 2 : 1
    net1 = Chain(
        Conv((1,1), channel_in =&gt; channel1, relu, stride = r)
    )

    net2 = Chain(
        Conv((1,1), channel_in =&gt; channel2[1], relu),
        Conv((1,7), channel2[1] =&gt; channel2[1], relu, pad = (0,3), stride = 1),
        Conv((7,1), channel2[1] =&gt; channel2[2], relu, pad = (3,0), stride = r),
    )

    net3 = Chain(
        Conv((1,7), channel_in =&gt; channel3[1], relu, pad = (0,3), stride = 1),
        Conv((7,1), channel3[1] =&gt; channel3[2], relu, pad = (3,0), stride = 1),
        Conv((1,7), channel3[2] =&gt; channel3[3], relu, pad = (0,3), stride = 1),
        Conv((7,1), channel3[3] =&gt; channel3[4], relu, pad = (3,0), stride = r),
    )

    net4 = Chain(
        MaxPool((3,3), pad = 1, stride = r),
        Conv((1,1), channel_in =&gt; channel4)
    )
    InceptionBlock(net1, net2, net3, net4)
end

function InceptionBlockFactorized3(channel_in::Int, channel1::Int, channel2::Tuple, channel3::Tuple, channel4::Int, reduce = false)
    r = reduce ? 2 : 1

    net1 = Chain(
        Conv((1,1), channel_in =&gt; channel1, relu, stride = r)
    )

    net2 = Chain(
        Conv((1,1), channel_in =&gt; channel2[1], relu),
        Conv((3,3), channel2[1] =&gt; channel2[2], relu, pad = 1, stride = 1),
        Parallel(
            (x,y)-&gt;cat(x,y, dims = 3),
            Conv((1, 3), channel2[2] =&gt; channel2[3], relu, pad = (0, 1), stride = r),    
            Conv((3, 1), channel2[2] =&gt; channel2[3], relu, pad = (1, 0), stride = r)
        )
    )

    net3 = Chain(
        Conv((1,1), channel_in =&gt; channel3[2], relu),
        Parallel(
            (x,y)-&gt;cat(x,y, dims = 3),
            Conv((1, 3), channel3[2] =&gt; channel3[3], relu, pad = (0, 1), stride = r),    
            Conv((3, 1), channel3[2] =&gt; channel3[3], relu, pad = (1, 0), stride = r)
        )
    )

    net4 = Chain(
        MaxPool((3,3), pad = 1, stride = r),
        Conv((1,1), channel_in =&gt; channel4)
    )
    InceptionBlock(net1, net2, net3, net4)
end</code></pre><pre><code class="nohighlight hljs">InceptionBlockFactorized3 (generic function with 3 methods)</code></pre><pre><code class="language-julia hljs">struct GoogLeNetFactorized{N} &lt;: AbstractClassifier 
    net::N
end

function GoogLeNetFactorized(num_classes::Int = 10)
    net = Flux.@autosize (150, 150, 1, 1) Chain(
        Conv((3,3), 1 =&gt; 16, stride = 2),
        Conv((3,3), 16 =&gt; 16, stride = 1),
        Conv((3,3), 16 =&gt; 32, stride = 1, pad =1 ),
        
        MaxPool((3,3), stride = 2),
        
        Conv((3,3), 32 =&gt; 40, stride = 1),
        Conv((3,3), 40 =&gt; 96, stride = 2),
        Conv((3,3), 96 =&gt; 144, stride = 1),

        InceptionBlockFactorized1(144, 64, (16, 64), (16, 64, 96), 64),
        InceptionBlockFactorized1(288, 64, (16, 64), (16, 64, 96), 64),
        InceptionBlockFactorized1(288, 64, (16, 64), (16, 64, 96), 64, true),

        InceptionBlockFactorized2(288, 128, (64, 128), (64, 96, 128, 256), 128),
        InceptionBlockFactorized2(640, 128, (64, 128), (64, 96, 128, 256), 128),
        InceptionBlockFactorized2(640, 128, (64, 128), (64, 96, 128, 256), 128),
        InceptionBlockFactorized2(640, 128, (64, 128), (64, 96, 128, 256), 128),
        InceptionBlockFactorized2(640, 128, (64, 128), (64, 96, 128, 256), 128, true),

        InceptionBlockFactorized3(640, 128, (64, 128, 144), (64, 128, 144), 64),
        InceptionBlockFactorized3(768, 128, (64, 144, 256), (64, 128, 256), 256, true),

        GlobalMeanPool(),
        Flux.flatten,

        Dense(_ =&gt; num_classes),
        softmax
        
        
        
    )
    GoogLeNetFactorized(net)
end

Flux.@layer GoogLeNetFactorized

(glf::GoogLeNetFactorized)(x) = glf.net(x)</code></pre><pre><code class="language-julia hljs">model = GoogLeNetFactorized()
model.net</code></pre><pre><code class="nohighlight hljs">Chain(
  Conv((3, 3), 1 =&gt; 16, stride=2),      # 160 parameters
  Conv((3, 3), 16 =&gt; 16),               # 2_320 parameters
  Conv((3, 3), 16 =&gt; 32, pad=1),        # 4_640 parameters
  MaxPool((3, 3), stride=2),
  Conv((3, 3), 32 =&gt; 40),               # 11_560 parameters
  Conv((3, 3), 40 =&gt; 96, stride=2),     # 34_656 parameters
  Conv((3, 3), 96 =&gt; 144),              # 124_560 parameters
  InceptionBlock(
    Chain(
      Conv((1, 1), 144 =&gt; 64, relu),    # 9_280 parameters
    ),
    Chain(
      Conv((1, 1), 144 =&gt; 16, relu),    # 2_320 parameters
      Conv((3, 3), 16 =&gt; 64, relu, pad=1),  # 9_280 parameters
    ),
    Chain(
      Conv((1, 1), 144 =&gt; 16, relu),    # 2_320 parameters
      Conv((3, 3), 16 =&gt; 64, relu, pad=1),  # 9_280 parameters
      Conv((3, 3), 64 =&gt; 96, relu, pad=1),  # 55_392 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 144 =&gt; 64),          # 9_280 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 288 =&gt; 64, relu),    # 18_496 parameters
    ),
    Chain(
      Conv((1, 1), 288 =&gt; 16, relu),    # 4_624 parameters
      Conv((3, 3), 16 =&gt; 64, relu, pad=1),  # 9_280 parameters
    ),
    Chain(
      Conv((1, 1), 288 =&gt; 16, relu),    # 4_624 parameters
      Conv((3, 3), 16 =&gt; 64, relu, pad=1),  # 9_280 parameters
      Conv((3, 3), 64 =&gt; 96, relu, pad=1),  # 55_392 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 288 =&gt; 64),          # 18_496 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 288 =&gt; 64, relu, stride=2),  # 18_496 parameters
    ),
    Chain(
      Conv((1, 1), 288 =&gt; 16, relu),    # 4_624 parameters
      Conv((3, 3), 16 =&gt; 64, relu, pad=1, stride=2),  # 9_280 parameters
    ),
    Chain(
      Conv((1, 1), 288 =&gt; 16, relu),    # 4_624 parameters
      Conv((3, 3), 16 =&gt; 64, relu, pad=1),  # 9_280 parameters
      Conv((3, 3), 64 =&gt; 96, relu, pad=1, stride=2),  # 55_392 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=2),
      Conv((1, 1), 288 =&gt; 64),          # 18_496 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 288 =&gt; 128, relu),   # 36_992 parameters
    ),
    Chain(
      Conv((1, 1), 288 =&gt; 64, relu),    # 18_496 parameters
      Conv((1, 7), 64 =&gt; 64, relu, pad=(0, 3)),  # 28_736 parameters
      Conv((7, 1), 64 =&gt; 128, relu, pad=(3, 0)),  # 57_472 parameters
    ),
    Chain(
      Conv((1, 7), 288 =&gt; 64, relu, pad=(0, 3)),  # 129_088 parameters
      Conv((7, 1), 64 =&gt; 96, relu, pad=(3, 0)),  # 43_104 parameters
      Conv((1, 7), 96 =&gt; 128, relu, pad=(0, 3)),  # 86_144 parameters
      Conv((7, 1), 128 =&gt; 256, relu, pad=(3, 0)),  # 229_632 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 288 =&gt; 128),         # 36_992 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 640 =&gt; 128, relu),   # 82_048 parameters
    ),
    Chain(
      Conv((1, 1), 640 =&gt; 64, relu),    # 41_024 parameters
      Conv((1, 7), 64 =&gt; 64, relu, pad=(0, 3)),  # 28_736 parameters
      Conv((7, 1), 64 =&gt; 128, relu, pad=(3, 0)),  # 57_472 parameters
    ),
    Chain(
      Conv((1, 7), 640 =&gt; 64, relu, pad=(0, 3)),  # 286_784 parameters
      Conv((7, 1), 64 =&gt; 96, relu, pad=(3, 0)),  # 43_104 parameters
      Conv((1, 7), 96 =&gt; 128, relu, pad=(0, 3)),  # 86_144 parameters
      Conv((7, 1), 128 =&gt; 256, relu, pad=(3, 0)),  # 229_632 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 640 =&gt; 128),         # 82_048 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 640 =&gt; 128, relu),   # 82_048 parameters
    ),
    Chain(
      Conv((1, 1), 640 =&gt; 64, relu),    # 41_024 parameters
      Conv((1, 7), 64 =&gt; 64, relu, pad=(0, 3)),  # 28_736 parameters
      Conv((7, 1), 64 =&gt; 128, relu, pad=(3, 0)),  # 57_472 parameters
    ),
    Chain(
      Conv((1, 7), 640 =&gt; 64, relu, pad=(0, 3)),  # 286_784 parameters
      Conv((7, 1), 64 =&gt; 96, relu, pad=(3, 0)),  # 43_104 parameters
      Conv((1, 7), 96 =&gt; 128, relu, pad=(0, 3)),  # 86_144 parameters
      Conv((7, 1), 128 =&gt; 256, relu, pad=(3, 0)),  # 229_632 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 640 =&gt; 128),         # 82_048 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 640 =&gt; 128, relu),   # 82_048 parameters
    ),
    Chain(
      Conv((1, 1), 640 =&gt; 64, relu),    # 41_024 parameters
      Conv((1, 7), 64 =&gt; 64, relu, pad=(0, 3)),  # 28_736 parameters
      Conv((7, 1), 64 =&gt; 128, relu, pad=(3, 0)),  # 57_472 parameters
    ),
    Chain(
      Conv((1, 7), 640 =&gt; 64, relu, pad=(0, 3)),  # 286_784 parameters
      Conv((7, 1), 64 =&gt; 96, relu, pad=(3, 0)),  # 43_104 parameters
      Conv((1, 7), 96 =&gt; 128, relu, pad=(0, 3)),  # 86_144 parameters
      Conv((7, 1), 128 =&gt; 256, relu, pad=(3, 0)),  # 229_632 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 640 =&gt; 128),         # 82_048 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 640 =&gt; 128, relu, stride=2),  # 82_048 parameters
    ),
    Chain(
      Conv((1, 1), 640 =&gt; 64, relu),    # 41_024 parameters
      Conv((1, 7), 64 =&gt; 64, relu, pad=(0, 3)),  # 28_736 parameters
      Conv((7, 1), 64 =&gt; 128, relu, pad=(3, 0), stride=2),  # 57_472 parameters
    ),
    Chain(
      Conv((1, 7), 640 =&gt; 64, relu, pad=(0, 3)),  # 286_784 parameters
      Conv((7, 1), 64 =&gt; 96, relu, pad=(3, 0)),  # 43_104 parameters
      Conv((1, 7), 96 =&gt; 128, relu, pad=(0, 3)),  # 86_144 parameters
      Conv((7, 1), 128 =&gt; 256, relu, pad=(3, 0), stride=2),  # 229_632 parameters
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=2),
      Conv((1, 1), 640 =&gt; 128),         # 82_048 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 640 =&gt; 128, relu),   # 82_048 parameters
    ),
    Chain(
      Conv((1, 1), 640 =&gt; 64, relu),    # 41_024 parameters
      Conv((3, 3), 64 =&gt; 128, relu, pad=1),  # 73_856 parameters
      Parallel(
        var&quot;#53#55&quot;(),
        Conv((1, 3), 128 =&gt; 144, relu, pad=(0, 1)),  # 55_440 parameters
        Conv((3, 1), 128 =&gt; 144, relu, pad=(1, 0)),  # 55_440 parameters
      ),
    ),
    Chain(
      Conv((1, 1), 640 =&gt; 128, relu),   # 82_048 parameters
      Parallel(
        var&quot;#54#56&quot;(),
        Conv((1, 3), 128 =&gt; 144, relu, pad=(0, 1)),  # 55_440 parameters
        Conv((3, 1), 128 =&gt; 144, relu, pad=(1, 0)),  # 55_440 parameters
      ),
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=1),
      Conv((1, 1), 640 =&gt; 64),          # 41_024 parameters
    ),
  ),
  InceptionBlock(
    Chain(
      Conv((1, 1), 768 =&gt; 128, relu, stride=2),  # 98_432 parameters
    ),
    Chain(
      Conv((1, 1), 768 =&gt; 64, relu),    # 49_216 parameters
      Conv((3, 3), 64 =&gt; 144, relu, pad=1),  # 83_088 parameters
      Parallel(
        var&quot;#53#55&quot;(),
        Conv((1, 3), 144 =&gt; 256, relu, pad=(0, 1), stride=2),  # 110_848 parameters
        Conv((3, 1), 144 =&gt; 256, relu, pad=(1, 0), stride=2),  # 110_848 parameters
      ),
    ),
    Chain(
      Conv((1, 1), 768 =&gt; 128, relu),   # 98_432 parameters
      Parallel(
        var&quot;#54#56&quot;(),
        Conv((1, 3), 128 =&gt; 256, relu, pad=(0, 1), stride=2),  # 98_560 parameters
        Conv((3, 1), 128 =&gt; 256, relu, pad=(1, 0), stride=2),  # 98_560 parameters
      ),
    ),
    Chain(
      MaxPool((3, 3), pad=1, stride=2),
      Conv((1, 1), 768 =&gt; 256),         # 196_864 parameters
    ),
  ),
  GlobalMeanPool(),
  Flux.flatten,
  Dense(1408 =&gt; 10),                    # 14_090 parameters
  NNlib.softmax,
)                   # Total: 182 arrays, 6_430_754 parameters, 24.548 MiB.</code></pre><pre><code class="language-julia hljs">function d2lai.loss(model::GoogLeNetFactorized, y_pred, y)
    Flux.crossentropy(y_pred, y)
end

function d2lai.get_dataloader(data::d2lai.FashionMNISTData; train = true, flatten = false)
    d = train ? data.train : data.val 
    if flatten 
        Flux.DataLoader((Flux.flatten(d[1]), d[2]); batchsize = data.args.batchsize, shuffle = train)
    else
        labels = Flux.onehotbatch(d[2], 0:9)
        labels_smooth = Flux.label_smoothing(labels, 0.01)
        d_reshaped = reshape(d[1], size(d[1])[1], size(d[1])[2], 1, :)
        Flux.DataLoader((d_reshaped, labels_smooth); batchsize = data.args.batchsize, shuffle = train)
    end
end

using Statistics 
function d2lai.accuracy(model::GoogLeNetFactorized, y_pred, y; averaged = true)
    y_labels = argmax(y_pred, dims = 1)
    y_labels = getindex.(y_labels , 1)
    y_p = argmax(y, dims = 1)
    y_p = getindex.(y_p , 1)
    compare = (y_labels .== y_p)
    return averaged ? Statistics.mean(compare, dims = 2)[1] : compare
end


</code></pre><pre><code class="language-julia hljs">data = d2lai.FashionMNISTData(batchsize = 128, resize = (150, 150));</code></pre><pre><code class="language-julia hljs">opt = Descent(0.05)
trainer = Trainer(model, data, opt; max_epochs = 10, gpu = true, board_yscale = :identity)
d2lai.fit(trainer);</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 1.2599708, Val Loss: 1.0393901, Val Acc: 0.6875
    [ Info: Train Loss: 0.41630396, Val Loss: 0.3696559, Val Acc: 0.875
    [ Info: Train Loss: 0.3655992, Val Loss: 0.3507933, Val Acc: 0.9375
    [ Info: Train Loss: 0.28544185, Val Loss: 0.25390226, Val Acc: 0.9375
    [ Info: Train Loss: 0.19135235, Val Loss: 0.24655311, Val Acc: 0.9375
    [ Info: Train Loss: 0.26055798, Val Loss: 0.3746107, Val Acc: 0.875
    [ Info: Train Loss: 0.23221236, Val Loss: 0.31955272, Val Acc: 0.875
    [ Info: Train Loss: 0.17181064, Val Loss: 0.42100507, Val Acc: 0.875
    [ Info: Train Loss: 0.16388367, Val Loss: 0.39053202, Val Acc: 0.9375
    [ Info: Train Loss: 0.17038195, Val Loss: 0.35706797, Val Acc: 0.9375</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip970">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip970)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip971">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip970)" d="M186.274 1423.18 L2352.76 1423.18 L2352.76 47.2441 L186.274 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip972">
    <rect x="186" y="47" width="2167" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="474.684,1423.18 474.684,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="928.873,1423.18 928.873,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1383.06,1423.18 1383.06,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1837.25,1423.18 1837.25,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2291.44,1423.18 2291.44,47.2441 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="186.274,1332.15 2352.76,1332.15 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="186.274,1044.47 2352.76,1044.47 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="186.274,756.792 2352.76,756.792 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="186.274,469.114 2352.76,469.114 "/>
<polyline clip-path="url(#clip972)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="186.274,181.436 2352.76,181.436 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="474.684,1423.18 474.684,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="928.873,1423.18 928.873,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1383.06,1423.18 1383.06,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1837.25,1423.18 1837.25,1404.28 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2291.44,1423.18 2291.44,1404.28 "/>
<path clip-path="url(#clip970)" d="M469.337 1481.64 L485.656 1481.64 L485.656 1485.58 L463.712 1485.58 L463.712 1481.64 Q466.374 1478.89 470.957 1474.26 Q475.564 1469.61 476.744 1468.27 Q478.99 1465.74 479.869 1464.01 Q480.772 1462.25 480.772 1460.56 Q480.772 1457.8 478.828 1456.07 Q476.906 1454.33 473.804 1454.33 Q471.605 1454.33 469.152 1455.09 Q466.721 1455.86 463.943 1457.41 L463.943 1452.69 Q466.767 1451.55 469.221 1450.97 Q471.675 1450.39 473.712 1450.39 Q479.082 1450.39 482.277 1453.08 Q485.471 1455.77 485.471 1460.26 Q485.471 1462.39 484.661 1464.31 Q483.874 1466.2 481.767 1468.8 Q481.189 1469.47 478.087 1472.69 Q474.985 1475.88 469.337 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M931.882 1455.09 L920.077 1473.54 L931.882 1473.54 L931.882 1455.09 M930.656 1451.02 L936.535 1451.02 L936.535 1473.54 L941.466 1473.54 L941.466 1477.43 L936.535 1477.43 L936.535 1485.58 L931.882 1485.58 L931.882 1477.43 L916.281 1477.43 L916.281 1472.92 L930.656 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1383.47 1466.44 Q1380.32 1466.44 1378.47 1468.59 Q1376.64 1470.74 1376.64 1474.49 Q1376.64 1478.22 1378.47 1480.39 Q1380.32 1482.55 1383.47 1482.55 Q1386.62 1482.55 1388.44 1480.39 Q1390.3 1478.22 1390.3 1474.49 Q1390.3 1470.74 1388.44 1468.59 Q1386.62 1466.44 1383.47 1466.44 M1392.75 1451.78 L1392.75 1456.04 Q1390.99 1455.21 1389.18 1454.77 Q1387.4 1454.33 1385.64 1454.33 Q1381.01 1454.33 1378.56 1457.45 Q1376.13 1460.58 1375.78 1466.9 Q1377.15 1464.89 1379.21 1463.82 Q1381.27 1462.73 1383.75 1462.73 Q1388.95 1462.73 1391.96 1465.9 Q1395 1469.05 1395 1474.49 Q1395 1479.82 1391.85 1483.03 Q1388.7 1486.25 1383.47 1486.25 Q1377.47 1486.25 1374.3 1481.67 Q1371.13 1477.06 1371.13 1468.33 Q1371.13 1460.14 1375.02 1455.28 Q1378.91 1450.39 1385.46 1450.39 Q1387.22 1450.39 1389 1450.74 Q1390.81 1451.09 1392.75 1451.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1837.25 1469.17 Q1833.92 1469.17 1832 1470.95 Q1830.1 1472.73 1830.1 1475.86 Q1830.1 1478.98 1832 1480.77 Q1833.92 1482.55 1837.25 1482.55 Q1840.58 1482.55 1842.51 1480.77 Q1844.43 1478.96 1844.43 1475.86 Q1844.43 1472.73 1842.51 1470.95 Q1840.61 1469.17 1837.25 1469.17 M1832.58 1467.18 Q1829.57 1466.44 1827.88 1464.38 Q1826.21 1462.32 1826.21 1459.35 Q1826.21 1455.21 1829.15 1452.8 Q1832.11 1450.39 1837.25 1450.39 Q1842.41 1450.39 1845.35 1452.8 Q1848.29 1455.21 1848.29 1459.35 Q1848.29 1462.32 1846.6 1464.38 Q1844.94 1466.44 1841.95 1467.18 Q1845.33 1467.96 1847.2 1470.26 Q1849.1 1472.55 1849.1 1475.86 Q1849.1 1480.88 1846.02 1483.57 Q1842.97 1486.25 1837.25 1486.25 Q1831.53 1486.25 1828.46 1483.57 Q1825.4 1480.88 1825.4 1475.86 Q1825.4 1472.55 1827.3 1470.26 Q1829.2 1467.96 1832.58 1467.18 M1830.86 1459.79 Q1830.86 1462.48 1832.53 1463.98 Q1834.22 1465.49 1837.25 1465.49 Q1840.26 1465.49 1841.95 1463.98 Q1843.66 1462.48 1843.66 1459.79 Q1843.66 1457.11 1841.95 1455.6 Q1840.26 1454.1 1837.25 1454.1 Q1834.22 1454.1 1832.53 1455.6 Q1830.86 1457.11 1830.86 1459.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2266.13 1481.64 L2273.77 1481.64 L2273.77 1455.28 L2265.46 1456.95 L2265.46 1452.69 L2273.72 1451.02 L2278.4 1451.02 L2278.4 1481.64 L2286.04 1481.64 L2286.04 1485.58 L2266.13 1485.58 L2266.13 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2305.48 1454.1 Q2301.87 1454.1 2300.04 1457.66 Q2298.23 1461.2 2298.23 1468.33 Q2298.23 1475.44 2300.04 1479.01 Q2301.87 1482.55 2305.48 1482.55 Q2309.11 1482.55 2310.92 1479.01 Q2312.75 1475.44 2312.75 1468.33 Q2312.75 1461.2 2310.92 1457.66 Q2309.11 1454.1 2305.48 1454.1 M2305.48 1450.39 Q2311.29 1450.39 2314.35 1455 Q2317.42 1459.58 2317.42 1468.33 Q2317.42 1477.06 2314.35 1481.67 Q2311.29 1486.25 2305.48 1486.25 Q2299.67 1486.25 2296.59 1481.67 Q2293.54 1477.06 2293.54 1468.33 Q2293.54 1459.58 2296.59 1455 Q2299.67 1450.39 2305.48 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1189.7 1548.76 L1189.7 1551.62 L1162.78 1551.62 Q1163.16 1557.67 1166.41 1560.85 Q1169.68 1564 1175.51 1564 Q1178.88 1564 1182.03 1563.17 Q1185.22 1562.35 1188.34 1560.69 L1188.34 1566.23 Q1185.19 1567.57 1181.88 1568.27 Q1178.56 1568.97 1175.16 1568.97 Q1166.63 1568.97 1161.63 1564 Q1156.67 1559.04 1156.67 1550.57 Q1156.67 1541.82 1161.38 1536.69 Q1166.12 1531.54 1174.14 1531.54 Q1181.33 1531.54 1185.5 1536.18 Q1189.7 1540.8 1189.7 1548.76 M1183.85 1547.04 Q1183.78 1542.23 1181.14 1539.37 Q1178.53 1536.5 1174.2 1536.5 Q1169.3 1536.5 1166.34 1539.27 Q1163.41 1542.04 1162.97 1547.07 L1183.85 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1204.98 1562.7 L1204.98 1581.6 L1199.09 1581.6 L1199.09 1532.4 L1204.98 1532.4 L1204.98 1537.81 Q1206.83 1534.62 1209.63 1533.1 Q1212.46 1531.54 1216.38 1531.54 Q1222.87 1531.54 1226.91 1536.69 Q1230.99 1541.85 1230.99 1550.25 Q1230.99 1558.65 1226.91 1563.81 Q1222.87 1568.97 1216.38 1568.97 Q1212.46 1568.97 1209.63 1567.44 Q1206.83 1565.88 1204.98 1562.7 M1224.91 1550.25 Q1224.91 1543.79 1222.23 1540.13 Q1219.59 1536.44 1214.94 1536.44 Q1210.3 1536.44 1207.62 1540.13 Q1204.98 1543.79 1204.98 1550.25 Q1204.98 1556.71 1207.62 1560.4 Q1210.3 1564.07 1214.94 1564.07 Q1219.59 1564.07 1222.23 1560.4 Q1224.91 1556.71 1224.91 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1254.51 1536.5 Q1249.8 1536.5 1247.06 1540.19 Q1244.32 1543.85 1244.32 1550.25 Q1244.32 1556.65 1247.03 1560.34 Q1249.77 1564 1254.51 1564 Q1259.19 1564 1261.92 1560.31 Q1264.66 1556.62 1264.66 1550.25 Q1264.66 1543.92 1261.92 1540.23 Q1259.19 1536.5 1254.51 1536.5 M1254.51 1531.54 Q1262.15 1531.54 1266.51 1536.5 Q1270.87 1541.47 1270.87 1550.25 Q1270.87 1559 1266.51 1564 Q1262.15 1568.97 1254.51 1568.97 Q1246.84 1568.97 1242.48 1564 Q1238.15 1559 1238.15 1550.25 Q1238.15 1541.47 1242.48 1536.5 Q1246.84 1531.54 1254.51 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1306.23 1533.76 L1306.23 1539.24 Q1303.75 1537.87 1301.23 1537.2 Q1298.75 1536.5 1296.2 1536.5 Q1290.51 1536.5 1287.35 1540.13 Q1284.2 1543.73 1284.2 1550.25 Q1284.2 1556.78 1287.35 1560.4 Q1290.51 1564 1296.2 1564 Q1298.75 1564 1301.23 1563.33 Q1303.75 1562.63 1306.23 1561.26 L1306.23 1566.68 Q1303.78 1567.82 1301.14 1568.39 Q1298.53 1568.97 1295.57 1568.97 Q1287.51 1568.97 1282.77 1563.91 Q1278.03 1558.85 1278.03 1550.25 Q1278.03 1541.53 1282.8 1536.53 Q1287.61 1531.54 1295.95 1531.54 Q1298.65 1531.54 1301.23 1532.11 Q1303.81 1532.65 1306.23 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1346.05 1546.53 L1346.05 1568.04 L1340.19 1568.04 L1340.19 1546.72 Q1340.19 1541.66 1338.22 1539.14 Q1336.24 1536.63 1332.3 1536.63 Q1327.55 1536.63 1324.82 1539.65 Q1322.08 1542.68 1322.08 1547.9 L1322.08 1568.04 L1316.19 1568.04 L1316.19 1518.52 L1322.08 1518.52 L1322.08 1537.93 Q1324.18 1534.72 1327.01 1533.13 Q1329.88 1531.54 1333.6 1531.54 Q1339.74 1531.54 1342.9 1535.36 Q1346.05 1539.14 1346.05 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M1380.45 1533.45 L1380.45 1538.98 Q1377.97 1537.71 1375.3 1537.07 Q1372.62 1536.44 1369.76 1536.44 Q1365.4 1536.44 1363.2 1537.77 Q1361.04 1539.11 1361.04 1541.79 Q1361.04 1543.82 1362.6 1545 Q1364.16 1546.15 1368.87 1547.2 L1370.87 1547.64 Q1377.11 1548.98 1379.72 1551.43 Q1382.36 1553.85 1382.36 1558.21 Q1382.36 1563.17 1378.42 1566.07 Q1374.5 1568.97 1367.63 1568.97 Q1364.76 1568.97 1361.64 1568.39 Q1358.56 1567.85 1355.12 1566.74 L1355.12 1560.69 Q1358.36 1562.38 1361.52 1563.24 Q1364.67 1564.07 1367.75 1564.07 Q1371.89 1564.07 1374.12 1562.66 Q1376.35 1561.23 1376.35 1558.65 Q1376.35 1556.27 1374.72 1554.99 Q1373.13 1553.72 1367.69 1552.54 L1365.65 1552.07 Q1360.21 1550.92 1357.79 1548.56 Q1355.37 1546.18 1355.37 1542.04 Q1355.37 1537.01 1358.94 1534.27 Q1362.5 1531.54 1369.06 1531.54 Q1372.31 1531.54 1375.17 1532.01 Q1378.03 1532.49 1380.45 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,1423.18 186.274,47.2441 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,1332.15 205.172,1332.15 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,1044.47 205.172,1044.47 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,756.792 205.172,756.792 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,469.114 205.172,469.114 "/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="186.274,181.436 205.172,181.436 "/>
<path clip-path="url(#clip970)" d="M63.9319 1317.95 Q60.3208 1317.95 58.4921 1321.51 Q56.6865 1325.05 56.6865 1332.18 Q56.6865 1339.29 58.4921 1342.85 Q60.3208 1346.4 63.9319 1346.4 Q67.5661 1346.4 69.3717 1342.85 Q71.2004 1339.29 71.2004 1332.18 Q71.2004 1325.05 69.3717 1321.51 Q67.5661 1317.95 63.9319 1317.95 M63.9319 1314.24 Q69.742 1314.24 72.7976 1318.85 Q75.8763 1323.43 75.8763 1332.18 Q75.8763 1340.91 72.7976 1345.52 Q69.742 1350.1 63.9319 1350.1 Q58.1217 1350.1 55.043 1345.52 Q51.9875 1340.91 51.9875 1332.18 Q51.9875 1323.43 55.043 1318.85 Q58.1217 1314.24 63.9319 1314.24 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M84.0938 1343.55 L88.978 1343.55 L88.978 1349.43 L84.0938 1349.43 L84.0938 1343.55 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M103.191 1345.49 L119.51 1345.49 L119.51 1349.43 L97.566 1349.43 L97.566 1345.49 Q100.228 1342.74 104.811 1338.11 Q109.418 1333.46 110.598 1332.11 Q112.844 1329.59 113.723 1327.85 Q114.626 1326.1 114.626 1324.41 Q114.626 1321.65 112.682 1319.91 Q110.76 1318.18 107.658 1318.18 Q105.459 1318.18 103.006 1318.94 Q100.575 1319.71 97.7974 1321.26 L97.7974 1316.54 Q100.621 1315.4 103.075 1314.82 Q105.529 1314.24 107.566 1314.24 Q112.936 1314.24 116.131 1316.93 Q119.325 1319.61 119.325 1324.1 Q119.325 1326.23 118.515 1328.16 Q117.728 1330.05 115.621 1332.65 Q115.043 1333.32 111.941 1336.54 Q108.839 1339.73 103.191 1345.49 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M129.371 1314.87 L147.728 1314.87 L147.728 1318.8 L133.654 1318.8 L133.654 1327.28 Q134.672 1326.93 135.691 1326.77 Q136.709 1326.58 137.728 1326.58 Q143.515 1326.58 146.894 1329.75 Q150.274 1332.92 150.274 1338.34 Q150.274 1343.92 146.802 1347.02 Q143.33 1350.1 137.01 1350.1 Q134.834 1350.1 132.566 1349.73 Q130.32 1349.36 127.913 1348.62 L127.913 1343.92 Q129.996 1345.05 132.219 1345.61 Q134.441 1346.16 136.918 1346.16 Q140.922 1346.16 143.26 1344.06 Q145.598 1341.95 145.598 1338.34 Q145.598 1334.73 143.26 1332.62 Q140.922 1330.52 136.918 1330.52 Q135.043 1330.52 133.168 1330.93 Q131.316 1331.35 129.371 1332.23 L129.371 1314.87 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M62.9365 1030.27 Q59.3254 1030.27 57.4967 1033.83 Q55.6912 1037.38 55.6912 1044.51 Q55.6912 1051.61 57.4967 1055.18 Q59.3254 1058.72 62.9365 1058.72 Q66.5707 1058.72 68.3763 1055.18 Q70.205 1051.61 70.205 1044.51 Q70.205 1037.38 68.3763 1033.83 Q66.5707 1030.27 62.9365 1030.27 M62.9365 1026.57 Q68.7467 1026.57 71.8022 1031.17 Q74.8809 1035.76 74.8809 1044.51 Q74.8809 1053.23 71.8022 1057.84 Q68.7467 1062.42 62.9365 1062.42 Q57.1264 1062.42 54.0477 1057.84 Q50.9921 1053.23 50.9921 1044.51 Q50.9921 1035.76 54.0477 1031.17 Q57.1264 1026.57 62.9365 1026.57 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M83.0984 1055.87 L87.9827 1055.87 L87.9827 1061.75 L83.0984 1061.75 L83.0984 1055.87 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M98.2141 1027.19 L116.57 1027.19 L116.57 1031.13 L102.496 1031.13 L102.496 1039.6 Q103.515 1039.25 104.534 1039.09 Q105.552 1038.9 106.571 1038.9 Q112.358 1038.9 115.737 1042.07 Q119.117 1045.25 119.117 1050.66 Q119.117 1056.24 115.645 1059.34 Q112.172 1062.42 105.853 1062.42 Q103.677 1062.42 101.409 1062.05 Q99.1632 1061.68 96.7558 1060.94 L96.7558 1056.24 Q98.8391 1057.38 101.061 1057.93 Q103.284 1058.49 105.76 1058.49 Q109.765 1058.49 112.103 1056.38 Q114.441 1054.27 114.441 1050.66 Q114.441 1047.05 112.103 1044.94 Q109.765 1042.84 105.76 1042.84 Q103.885 1042.84 102.01 1043.26 Q100.159 1043.67 98.2141 1044.55 L98.2141 1027.19 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M138.33 1030.27 Q134.719 1030.27 132.89 1033.83 Q131.084 1037.38 131.084 1044.51 Q131.084 1051.61 132.89 1055.18 Q134.719 1058.72 138.33 1058.72 Q141.964 1058.72 143.769 1055.18 Q145.598 1051.61 145.598 1044.51 Q145.598 1037.38 143.769 1033.83 Q141.964 1030.27 138.33 1030.27 M138.33 1026.57 Q144.14 1026.57 147.195 1031.17 Q150.274 1035.76 150.274 1044.51 Q150.274 1053.23 147.195 1057.84 Q144.14 1062.42 138.33 1062.42 Q132.519 1062.42 129.441 1057.84 Q126.385 1053.23 126.385 1044.51 Q126.385 1035.76 129.441 1031.17 Q132.519 1026.57 138.33 1026.57 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M63.9319 742.591 Q60.3208 742.591 58.4921 746.156 Q56.6865 749.698 56.6865 756.827 Q56.6865 763.934 58.4921 767.498 Q60.3208 771.04 63.9319 771.04 Q67.5661 771.04 69.3717 767.498 Q71.2004 763.934 71.2004 756.827 Q71.2004 749.698 69.3717 746.156 Q67.5661 742.591 63.9319 742.591 M63.9319 738.887 Q69.742 738.887 72.7976 743.494 Q75.8763 748.077 75.8763 756.827 Q75.8763 765.554 72.7976 770.16 Q69.742 774.744 63.9319 774.744 Q58.1217 774.744 55.043 770.16 Q51.9875 765.554 51.9875 756.827 Q51.9875 748.077 55.043 743.494 Q58.1217 738.887 63.9319 738.887 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M84.0938 768.193 L88.978 768.193 L88.978 774.072 L84.0938 774.072 L84.0938 768.193 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M97.9826 739.512 L120.205 739.512 L120.205 741.503 L107.658 774.072 L102.774 774.072 L114.58 743.448 L97.9826 743.448 L97.9826 739.512 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M129.371 739.512 L147.728 739.512 L147.728 743.448 L133.654 743.448 L133.654 751.92 Q134.672 751.573 135.691 751.411 Q136.709 751.225 137.728 751.225 Q143.515 751.225 146.894 754.397 Q150.274 757.568 150.274 762.985 Q150.274 768.563 146.802 771.665 Q143.33 774.744 137.01 774.744 Q134.834 774.744 132.566 774.373 Q130.32 774.003 127.913 773.262 L127.913 768.563 Q129.996 769.697 132.219 770.253 Q134.441 770.809 136.918 770.809 Q140.922 770.809 143.26 768.702 Q145.598 766.596 145.598 762.985 Q145.598 759.373 143.26 757.267 Q140.922 755.161 136.918 755.161 Q135.043 755.161 133.168 755.577 Q131.316 755.994 129.371 756.873 L129.371 739.512 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M53.7467 482.459 L61.3856 482.459 L61.3856 456.094 L53.0754 457.76 L53.0754 453.501 L61.3393 451.834 L66.0152 451.834 L66.0152 482.459 L73.654 482.459 L73.654 486.394 L53.7467 486.394 L53.7467 482.459 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M83.0984 480.515 L87.9827 480.515 L87.9827 486.394 L83.0984 486.394 L83.0984 480.515 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M108.168 454.913 Q104.557 454.913 102.728 458.478 Q100.922 462.02 100.922 469.149 Q100.922 476.256 102.728 479.82 Q104.557 483.362 108.168 483.362 Q111.802 483.362 113.608 479.82 Q115.436 476.256 115.436 469.149 Q115.436 462.02 113.608 458.478 Q111.802 454.913 108.168 454.913 M108.168 451.209 Q113.978 451.209 117.033 455.816 Q120.112 460.399 120.112 469.149 Q120.112 477.876 117.033 482.482 Q113.978 487.066 108.168 487.066 Q102.358 487.066 99.2789 482.482 Q96.2234 477.876 96.2234 469.149 Q96.2234 460.399 99.2789 455.816 Q102.358 451.209 108.168 451.209 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M138.33 454.913 Q134.719 454.913 132.89 458.478 Q131.084 462.02 131.084 469.149 Q131.084 476.256 132.89 479.82 Q134.719 483.362 138.33 483.362 Q141.964 483.362 143.769 479.82 Q145.598 476.256 145.598 469.149 Q145.598 462.02 143.769 458.478 Q141.964 454.913 138.33 454.913 M138.33 451.209 Q144.14 451.209 147.195 455.816 Q150.274 460.399 150.274 469.149 Q150.274 477.876 147.195 482.482 Q144.14 487.066 138.33 487.066 Q132.519 487.066 129.441 482.482 Q126.385 477.876 126.385 469.149 Q126.385 460.399 129.441 455.816 Q132.519 451.209 138.33 451.209 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M54.7421 194.781 L62.381 194.781 L62.381 168.416 L54.0708 170.082 L54.0708 165.823 L62.3347 164.156 L67.0106 164.156 L67.0106 194.781 L74.6494 194.781 L74.6494 198.716 L54.7421 198.716 L54.7421 194.781 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M84.0938 192.837 L88.978 192.837 L88.978 198.716 L84.0938 198.716 L84.0938 192.837 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M103.191 194.781 L119.51 194.781 L119.51 198.716 L97.566 198.716 L97.566 194.781 Q100.228 192.027 104.811 187.397 Q109.418 182.744 110.598 181.402 Q112.844 178.879 113.723 177.142 Q114.626 175.383 114.626 173.693 Q114.626 170.939 112.682 169.203 Q110.76 167.467 107.658 167.467 Q105.459 167.467 103.006 168.23 Q100.575 168.994 97.7974 170.545 L97.7974 165.823 Q100.621 164.689 103.075 164.11 Q105.529 163.531 107.566 163.531 Q112.936 163.531 116.131 166.217 Q119.325 168.902 119.325 173.392 Q119.325 175.522 118.515 177.443 Q117.728 179.342 115.621 181.934 Q115.043 182.605 111.941 185.823 Q108.839 189.017 103.191 194.781 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M129.371 164.156 L147.728 164.156 L147.728 168.092 L133.654 168.092 L133.654 176.564 Q134.672 176.217 135.691 176.054 Q136.709 175.869 137.728 175.869 Q143.515 175.869 146.894 179.041 Q150.274 182.212 150.274 187.629 Q150.274 193.207 146.802 196.309 Q143.33 199.388 137.01 199.388 Q134.834 199.388 132.566 199.017 Q130.32 198.647 127.913 197.906 L127.913 193.207 Q129.996 194.341 132.219 194.897 Q134.441 195.453 136.918 195.453 Q140.922 195.453 143.26 193.346 Q145.598 191.24 145.598 187.629 Q145.598 184.017 143.26 181.911 Q140.922 179.804 136.918 179.804 Q135.043 179.804 133.168 180.221 Q131.316 180.638 129.371 181.517 L129.371 164.156 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip972)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="247.59,180.855 474.684,1036.28 701.779,1151.85 928.873,1221.18 1155.97,1270.99 1383.06,1301.38 1610.16,1327.25 1837.25,1349.88 2064.35,1368.71 2291.44,1384.24 "/>
<polyline clip-path="url(#clip972)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="247.59,86.1857 474.684,985.045 701.779,1025.6 928.873,1111.28 1155.97,1121.99 1383.06,1153.48 1610.16,1155.8 1837.25,1150.11 2064.35,1202.23 2291.44,1192.7 "/>
<polyline clip-path="url(#clip972)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="247.59,1014.2 474.684,673.835 701.779,661.772 928.873,628.657 1155.97,623.423 1383.06,615.002 1610.16,613.295 1837.25,611.929 2064.35,592.242 2291.44,593.722 "/>
<path clip-path="url(#clip970)" d="M1841.86 300.469 L2280.54 300.469 L2280.54 93.1086 L1841.86 93.1086  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip970)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1841.86,300.469 2280.54,300.469 2280.54,93.1086 1841.86,93.1086 1841.86,300.469 "/>
<polyline clip-path="url(#clip970)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1865.93,144.949 2010.36,144.949 "/>
<path clip-path="url(#clip970)" d="M2041.84 128.942 L2041.84 136.303 L2050.61 136.303 L2050.61 139.613 L2041.84 139.613 L2041.84 153.687 Q2041.84 156.858 2042.7 157.761 Q2043.58 158.664 2046.24 158.664 L2050.61 158.664 L2050.61 162.229 L2046.24 162.229 Q2041.31 162.229 2039.43 160.4 Q2037.56 158.548 2037.56 153.687 L2037.56 139.613 L2034.43 139.613 L2034.43 136.303 L2037.56 136.303 L2037.56 128.942 L2041.84 128.942 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2071.24 140.284 Q2070.52 139.868 2069.66 139.682 Q2068.83 139.474 2067.81 139.474 Q2064.2 139.474 2062.26 141.835 Q2060.33 144.173 2060.33 148.571 L2060.33 162.229 L2056.05 162.229 L2056.05 136.303 L2060.33 136.303 L2060.33 140.331 Q2061.68 137.969 2063.83 136.835 Q2065.98 135.678 2069.06 135.678 Q2069.5 135.678 2070.03 135.747 Q2070.57 135.794 2071.21 135.909 L2071.24 140.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2087.49 149.196 Q2082.33 149.196 2080.33 150.377 Q2078.34 151.557 2078.34 154.405 Q2078.34 156.673 2079.83 158.016 Q2081.33 159.335 2083.9 159.335 Q2087.44 159.335 2089.57 156.835 Q2091.72 154.312 2091.72 150.145 L2091.72 149.196 L2087.49 149.196 M2095.98 147.437 L2095.98 162.229 L2091.72 162.229 L2091.72 158.293 Q2090.27 160.655 2088.09 161.789 Q2085.91 162.9 2082.77 162.9 Q2078.78 162.9 2076.42 160.678 Q2074.08 158.432 2074.08 154.682 Q2074.08 150.307 2077 148.085 Q2079.94 145.863 2085.75 145.863 L2091.72 145.863 L2091.72 145.446 Q2091.72 142.507 2089.78 140.909 Q2087.86 139.289 2084.36 139.289 Q2082.14 139.289 2080.03 139.821 Q2077.93 140.354 2075.98 141.419 L2075.98 137.483 Q2078.32 136.581 2080.52 136.141 Q2082.72 135.678 2084.8 135.678 Q2090.43 135.678 2093.2 138.594 Q2095.98 141.511 2095.98 147.437 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2104.76 136.303 L2109.01 136.303 L2109.01 162.229 L2104.76 162.229 L2104.76 136.303 M2104.76 126.21 L2109.01 126.21 L2109.01 131.604 L2104.76 131.604 L2104.76 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2139.48 146.581 L2139.48 162.229 L2135.22 162.229 L2135.22 146.719 Q2135.22 143.039 2133.78 141.21 Q2132.35 139.382 2129.48 139.382 Q2126.03 139.382 2124.04 141.581 Q2122.05 143.78 2122.05 147.576 L2122.05 162.229 L2117.76 162.229 L2117.76 136.303 L2122.05 136.303 L2122.05 140.331 Q2123.58 137.993 2125.64 136.835 Q2127.72 135.678 2130.43 135.678 Q2134.89 135.678 2137.19 138.456 Q2139.48 141.21 2139.48 146.581 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2167.67 170.099 L2167.67 173.409 L2143.04 173.409 L2143.04 170.099 L2167.67 170.099 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2171.68 126.21 L2175.94 126.21 L2175.94 162.229 L2171.68 162.229 L2171.68 126.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2194.89 139.289 Q2191.47 139.289 2189.48 141.974 Q2187.49 144.636 2187.49 149.289 Q2187.49 153.942 2189.45 156.627 Q2191.45 159.289 2194.89 159.289 Q2198.3 159.289 2200.29 156.604 Q2202.28 153.918 2202.28 149.289 Q2202.28 144.682 2200.29 141.997 Q2198.3 139.289 2194.89 139.289 M2194.89 135.678 Q2200.45 135.678 2203.62 139.289 Q2206.79 142.9 2206.79 149.289 Q2206.79 155.655 2203.62 159.289 Q2200.45 162.9 2194.89 162.9 Q2189.32 162.9 2186.14 159.289 Q2183 155.655 2183 149.289 Q2183 142.9 2186.14 139.289 Q2189.32 135.678 2194.89 135.678 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2230.38 137.067 L2230.38 141.094 Q2228.57 140.169 2226.63 139.706 Q2224.69 139.243 2222.6 139.243 Q2219.43 139.243 2217.83 140.215 Q2216.26 141.187 2216.26 143.131 Q2216.26 144.613 2217.39 145.469 Q2218.53 146.303 2221.95 147.067 L2223.41 147.391 Q2227.95 148.363 2229.85 150.145 Q2231.77 151.905 2231.77 155.076 Q2231.77 158.687 2228.9 160.793 Q2226.05 162.9 2221.05 162.9 Q2218.97 162.9 2216.7 162.483 Q2214.45 162.09 2211.95 161.28 L2211.95 156.881 Q2214.32 158.108 2216.61 158.733 Q2218.9 159.335 2221.14 159.335 Q2224.15 159.335 2225.77 158.317 Q2227.39 157.275 2227.39 155.4 Q2227.39 153.664 2226.21 152.738 Q2225.06 151.812 2221.1 150.956 L2219.62 150.608 Q2215.66 149.775 2213.9 148.062 Q2212.14 146.326 2212.14 143.317 Q2212.14 139.659 2214.73 137.669 Q2217.32 135.678 2222.09 135.678 Q2224.45 135.678 2226.54 136.025 Q2228.62 136.372 2230.38 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2255.08 137.067 L2255.08 141.094 Q2253.27 140.169 2251.33 139.706 Q2249.38 139.243 2247.3 139.243 Q2244.13 139.243 2242.53 140.215 Q2240.96 141.187 2240.96 143.131 Q2240.96 144.613 2242.09 145.469 Q2243.23 146.303 2246.65 147.067 L2248.11 147.391 Q2252.65 148.363 2254.55 150.145 Q2256.47 151.905 2256.47 155.076 Q2256.47 158.687 2253.6 160.793 Q2250.75 162.9 2245.75 162.9 Q2243.67 162.9 2241.4 162.483 Q2239.15 162.09 2236.65 161.28 L2236.65 156.881 Q2239.01 158.108 2241.31 158.733 Q2243.6 159.335 2245.84 159.335 Q2248.85 159.335 2250.47 158.317 Q2252.09 157.275 2252.09 155.4 Q2252.09 153.664 2250.91 152.738 Q2249.75 151.812 2245.8 150.956 L2244.32 150.608 Q2240.36 149.775 2238.6 148.062 Q2236.84 146.326 2236.84 143.317 Q2236.84 139.659 2239.43 137.669 Q2242.02 135.678 2246.79 135.678 Q2249.15 135.678 2251.24 136.025 Q2253.32 136.372 2255.08 137.067 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip970)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1865.93,196.789 2010.36,196.789 "/>
<path clip-path="url(#clip970)" d="M2034.43 188.143 L2038.95 188.143 L2047.05 209.902 L2055.15 188.143 L2059.66 188.143 L2049.94 214.069 L2044.15 214.069 L2034.43 188.143 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2077.33 201.036 Q2072.16 201.036 2070.17 202.217 Q2068.18 203.397 2068.18 206.245 Q2068.18 208.513 2069.66 209.856 Q2071.17 211.175 2073.74 211.175 Q2077.28 211.175 2079.41 208.675 Q2081.56 206.152 2081.56 201.985 L2081.56 201.036 L2077.33 201.036 M2085.82 199.277 L2085.82 214.069 L2081.56 214.069 L2081.56 210.133 Q2080.1 212.495 2077.93 213.629 Q2075.75 214.74 2072.6 214.74 Q2068.62 214.74 2066.26 212.518 Q2063.92 210.272 2063.92 206.522 Q2063.92 202.147 2066.84 199.925 Q2069.78 197.703 2075.59 197.703 L2081.56 197.703 L2081.56 197.286 Q2081.56 194.347 2079.62 192.749 Q2077.7 191.129 2074.2 191.129 Q2071.98 191.129 2069.87 191.661 Q2067.77 192.194 2065.82 193.259 L2065.82 189.323 Q2068.16 188.421 2070.36 187.981 Q2072.56 187.518 2074.64 187.518 Q2080.27 187.518 2083.04 190.434 Q2085.82 193.351 2085.82 199.277 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2094.59 178.05 L2098.85 178.05 L2098.85 214.069 L2094.59 214.069 L2094.59 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2127.46 221.939 L2127.46 225.249 L2102.83 225.249 L2102.83 221.939 L2127.46 221.939 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2131.47 178.05 L2135.73 178.05 L2135.73 214.069 L2131.47 214.069 L2131.47 178.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2154.69 191.129 Q2151.26 191.129 2149.27 193.814 Q2147.28 196.476 2147.28 201.129 Q2147.28 205.782 2149.25 208.467 Q2151.24 211.129 2154.69 211.129 Q2158.09 211.129 2160.08 208.444 Q2162.07 205.758 2162.07 201.129 Q2162.07 196.522 2160.08 193.837 Q2158.09 191.129 2154.69 191.129 M2154.69 187.518 Q2160.24 187.518 2163.41 191.129 Q2166.58 194.74 2166.58 201.129 Q2166.58 207.495 2163.41 211.129 Q2160.24 214.74 2154.69 214.74 Q2149.11 214.74 2145.94 211.129 Q2142.79 207.495 2142.79 201.129 Q2142.79 194.74 2145.94 191.129 Q2149.11 187.518 2154.69 187.518 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2190.17 188.907 L2190.17 192.934 Q2188.37 192.009 2186.42 191.546 Q2184.48 191.083 2182.39 191.083 Q2179.22 191.083 2177.63 192.055 Q2176.05 193.027 2176.05 194.971 Q2176.05 196.453 2177.19 197.309 Q2178.32 198.143 2181.75 198.907 L2183.2 199.231 Q2187.74 200.203 2189.64 201.985 Q2191.56 203.745 2191.56 206.916 Q2191.56 210.527 2188.69 212.633 Q2185.84 214.74 2180.84 214.74 Q2178.76 214.74 2176.49 214.323 Q2174.25 213.93 2171.75 213.12 L2171.75 208.721 Q2174.11 209.948 2176.4 210.573 Q2178.69 211.175 2180.94 211.175 Q2183.95 211.175 2185.57 210.157 Q2187.19 209.115 2187.19 207.24 Q2187.19 205.504 2186.01 204.578 Q2184.85 203.652 2180.89 202.796 L2179.41 202.448 Q2175.45 201.615 2173.69 199.902 Q2171.93 198.166 2171.93 195.157 Q2171.93 191.499 2174.52 189.509 Q2177.12 187.518 2181.88 187.518 Q2184.25 187.518 2186.33 187.865 Q2188.41 188.212 2190.17 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2214.87 188.907 L2214.87 192.934 Q2213.07 192.009 2211.12 191.546 Q2209.18 191.083 2207.09 191.083 Q2203.92 191.083 2202.32 192.055 Q2200.75 193.027 2200.75 194.971 Q2200.75 196.453 2201.88 197.309 Q2203.02 198.143 2206.44 198.907 L2207.9 199.231 Q2212.44 200.203 2214.34 201.985 Q2216.26 203.745 2216.26 206.916 Q2216.26 210.527 2213.39 212.633 Q2210.54 214.74 2205.54 214.74 Q2203.46 214.74 2201.19 214.323 Q2198.94 213.93 2196.44 213.12 L2196.44 208.721 Q2198.81 209.948 2201.1 210.573 Q2203.39 211.175 2205.63 211.175 Q2208.64 211.175 2210.26 210.157 Q2211.88 209.115 2211.88 207.24 Q2211.88 205.504 2210.7 204.578 Q2209.55 203.652 2205.59 202.796 L2204.11 202.448 Q2200.15 201.615 2198.39 199.902 Q2196.63 198.166 2196.63 195.157 Q2196.63 191.499 2199.22 189.509 Q2201.82 187.518 2206.58 187.518 Q2208.94 187.518 2211.03 187.865 Q2213.11 188.212 2214.87 188.907 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip970)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1865.93,248.629 2010.36,248.629 "/>
<path clip-path="url(#clip970)" d="M2034.43 239.983 L2038.95 239.983 L2047.05 261.742 L2055.15 239.983 L2059.66 239.983 L2049.94 265.909 L2044.15 265.909 L2034.43 239.983 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2077.33 252.876 Q2072.16 252.876 2070.17 254.057 Q2068.18 255.237 2068.18 258.085 Q2068.18 260.353 2069.66 261.696 Q2071.17 263.015 2073.74 263.015 Q2077.28 263.015 2079.41 260.515 Q2081.56 257.992 2081.56 253.825 L2081.56 252.876 L2077.33 252.876 M2085.82 251.117 L2085.82 265.909 L2081.56 265.909 L2081.56 261.973 Q2080.1 264.335 2077.93 265.469 Q2075.75 266.58 2072.6 266.58 Q2068.62 266.58 2066.26 264.358 Q2063.92 262.112 2063.92 258.362 Q2063.92 253.987 2066.84 251.765 Q2069.78 249.543 2075.59 249.543 L2081.56 249.543 L2081.56 249.126 Q2081.56 246.187 2079.62 244.589 Q2077.7 242.969 2074.2 242.969 Q2071.98 242.969 2069.87 243.501 Q2067.77 244.034 2065.82 245.099 L2065.82 241.163 Q2068.16 240.261 2070.36 239.821 Q2072.56 239.358 2074.64 239.358 Q2080.27 239.358 2083.04 242.274 Q2085.82 245.191 2085.82 251.117 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2094.59 229.89 L2098.85 229.89 L2098.85 265.909 L2094.59 265.909 L2094.59 229.89 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2127.46 273.779 L2127.46 277.089 L2102.83 277.089 L2102.83 273.779 L2127.46 273.779 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2143.25 252.876 Q2138.09 252.876 2136.1 254.057 Q2134.11 255.237 2134.11 258.085 Q2134.11 260.353 2135.59 261.696 Q2137.09 263.015 2139.66 263.015 Q2143.2 263.015 2145.33 260.515 Q2147.49 257.992 2147.49 253.825 L2147.49 252.876 L2143.25 252.876 M2151.75 251.117 L2151.75 265.909 L2147.49 265.909 L2147.49 261.973 Q2146.03 264.335 2143.85 265.469 Q2141.68 266.58 2138.53 266.58 Q2134.55 266.58 2132.19 264.358 Q2129.85 262.112 2129.85 258.362 Q2129.85 253.987 2132.76 251.765 Q2135.7 249.543 2141.51 249.543 L2147.49 249.543 L2147.49 249.126 Q2147.49 246.187 2145.54 244.589 Q2143.62 242.969 2140.13 242.969 Q2137.9 242.969 2135.8 243.501 Q2133.69 244.034 2131.75 245.099 L2131.75 241.163 Q2134.08 240.261 2136.28 239.821 Q2138.48 239.358 2140.57 239.358 Q2146.19 239.358 2148.97 242.274 Q2151.75 245.191 2151.75 251.117 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2179.18 240.978 L2179.18 244.96 Q2177.37 243.964 2175.54 243.478 Q2173.74 242.969 2171.88 242.969 Q2167.74 242.969 2165.45 245.608 Q2163.16 248.224 2163.16 252.969 Q2163.16 257.714 2165.45 260.353 Q2167.74 262.969 2171.88 262.969 Q2173.74 262.969 2175.54 262.483 Q2177.37 261.973 2179.18 260.978 L2179.18 264.913 Q2177.39 265.747 2175.47 266.163 Q2173.57 266.58 2171.42 266.58 Q2165.57 266.58 2162.12 262.899 Q2158.67 259.219 2158.67 252.969 Q2158.67 246.626 2162.14 242.992 Q2165.63 239.358 2171.7 239.358 Q2173.67 239.358 2175.54 239.775 Q2177.42 240.168 2179.18 240.978 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip970)" d="M2205.24 240.978 L2205.24 244.96 Q2203.44 243.964 2201.61 243.478 Q2199.8 242.969 2197.95 242.969 Q2193.81 242.969 2191.51 245.608 Q2189.22 248.224 2189.22 252.969 Q2189.22 257.714 2191.51 260.353 Q2193.81 262.969 2197.95 262.969 Q2199.8 262.969 2201.61 262.483 Q2203.44 261.973 2205.24 260.978 L2205.24 264.913 Q2203.46 265.747 2201.54 266.163 Q2199.64 266.58 2197.49 266.58 Q2191.63 266.58 2188.18 262.899 Q2184.73 259.219 2184.73 252.969 Q2184.73 246.626 2188.2 242.992 Q2191.7 239.358 2197.76 239.358 Q2199.73 239.358 2201.61 239.775 Q2203.48 240.168 2205.24 240.978 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="language-julia hljs"></code></pre><pre><code class="nohighlight hljs">1</code></pre><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MCNN_3/">« -</a><a class="docs-footer-nextpage" href="../MCNN_5/">- »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
