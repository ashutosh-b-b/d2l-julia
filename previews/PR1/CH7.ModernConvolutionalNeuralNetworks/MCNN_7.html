<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Densely Connected Networks (DenseNet) | d2l Julia</title>
    <meta name="description" content="Documentation for d2l-julia">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/d2l-julia/previews/PR1/assets/style.BUOi7SFr.css" as="style">
    <link rel="preload stylesheet" href="/d2l-julia/previews/PR1/vp-icons.css" as="style">
    
    <script type="module" src="/d2l-julia/previews/PR1/assets/app.DyCk50An.js"></script>
    <link rel="preload" href="/d2l-julia/previews/PR1/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/chunks/theme.CszcN3qP.js">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/chunks/framework.DjA5121Y.js">
    <link rel="modulepreload" href="/d2l-julia/previews/PR1/assets/CH7.ModernConvolutionalNeuralNetworks_MCNN_7.md.BY0qhW6e.lean.js">
    <script src="/d2l-julia/versions.js"></script>
    <script src="/d2l-julia/previews/PR1/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/d2l-julia/previews/PR1/" data-v-0f4f798b><!--[--><!--]--><!--[--><img class="VPImage logo" src="/d2l-julia/previews/PR1/logo.png" width="24" height="24" alt data-v-35a7d0b8><!--]--><span data-v-0f4f798b>d2l Julia</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/d2l-julia/previews/PR1/index" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Chapters</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/chapters" data-v-acbfed09><!--[--><span data-v-acbfed09>ðŸ“˜ Chapters Overview</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Linear Neural Networks for Regression</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Linear Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Multiple Dispatch Design for Implementation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Synthetic Regression Data</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Linear Regression Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Linear Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Weight Decay</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Linear Neural Networks for Classification</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Softmax Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>The Image Classification Dataset</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Softmax Regression Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Softmax Regression</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization in Classification</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Environment and Distribution Shift</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Multilayer Perceptron</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Multilayer Perceptrons</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Implementation of Multilayer Perceptrons</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Forward Propagation, Backward Propagation, and Computational Graphs</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Numerical Stability and Initialization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Generalization in Deep Learning</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Dropout</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Convolutional Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Convolutions for Images</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Padding and Stride</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Multiple Input and Multiple Output Channels</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Pooling</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Convolutional Neural Networks (LeNet)</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Modern Convolutional Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_0" data-v-acbfed09><!--[--><span data-v-acbfed09>Modern Convolutional Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Deep Convolutional Neural Networks (AlexNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Networks Using Blocks (VGG)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Network in Network (NiN)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Branch Networks  (GoogLeNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Batch Normalization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Residual Networks (ResNet) and ResNeXt</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link active" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Densely Connected Networks (DenseNet)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-acbfed09><!--[--><span data-v-acbfed09>Designing Convolution Network Architectures</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Recurrent Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_0" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Working with Sequences</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Converting Raw Text into Sequence Data</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Language Models</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Recurrent Neural Network Implementation from Scratch</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Concise Implementation of Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Backpropagation Through Time</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Modern Recurrent Neural Networks</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Long Short-Term Memory (LSTM)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Gated Recurrent Units (GRU)</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Deep Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>Bidirectional Recurrent Neural Networks</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Machine Translation and the Dataset</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>The Encoderâ€“Decoder Architecture</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_7" data-v-acbfed09><!--[--><span data-v-acbfed09>Sequence-to-Sequence Learning for Machine Translation</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Attention Mechanisms and Transformers</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_1" data-v-acbfed09><!--[--><span data-v-acbfed09>Queries, Keys, and Values</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_2" data-v-acbfed09><!--[--><span data-v-acbfed09>Attention Pooling by Similarity</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_3" data-v-acbfed09><!--[--><span data-v-acbfed09>Attention Scoring Functions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_4" data-v-acbfed09><!--[--><span data-v-acbfed09>The Bahdanau Attention Mechanism</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_5" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Head Attention</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_6" data-v-acbfed09><!--[--><span data-v-acbfed09>Self-Attention and Positional Encoding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-acbfed09><!--[--><span data-v-acbfed09>CH10.Attention_Mechanisms_and_Transformers/Untitled</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/d2l-julia/previews/PR1/references" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>References</span><!--]--></a><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ashutosh-b-b/d2l-julia" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/ashutosh-b-b/d2l-julia" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/index" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Home</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible has-active" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Chapters</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/chapters" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>ðŸ“˜ Chapters Overview</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Linear Neural Networks for Regression</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Linear Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multiple Dispatch Design for Implementation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Synthetic Regression Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Linear Regression Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Linear Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH3.Linear_Regression/LNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Weight Decay</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Linear Neural Networks for Classification</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Softmax Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Image Classification Dataset</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Softmax Regression Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Softmax Regression</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization in Classification</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH4.Linear_Classification/LCN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Environment and Distribution Shift</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Multilayer Perceptron</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multilayer Perceptrons</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Implementation of Multilayer Perceptrons</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Forward Propagation, Backward Propagation, and Computational Graphs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Numerical Stability and Initialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Generalization in Deep Learning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH5.MLP/MLP_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Dropout</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Convolutional Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Convolutions for Images</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Padding and Stride</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multiple Input and Multiple Output Channels</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Pooling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH6.Convolutional_Neural_Networks/CNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Convolutional Neural Networks (LeNet)</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible has-active" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Modern Convolutional Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_0" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Modern Convolutional Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Deep Convolutional Neural Networks (AlexNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Networks Using Blocks (VGG)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Network in Network (NiN)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multi-Branch Networks  (GoogLeNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Batch Normalization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Residual Networks (ResNet) and ResNeXt</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Densely Connected Networks (DenseNet)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Designing Convolution Network Architectures</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Recurrent Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_0" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Working with Sequences</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Converting Raw Text into Sequence Data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Language Models</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Recurrent Neural Network Implementation from Scratch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Concise Implementation of Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH8.Recurrent_Neural_Networks/RNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Backpropagation Through Time</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Modern Recurrent Neural Networks</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Long Short-Term Memory (LSTM)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Gated Recurrent Units (GRU)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Deep Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Bidirectional Recurrent Neural Networks</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Machine Translation and the Dataset</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Encoderâ€“Decoder Architecture</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH9.Modern_Recurrent_Neural_Networks/MRNN_7" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Sequence-to-Sequence Learning for Machine Translation</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Attention Mechanisms and Transformers</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_1" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Queries, Keys, and Values</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_2" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Attention Pooling by Similarity</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_3" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Attention Scoring Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_4" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>The Bahdanau Attention Mechanism</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_5" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Multi-Head Attention</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/ATTN_6" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Self-Attention and Positional Encoding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/CH10.Attention_Mechanisms_and_Transformers/Untitled" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>CH10.Attention_Mechanisms_and_Transformers/Untitled</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/d2l-julia/previews/PR1/references" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>References</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _d2l-julia_previews_PR1_CH7_ModernConvolutionalNeuralNetworks_MCNN_7" data-v-83890dd9><div><h1 id="Densely-Connected-Networks-DenseNet" tabindex="-1">Densely Connected Networks (DenseNet) <a class="header-anchor" href="#Densely-Connected-Networks-DenseNet" aria-label="Permalink to &quot;Densely Connected Networks (DenseNet) {#Densely-Connected-Networks-DenseNet}&quot;">â€‹</a></h1><p>ResNet significantly changed the view of how to parametrize the functions in deep networks. <em>DenseNet</em> (dense convolutional network) is to some extent the logical extension of this [<a href="/d2l-julia/previews/PR1/references#Huang_Liu_Van-Der-Maaten_ea_2017">94</a>]. DenseNet is characterized by both the connectivity pattern where each layer connects to all the preceding layers and the concatenation operation (rather than the addition operator in ResNet) to preserve and reuse features from earlier layers. To understand how to arrive at it, let&#39;s take a small detour to mathematics.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Pkg; Pkg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">activate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;../../d2lai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d2lai</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Flux </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CUDA, cuDNN</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  Activating project at `/workspace/d2l-julia/d2lai`</span></span></code></pre></div><h2 id="From-ResNet-to-DenseNet" tabindex="-1">From ResNet to DenseNet <a class="header-anchor" href="#From-ResNet-to-DenseNet" aria-label="Permalink to &quot;From ResNet to DenseNet {#From-ResNet-to-DenseNet}&quot;">â€‹</a></h2><p>Recall the Taylor expansion for functions. At the point <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.442ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2405.6 748" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(849.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1905.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>=</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container> it can be written as</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-2.148ex;" xmlns="http://www.w3.org/2000/svg" width="59.709ex" height="5.472ex" role="img" focusable="false" viewBox="0 -1469 26391.4 2418.5" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1511,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2177.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3233.6,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3783.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(4172.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4672.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5283.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6284,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7078.2,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(7578.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(528,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(636,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1408.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1797.5,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2297.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2908.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3908.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4703.1,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(5203.3,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z" style="stroke-width:3;"></path></g><g data-mml-node="mfrac" transform="translate(528,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(636,363) scale(0.707)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(1074.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1463.9,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1963.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(1007.5,-686)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(500,0)"><path data-c="21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z" style="stroke-width:3;"></path></g></g><rect width="2552.9" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3543.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4543.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5337.6,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(5837.8,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z" style="stroke-width:3;"></path></g><g data-mml-node="mfrac" transform="translate(528,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(636,363) scale(0.707)"><g data-c="2034"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)" style="stroke-width:3;"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(550,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(1269.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1658.4,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2158.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(1104.7,-686)"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(500,0)"><path data-c="21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z" style="stroke-width:3;"></path></g></g><rect width="2747.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3737.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4737.8,0)"><path data-c="22EF" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250ZM525 250Q525 274 542 292T585 310Q609 310 627 294T646 251Q646 226 629 208T586 190T543 207T525 250ZM972 250Q972 274 989 292T1032 310Q1056 310 1074 294T1093 251Q1093 226 1076 208T1033 190T990 207T972 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5909.8,0) translate(0 -0.5)"><path data-c="5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(12275.6,0) translate(0 -0.5)"><path data-c="5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(18006.9,0) translate(0 -0.5)"><path data-c="5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(26113.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;overflow:hidden;width:100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi><mo>â‹…</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><msup><mi>f</mi><mo data-mjx-alternate="1">â€²</mo></msup><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi><mo>â‹…</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mfrac><mrow><msup><mi>f</mi><mo data-mjx-alternate="1">â€³</mo></msup><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn><mo>!</mo></mrow></mfrac><mo>+</mo><mi>x</mi><mo>â‹…</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mfrac><mrow><msup><mi>f</mi><mo data-mjx-alternate="1">â€´</mo></msup><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><mrow><mn>3</mn><mo>!</mo></mrow></mfrac><mo>+</mo><mo>â‹¯</mo><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo>.</mo></math></mjx-assistive-mml></mjx-container><p>The key point is that it decomposes a function into terms of increasingly higher order. In a similar vein, ResNet decomposes functions into</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="16.376ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7238 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(939,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1546,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2212.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3268.6,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(4097.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5098,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5575,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5964,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(6571,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6960,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;overflow:hidden;width:100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>f</mi><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>+</mo><mi>g</mi><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>.</mo></math></mjx-assistive-mml></mjx-container><p>That is, ResNet decomposes <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 550 910" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container> into a simple linear term and a more complex nonlinear one. What if we wanted to capture (not necessarily add) information beyond two terms? One such solution is DenseNet [<a href="/d2l-julia/previews/PR1/references#Huang_Liu_Van-Der-Maaten_ea_2017">94</a>]. <img src="/d2l-julia/previews/PR1/assets/densenet-block.C55eK17Z.svg" id="fig_densenet_block"></p><p><em>The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation.</em></p><p>As shown in <a href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7#fig_densenet_block">Figure</a>, the key difference between ResNet and DenseNet is that in the latter case outputs are <em>concatenated</em> (denoted by <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.264ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1000.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(278,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(722.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mo>,</mo><mo stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container>) rather than added. As a result, we perform a mapping from <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow></math></mjx-assistive-mml></mjx-container> to its values after applying an increasingly complex sequence of functions:</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="61.497ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 27181.8 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(884.8,0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(2162.6,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(885,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1329.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2256.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2645.2,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3252.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3641.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4085.9,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(5179.1,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(389,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(885,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1329.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(2422.9,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3807.9,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(4474.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(10043,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(10487.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(11580.9,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(389,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(885,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1329.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(2422.9,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3807.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4252.6,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(5345.8,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(389,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(885,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1329.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(2422.9,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3807.9,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(4474.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(10209.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(10876.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(22846.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(23291.2,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(24463.2,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(26903.8,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;overflow:hidden;width:100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo accent="false" stretchy="false">â†’</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>,</mo><msub><mi>f</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>,</mo><msub><mi>f</mi><mn>1</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mi>f</mi><mn>3</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>,</mo><msub><mi>f</mi><mn>1</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>,</mo><msub><mi>f</mi><mn>1</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mo>â€¦</mo><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo>.</mo></math></mjx-assistive-mml></mjx-container><p>In the end, all these functions are combined in MLP to reduce the number of features again. In terms of implementation this is quite simple: rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The final layer of such a chain is densely connected to all previous layers. The dense connections are shown in <a href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_7#fig_densenet">Figure</a>. <img src="/d2l-julia/previews/PR1/assets/densenet.Dus3pNUB.svg" id="fig_densenet"></p><p><em>Dense connections in DenseNet. Note how the dimensionality increases with depth.</em></p><p>The main components that comprise a DenseNet are <em>dense blocks</em> and <em>transition layers</em>. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large, since the expansion <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="31.901ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 14100.2 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(884.8,0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(2162.6,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(885,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1329.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2256.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2645.2,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3252.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3641.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4085.9,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(5179.1,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(389,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(885,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1329.7,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(2422.9,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3807.9,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(4474.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(10043,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10487.7,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(11659.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo accent="false" stretchy="false">â†’</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>,</mo><msub><mi>f</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo>,</mo><msub><mi>f</mi><mn>1</mn></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">x</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mo>â€¦</mo><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> can be quite high-dimensional.</p><h2 id="Dense-Blocks" tabindex="-1">Dense Blocks <a class="header-anchor" href="#Dense-Blocks" aria-label="Permalink to &quot;Dense Blocks {#Dense-Blocks}&quot;">â€‹</a></h2><p>DenseNet uses the modified &quot;batch normalization, activation, and convolution&quot; structure of ResNet (see the exercise in :numref:<code>sec_resnet</code>). First, we implement this convolution block structure.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetConvBlock{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@layer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetConvBlock</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DenseNetConvBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">net</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseNetConvBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, num_channels</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Chain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Conv</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), channel_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_channels, pad </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        BatchNorm</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(num_channels, relu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DenseNetConvBlock</span></span></code></pre></div><p>A <em>dense block</em> consists of multiple convolution blocks, each using the same number of output channels. In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension. Lazy evaluation allows us to adjust the dimensionality automatically.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseBlock{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@layer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseBlock</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, num_convs, num_channels; return_output_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> channel_in</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    conv_layers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> map</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">num_convs) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">do</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNetConvBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prev_channels, num_channels)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_channels </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> block</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(conv_layers)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> return_output_channels</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> net, prev_channels </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> net</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DenseBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(x)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> block</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> cat</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x, y; dims </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><p>In the following example, we define a <code>DenseBlock</code> instance with two convolution blocks of 10 output channels. When using an input with three channels, we will get an output with <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.186ex;" xmlns="http://www.w3.org/2000/svg" width="16.467ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 7278.4 748" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2944.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(3944.9,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5222.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(6278.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(500,0)" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn><mo>+</mo><mn>10</mn><mo>+</mo><mn>10</mn><mo>=</mo><mn>23</mn></math></mjx-assistive-mml></mjx-container> channels. The number of convolution block channels controls the growth in the number of output channels relative to the number of input channels. This is also referred to as the <em>growth rate</em>.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">block</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> size</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>(8, 8, 23, 16)</span></span></code></pre></div><h2 id="Transition-Layers" tabindex="-1">Transition Layers <a class="header-anchor" href="#Transition-Layers" aria-label="Permalink to &quot;Transition Layers {#Transition-Layers}&quot;">â€‹</a></h2><p>Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A <em>transition layer</em> is used to control the complexity of the model. It reduces the number of channels by using a <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0;" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 2222.4 666" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>Ã—</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> convolution. Moreover, it halves the height and width via average pooling with a stride of 2.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetTransitionBlock{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@layer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetTransitionBlock</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseNetTransitionBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in, num_channels)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Chain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        BatchNorm</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in, relu),</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Conv</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), channel_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_channels),</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        MeanPool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), pad </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, stride </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dtb</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DenseNetTransitionBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dtb</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">net</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x)</span></span></code></pre></div><p>Apply a transition layer with 10 channels to the output of the dense block in the previous example. This reduces the number of output channels to 10, and halves the height and width.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNetTransitionBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">23</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">block</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">23</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> size</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>(5, 5, 10, 16)</span></span></code></pre></div><h2 id="DenseNet-Model" tabindex="-1">DenseNet Model <a class="header-anchor" href="#DenseNet-Model" aria-label="Permalink to &quot;DenseNet Model {#DenseNet-Model}&quot;">â€‹</a></h2><p>Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and max-pooling layer as in ResNet.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetB1{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@layer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetB1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(b1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DenseNetB1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">net</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseNetB1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Chain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Conv</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">7</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">7</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), channel_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, pad </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, stride </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        BatchNorm</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, relu),</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        MaxPool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">((</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), stride </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, pad </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    DenseNetB1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DenseNetB1</span></span></code></pre></div><p>Then, similar to the four modules made up of residual blocks that ResNet uses, DenseNet uses four dense blocks. As with ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4, consistent with the ResNet-18 model in :numref:<code>sec_resnet</code>. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block.</p><p>In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. Here, we use the transition layer to halve the height and width and halve the number of channels. Similar to ResNet, a global pooling layer and a fully connected layer are connected at the end to produce the output.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNet{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AbstractClassifier</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@layer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNet </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">net</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; growth_rate </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, arch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), num_classes </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 64</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    layers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">length</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(arch)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        block, prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prev_channels, arch[i], growth_rate; return_output_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        push!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(layers, block)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">!=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> length</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(arch)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            transition_layer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNetTransitionBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prev_channels, prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">Ã·</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> prev_channels </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">Ã·</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            push!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(layers, transition_layer)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@autosize</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">96</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">96</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Chain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        DenseNetB1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(channel_in),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        layers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        GlobalMeanPool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        Flux</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">flatten,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Dense</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(_ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_classes),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        softmax</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DenseNet</span></span></code></pre></div><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DenseNet(</span></span>
<span class="line"><span>  Chain(</span></span>
<span class="line"><span>    DenseNetB1(</span></span>
<span class="line"><span>      Chain(</span></span>
<span class="line"><span>        Conv((7, 7), 1 =&gt; 64, pad=3, stride=2),  # 3_200 parameters</span></span>
<span class="line"><span>        BatchNorm(64, relu),            # 128 parameters, plus 128</span></span>
<span class="line"><span>        MaxPool((3, 3), pad=1, stride=2),</span></span>
<span class="line"><span>      ),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 64 =&gt; 32, pad=1),  # 18_464 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 96 =&gt; 32, pad=1),  # 27_680 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    Chain(</span></span>
<span class="line"><span>      BatchNorm(192, relu),             # 384 parameters, plus 384</span></span>
<span class="line"><span>      Conv((1, 1), 192 =&gt; 96),          # 18_528 parameters</span></span>
<span class="line"><span>      MeanPool((2, 2), pad=1),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 96 =&gt; 32, pad=1),  # 27_680 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 192 =&gt; 32, pad=1),  # 55_328 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    Chain(</span></span>
<span class="line"><span>      BatchNorm(224, relu),             # 448 parameters, plus 448</span></span>
<span class="line"><span>      Conv((1, 1), 224 =&gt; 112),         # 25_200 parameters</span></span>
<span class="line"><span>      MeanPool((2, 2), pad=1),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 112 =&gt; 32, pad=1),  # 32_288 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 144 =&gt; 32, pad=1),  # 41_504 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 176 =&gt; 32, pad=1),  # 50_720 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 208 =&gt; 32, pad=1),  # 59_936 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    Chain(</span></span>
<span class="line"><span>      BatchNorm(240, relu),             # 480 parameters, plus 480</span></span>
<span class="line"><span>      Conv((1, 1), 240 =&gt; 120),         # 28_920 parameters</span></span>
<span class="line"><span>      MeanPool((2, 2), pad=1),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 120 =&gt; 32, pad=1),  # 34_592 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 152 =&gt; 32, pad=1),  # 43_808 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 184 =&gt; 32, pad=1),  # 53_024 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 216 =&gt; 32, pad=1),  # 62_240 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    GlobalMeanPool(),</span></span>
<span class="line"><span>    Flux.flatten,</span></span>
<span class="line"><span>    Dense(248 =&gt; 10),                   # 2_490 parameters</span></span>
<span class="line"><span>    NNlib.softmax,</span></span>
<span class="line"><span>  ),</span></span>
<span class="line"><span>)         # Total: 82 trainable arrays, 754_082 parameters,</span></span>
<span class="line"><span>          # plus 40 non-trainable, 2_464 parameters, summarysize 2.894 MiB.</span></span></code></pre></div><h2 id="Training" tabindex="-1">Training <a class="header-anchor" href="#Training" aria-label="Permalink to &quot;Training {#Training}&quot;">â€‹</a></h2><p>Since we are using a deeper network here, in this section, we will reduce the input height and width from 224 to 96 to simplify the computation.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d2lai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">FashionMNISTData</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(batchsize </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 128</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, resize </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">96</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">96</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">opt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Descent</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.01</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">trainer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Trainer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model, data, opt; max_epochs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, gpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, board_yscale </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> :identity</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d2lai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">fit</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(trainer);</span></span></code></pre></div><div style="max-height:300px;overflow-y:auto;background:#111;color:#eee;padding:1em;border-radius:5px;"><pre>    [ Info: Train Loss: 0.63423675, Val Loss: 0.44235954, Val Acc: 0.875
    [ Info: Train Loss: 0.26092514, Val Loss: 0.35349703, Val Acc: 0.9375
    [ Info: Train Loss: 0.23870032, Val Loss: 0.54305226, Val Acc: 0.8125
    [ Info: Train Loss: 0.36317107, Val Loss: 0.43822515, Val Acc: 0.875
    [ Info: Train Loss: 0.3757471, Val Loss: 0.3553314, Val Acc: 0.875
    [ Info: Train Loss: 0.16658472, Val Loss: 0.48415565, Val Acc: 0.875
    [ Info: Train Loss: 0.16947578, Val Loss: 0.3061001, Val Acc: 0.875
    [ Info: Train Loss: 0.25147822, Val Loss: 0.4952318, Val Acc: 0.8125
    [ Info: Train Loss: 0.14965303, Val Loss: 0.32668728, Val Acc: 0.875
    [ Info: Train Loss: 0.14355798, Val Loss: 0.32147193, Val Acc: 0.875</pre></div><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600"><defs><clipPath id="clip230"><rect x="0" y="0" width="2400" height="1600"></rect></clipPath></defs><path clip-path="url(#clip230)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path><defs><clipPath id="clip231"><rect x="480" y="0" width="1681" height="1600"></rect></clipPath></defs><path clip-path="url(#clip230)" d="M156.598 1423.18 L2352.76 1423.18 L2352.76 47.2441 L156.598 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path><defs><clipPath id="clip232"><rect x="156" y="47" width="2197" height="1377"></rect></clipPath></defs><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="448.959,1423.18 448.959,47.2441 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="909.369,1423.18 909.369,47.2441 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="1369.78,1423.18 1369.78,47.2441 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="1830.19,1423.18 1830.19,47.2441 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="2290.6,1423.18 2290.6,47.2441 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="156.598,1408.07 2352.76,1408.07 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="156.598,1045.43 2352.76,1045.43 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="156.598,682.786 2352.76,682.786 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:2;stroke-opacity:0.1;fill:none;" points="156.598,320.145 2352.76,320.145 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="156.598,1423.18 2352.76,1423.18 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="448.959,1423.18 448.959,1404.28 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="909.369,1423.18 909.369,1404.28 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="1369.78,1423.18 1369.78,1404.28 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="1830.19,1423.18 1830.19,1404.28 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="2290.6,1423.18 2290.6,1404.28 "></polyline><path clip-path="url(#clip230)" d="M443.612 1481.64 L459.931 1481.64 L459.931 1485.58 L437.987 1485.58 L437.987 1481.64 Q440.649 1478.89 445.232 1474.26 Q449.838 1469.61 451.019 1468.27 Q453.264 1465.74 454.144 1464.01 Q455.047 1462.25 455.047 1460.56 Q455.047 1457.8 453.102 1456.07 Q451.181 1454.33 448.079 1454.33 Q445.88 1454.33 443.426 1455.09 Q440.996 1455.86 438.218 1457.41 L438.218 1452.69 Q441.042 1451.55 443.496 1450.97 Q445.95 1450.39 447.987 1450.39 Q453.357 1450.39 456.551 1453.08 Q459.746 1455.77 459.746 1460.26 Q459.746 1462.39 458.936 1464.31 Q458.149 1466.2 456.042 1468.8 Q455.463 1469.47 452.362 1472.69 Q449.26 1475.88 443.612 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M912.379 1455.09 L900.573 1473.54 L912.379 1473.54 L912.379 1455.09 M911.152 1451.02 L917.031 1451.02 L917.031 1473.54 L921.962 1473.54 L921.962 1477.43 L917.031 1477.43 L917.031 1485.58 L912.379 1485.58 L912.379 1477.43 L896.777 1477.43 L896.777 1472.92 L911.152 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1370.18 1466.44 Q1367.04 1466.44 1365.18 1468.59 Q1363.36 1470.74 1363.36 1474.49 Q1363.36 1478.22 1365.18 1480.39 Q1367.04 1482.55 1370.18 1482.55 Q1373.33 1482.55 1375.16 1480.39 Q1377.01 1478.22 1377.01 1474.49 Q1377.01 1470.74 1375.16 1468.59 Q1373.33 1466.44 1370.18 1466.44 M1379.47 1451.78 L1379.47 1456.04 Q1377.71 1455.21 1375.9 1454.77 Q1374.12 1454.33 1372.36 1454.33 Q1367.73 1454.33 1365.28 1457.45 Q1362.85 1460.58 1362.5 1466.9 Q1363.87 1464.89 1365.93 1463.82 Q1367.99 1462.73 1370.46 1462.73 Q1375.67 1462.73 1378.68 1465.9 Q1381.71 1469.05 1381.71 1474.49 Q1381.71 1479.82 1378.56 1483.03 Q1375.42 1486.25 1370.18 1486.25 Q1364.19 1486.25 1361.02 1481.67 Q1357.85 1477.06 1357.85 1468.33 Q1357.85 1460.14 1361.74 1455.28 Q1365.62 1450.39 1372.18 1450.39 Q1373.93 1450.39 1375.72 1450.74 Q1377.52 1451.09 1379.47 1451.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1830.19 1469.17 Q1826.86 1469.17 1824.94 1470.95 Q1823.04 1472.73 1823.04 1475.86 Q1823.04 1478.98 1824.94 1480.77 Q1826.86 1482.55 1830.19 1482.55 Q1833.52 1482.55 1835.44 1480.77 Q1837.37 1478.96 1837.37 1475.86 Q1837.37 1472.73 1835.44 1470.95 Q1833.55 1469.17 1830.19 1469.17 M1825.51 1467.18 Q1822.5 1466.44 1820.82 1464.38 Q1819.15 1462.32 1819.15 1459.35 Q1819.15 1455.21 1822.09 1452.8 Q1825.05 1450.39 1830.19 1450.39 Q1835.35 1450.39 1838.29 1452.8 Q1841.23 1455.21 1841.23 1459.35 Q1841.23 1462.32 1839.54 1464.38 Q1837.88 1466.44 1834.89 1467.18 Q1838.27 1467.96 1840.14 1470.26 Q1842.04 1472.55 1842.04 1475.86 Q1842.04 1480.88 1838.96 1483.57 Q1835.91 1486.25 1830.19 1486.25 Q1824.47 1486.25 1821.39 1483.57 Q1818.34 1480.88 1818.34 1475.86 Q1818.34 1472.55 1820.24 1470.26 Q1822.13 1467.96 1825.51 1467.18 M1823.8 1459.79 Q1823.8 1462.48 1825.47 1463.98 Q1827.16 1465.49 1830.19 1465.49 Q1833.2 1465.49 1834.89 1463.98 Q1836.6 1462.48 1836.6 1459.79 Q1836.6 1457.11 1834.89 1455.6 Q1833.2 1454.1 1830.19 1454.1 Q1827.16 1454.1 1825.47 1455.6 Q1823.8 1457.11 1823.8 1459.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M2265.29 1481.64 L2272.93 1481.64 L2272.93 1455.28 L2264.62 1456.95 L2264.62 1452.69 L2272.88 1451.02 L2277.56 1451.02 L2277.56 1481.64 L2285.2 1481.64 L2285.2 1485.58 L2265.29 1485.58 L2265.29 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M2304.64 1454.1 Q2301.03 1454.1 2299.2 1457.66 Q2297.39 1461.2 2297.39 1468.33 Q2297.39 1475.44 2299.2 1479.01 Q2301.03 1482.55 2304.64 1482.55 Q2308.27 1482.55 2310.08 1479.01 Q2311.91 1475.44 2311.91 1468.33 Q2311.91 1461.2 2310.08 1457.66 Q2308.27 1454.1 2304.64 1454.1 M2304.64 1450.39 Q2310.45 1450.39 2313.51 1455 Q2316.58 1459.58 2316.58 1468.33 Q2316.58 1477.06 2313.51 1481.67 Q2310.45 1486.25 2304.64 1486.25 Q2298.83 1486.25 2295.75 1481.67 Q2292.7 1477.06 2292.7 1468.33 Q2292.7 1459.58 2295.75 1455 Q2298.83 1450.39 2304.64 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1174.87 1548.76 L1174.87 1551.62 L1147.94 1551.62 Q1148.32 1557.67 1151.57 1560.85 Q1154.85 1564 1160.67 1564 Q1164.05 1564 1167.2 1563.17 Q1170.38 1562.35 1173.5 1560.69 L1173.5 1566.23 Q1170.35 1567.57 1167.04 1568.27 Q1163.73 1568.97 1160.32 1568.97 Q1151.79 1568.97 1146.79 1564 Q1141.83 1559.04 1141.83 1550.57 Q1141.83 1541.82 1146.54 1536.69 Q1151.28 1531.54 1159.3 1531.54 Q1166.5 1531.54 1170.67 1536.18 Q1174.87 1540.8 1174.87 1548.76 M1169.01 1547.04 Q1168.95 1542.23 1166.31 1539.37 Q1163.7 1536.5 1159.37 1536.5 Q1154.46 1536.5 1151.5 1539.27 Q1148.58 1542.04 1148.13 1547.07 L1169.01 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1190.14 1562.7 L1190.14 1581.6 L1184.26 1581.6 L1184.26 1532.4 L1190.14 1532.4 L1190.14 1537.81 Q1191.99 1534.62 1194.79 1533.1 Q1197.62 1531.54 1201.54 1531.54 Q1208.03 1531.54 1212.07 1536.69 Q1216.15 1541.85 1216.15 1550.25 Q1216.15 1558.65 1212.07 1563.81 Q1208.03 1568.97 1201.54 1568.97 Q1197.62 1568.97 1194.79 1567.44 Q1191.99 1565.88 1190.14 1562.7 M1210.07 1550.25 Q1210.07 1543.79 1207.4 1540.13 Q1204.75 1536.44 1200.11 1536.44 Q1195.46 1536.44 1192.79 1540.13 Q1190.14 1543.79 1190.14 1550.25 Q1190.14 1556.71 1192.79 1560.4 Q1195.46 1564.07 1200.11 1564.07 Q1204.75 1564.07 1207.4 1560.4 Q1210.07 1556.71 1210.07 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1239.67 1536.5 Q1234.96 1536.5 1232.22 1540.19 Q1229.48 1543.85 1229.48 1550.25 Q1229.48 1556.65 1232.19 1560.34 Q1234.93 1564 1239.67 1564 Q1244.35 1564 1247.09 1560.31 Q1249.82 1556.62 1249.82 1550.25 Q1249.82 1543.92 1247.09 1540.23 Q1244.35 1536.5 1239.67 1536.5 M1239.67 1531.54 Q1247.31 1531.54 1251.67 1536.5 Q1256.03 1541.47 1256.03 1550.25 Q1256.03 1559 1251.67 1564 Q1247.31 1568.97 1239.67 1568.97 Q1232 1568.97 1227.64 1564 Q1223.31 1559 1223.31 1550.25 Q1223.31 1541.47 1227.64 1536.5 Q1232 1531.54 1239.67 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1291.39 1533.76 L1291.39 1539.24 Q1288.91 1537.87 1286.39 1537.2 Q1283.91 1536.5 1281.37 1536.5 Q1275.67 1536.5 1272.52 1540.13 Q1269.37 1543.73 1269.37 1550.25 Q1269.37 1556.78 1272.52 1560.4 Q1275.67 1564 1281.37 1564 Q1283.91 1564 1286.39 1563.33 Q1288.91 1562.63 1291.39 1561.26 L1291.39 1566.68 Q1288.94 1567.82 1286.3 1568.39 Q1283.69 1568.97 1280.73 1568.97 Q1272.68 1568.97 1267.93 1563.91 Q1263.19 1558.85 1263.19 1550.25 Q1263.19 1541.53 1267.97 1536.53 Q1272.77 1531.54 1281.11 1531.54 Q1283.82 1531.54 1286.39 1532.11 Q1288.97 1532.65 1291.39 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1331.21 1546.53 L1331.21 1568.04 L1325.35 1568.04 L1325.35 1546.72 Q1325.35 1541.66 1323.38 1539.14 Q1321.41 1536.63 1317.46 1536.63 Q1312.72 1536.63 1309.98 1539.65 Q1307.24 1542.68 1307.24 1547.9 L1307.24 1568.04 L1301.35 1568.04 L1301.35 1518.52 L1307.24 1518.52 L1307.24 1537.93 Q1309.34 1534.72 1312.18 1533.13 Q1315.04 1531.54 1318.76 1531.54 Q1324.91 1531.54 1328.06 1535.36 Q1331.21 1539.14 1331.21 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M1365.62 1533.45 L1365.62 1538.98 Q1363.13 1537.71 1360.46 1537.07 Q1357.79 1536.44 1354.92 1536.44 Q1350.56 1536.44 1348.36 1537.77 Q1346.2 1539.11 1346.2 1541.79 Q1346.2 1543.82 1347.76 1545 Q1349.32 1546.15 1354.03 1547.2 L1356.04 1547.64 Q1362.27 1548.98 1364.88 1551.43 Q1367.53 1553.85 1367.53 1558.21 Q1367.53 1563.17 1363.58 1566.07 Q1359.66 1568.97 1352.79 1568.97 Q1349.92 1568.97 1346.8 1568.39 Q1343.72 1567.85 1340.28 1566.74 L1340.28 1560.69 Q1343.53 1562.38 1346.68 1563.24 Q1349.83 1564.07 1352.92 1564.07 Q1357.05 1564.07 1359.28 1562.66 Q1361.51 1561.23 1361.51 1558.65 Q1361.51 1556.27 1359.89 1554.99 Q1358.29 1553.72 1352.85 1552.54 L1350.82 1552.07 Q1345.37 1550.92 1342.95 1548.56 Q1340.53 1546.18 1340.53 1542.04 Q1340.53 1537.01 1344.1 1534.27 Q1347.66 1531.54 1354.22 1531.54 Q1357.47 1531.54 1360.33 1532.01 Q1363.2 1532.49 1365.62 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="156.598,1423.18 156.598,47.2441 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="156.598,1408.07 175.496,1408.07 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="156.598,1045.43 175.496,1045.43 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="156.598,682.786 175.496,682.786 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="156.598,320.145 175.496,320.145 "></polyline><path clip-path="url(#clip230)" d="M65.0198 1393.87 Q61.4087 1393.87 59.58 1397.43 Q57.7745 1400.97 57.7745 1408.1 Q57.7745 1415.21 59.58 1418.77 Q61.4087 1422.32 65.0198 1422.32 Q68.6541 1422.32 70.4596 1418.77 Q72.2883 1415.21 72.2883 1408.1 Q72.2883 1400.97 70.4596 1397.43 Q68.6541 1393.87 65.0198 1393.87 M65.0198 1390.16 Q70.83 1390.16 73.8855 1394.77 Q76.9642 1399.35 76.9642 1408.1 Q76.9642 1416.83 73.8855 1421.44 Q70.83 1426.02 65.0198 1426.02 Q59.2097 1426.02 56.131 1421.44 Q53.0754 1416.83 53.0754 1408.1 Q53.0754 1399.35 56.131 1394.77 Q59.2097 1390.16 65.0198 1390.16 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M85.1818 1419.47 L90.066 1419.47 L90.066 1425.35 L85.1818 1425.35 L85.1818 1419.47 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M104.279 1421.41 L120.598 1421.41 L120.598 1425.35 L98.6539 1425.35 L98.6539 1421.41 Q101.316 1418.66 105.899 1414.03 Q110.506 1409.38 111.686 1408.03 Q113.932 1405.51 114.811 1403.77 Q115.714 1402.01 115.714 1400.32 Q115.714 1397.57 113.77 1395.83 Q111.848 1394.1 108.746 1394.1 Q106.547 1394.1 104.094 1394.86 Q101.663 1395.63 98.8854 1397.18 L98.8854 1392.45 Q101.709 1391.32 104.163 1390.74 Q106.617 1390.16 108.654 1390.16 Q114.024 1390.16 117.219 1392.85 Q120.413 1395.53 120.413 1400.02 Q120.413 1402.15 119.603 1404.07 Q118.816 1405.97 116.709 1408.57 Q116.131 1409.24 113.029 1412.45 Q109.927 1415.65 104.279 1421.41 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M62.9365 1031.23 Q59.3254 1031.23 57.4967 1034.79 Q55.6912 1038.33 55.6912 1045.46 Q55.6912 1052.57 57.4967 1056.13 Q59.3254 1059.67 62.9365 1059.67 Q66.5707 1059.67 68.3763 1056.13 Q70.205 1052.57 70.205 1045.46 Q70.205 1038.33 68.3763 1034.79 Q66.5707 1031.23 62.9365 1031.23 M62.9365 1027.52 Q68.7467 1027.52 71.8022 1032.13 Q74.8809 1036.71 74.8809 1045.46 Q74.8809 1054.19 71.8022 1058.79 Q68.7467 1063.38 62.9365 1063.38 Q57.1264 1063.38 54.0477 1058.79 Q50.9921 1054.19 50.9921 1045.46 Q50.9921 1036.71 54.0477 1032.13 Q57.1264 1027.52 62.9365 1027.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M83.0984 1056.83 L87.9827 1056.83 L87.9827 1062.71 L83.0984 1062.71 L83.0984 1056.83 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M111.015 1032.22 L99.2095 1050.67 L111.015 1050.67 L111.015 1032.22 M109.788 1028.15 L115.668 1028.15 L115.668 1050.67 L120.598 1050.67 L120.598 1054.56 L115.668 1054.56 L115.668 1062.71 L111.015 1062.71 L111.015 1054.56 L95.4132 1054.56 L95.4132 1050.04 L109.788 1028.15 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M63.2606 668.585 Q59.6495 668.585 57.8208 672.149 Q56.0152 675.691 56.0152 682.821 Q56.0152 689.927 57.8208 693.492 Q59.6495 697.033 63.2606 697.033 Q66.8948 697.033 68.7004 693.492 Q70.5291 689.927 70.5291 682.821 Q70.5291 675.691 68.7004 672.149 Q66.8948 668.585 63.2606 668.585 M63.2606 664.881 Q69.0707 664.881 72.1263 669.487 Q75.205 674.071 75.205 682.821 Q75.205 691.547 72.1263 696.154 Q69.0707 700.737 63.2606 700.737 Q57.4504 700.737 54.3717 696.154 Q51.3162 691.547 51.3162 682.821 Q51.3162 674.071 54.3717 669.487 Q57.4504 664.881 63.2606 664.881 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M83.4225 694.186 L88.3067 694.186 L88.3067 700.066 L83.4225 700.066 L83.4225 694.186 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M109.071 680.922 Q105.922 680.922 104.071 683.075 Q102.242 685.228 102.242 688.978 Q102.242 692.705 104.071 694.881 Q105.922 697.033 109.071 697.033 Q112.219 697.033 114.047 694.881 Q115.899 692.705 115.899 688.978 Q115.899 685.228 114.047 683.075 Q112.219 680.922 109.071 680.922 M118.353 666.27 L118.353 670.529 Q116.594 669.696 114.788 669.256 Q113.006 668.816 111.246 668.816 Q106.617 668.816 104.163 671.941 Q101.733 675.066 101.385 681.385 Q102.751 679.371 104.811 678.307 Q106.871 677.219 109.348 677.219 Q114.557 677.219 117.566 680.39 Q120.598 683.538 120.598 688.978 Q120.598 694.302 117.45 697.52 Q114.302 700.737 109.071 700.737 Q103.075 700.737 99.9039 696.154 Q96.7326 691.547 96.7326 682.821 Q96.7326 674.626 100.621 669.765 Q104.51 664.881 111.061 664.881 Q112.82 664.881 114.603 665.228 Q116.408 665.575 118.353 666.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M63.5152 305.944 Q59.9041 305.944 58.0754 309.508 Q56.2699 313.05 56.2699 320.18 Q56.2699 327.286 58.0754 330.851 Q59.9041 334.392 63.5152 334.392 Q67.1494 334.392 68.955 330.851 Q70.7837 327.286 70.7837 320.18 Q70.7837 313.05 68.955 309.508 Q67.1494 305.944 63.5152 305.944 M63.5152 302.24 Q69.3254 302.24 72.3809 306.846 Q75.4596 311.43 75.4596 320.18 Q75.4596 328.906 72.3809 333.513 Q69.3254 338.096 63.5152 338.096 Q57.7051 338.096 54.6264 333.513 Q51.5708 328.906 51.5708 320.18 Q51.5708 311.43 54.6264 306.846 Q57.7051 302.24 63.5152 302.24 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M83.6771 331.545 L88.5614 331.545 L88.5614 337.425 L83.6771 337.425 L83.6771 331.545 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M108.746 321.013 Q105.413 321.013 103.492 322.795 Q101.594 324.578 101.594 327.703 Q101.594 330.828 103.492 332.61 Q105.413 334.392 108.746 334.392 Q112.08 334.392 114.001 332.61 Q115.922 330.805 115.922 327.703 Q115.922 324.578 114.001 322.795 Q112.103 321.013 108.746 321.013 M104.071 319.022 Q101.061 318.281 99.3715 316.221 Q97.7048 314.161 97.7048 311.198 Q97.7048 307.055 100.645 304.647 Q103.608 302.24 108.746 302.24 Q113.908 302.24 116.848 304.647 Q119.788 307.055 119.788 311.198 Q119.788 314.161 118.098 316.221 Q116.432 318.281 113.445 319.022 Q116.825 319.809 118.7 322.101 Q120.598 324.393 120.598 327.703 Q120.598 332.726 117.52 335.411 Q114.464 338.096 108.746 338.096 Q103.029 338.096 99.9502 335.411 Q96.8947 332.726 96.8947 327.703 Q96.8947 324.393 98.7928 322.101 Q100.691 319.809 104.071 319.022 M102.358 311.638 Q102.358 314.323 104.024 315.828 Q105.714 317.332 108.746 317.332 Q111.756 317.332 113.445 315.828 Q115.158 314.323 115.158 311.638 Q115.158 308.953 113.445 307.448 Q111.756 305.944 108.746 305.944 Q105.714 305.944 104.024 307.448 Q102.358 308.953 102.358 311.638 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip232)" style="stroke:#009af9;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="218.754,86.1857 448.959,880.167 679.164,1054.28 909.369,1132.1 1139.57,1216.05 1369.78,1245.25 1599.98,1299.49 1830.19,1323.73 2060.4,1347.9 2290.6,1384.24 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#e26f46;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="218.754,658.425 448.959,1002.99 679.164,1050.43 909.369,725.134 1139.57,1035.94 1369.78,1221.19 1599.98,1266.95 1830.19,1088.64 2060.4,1202.21 2290.6,1157.27 "></polyline><polyline clip-path="url(#clip232)" style="stroke:#3da44d;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="218.754,332.804 448.959,222.169 679.164,209.258 909.369,314.156 1139.57,223.783 1369.78,146.14 1599.98,138.071 1830.19,197.782 2060.4,153.851 2290.6,172.858 "></polyline><path clip-path="url(#clip230)" d="M229.803 1377.32 L671.455 1377.32 L671.455 1169.96 L229.803 1169.96  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path><polyline clip-path="url(#clip230)" style="stroke:#000000;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="229.803,1377.32 671.455,1377.32 671.455,1169.96 229.803,1169.96 229.803,1377.32 "></polyline><polyline clip-path="url(#clip230)" style="stroke:#009af9;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="254.205,1221.8 400.616,1221.8 "></polyline><path clip-path="url(#clip230)" d="M432.425 1205.79 L432.425 1213.15 L441.198 1213.15 L441.198 1216.46 L432.425 1216.46 L432.425 1230.53 Q432.425 1233.7 433.281 1234.61 Q434.161 1235.51 436.823 1235.51 L441.198 1235.51 L441.198 1239.08 L436.823 1239.08 Q431.892 1239.08 430.017 1237.25 Q428.142 1235.39 428.142 1230.53 L428.142 1216.46 L425.018 1216.46 L425.018 1213.15 L428.142 1213.15 L428.142 1205.79 L432.425 1205.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M461.823 1217.13 Q461.105 1216.71 460.249 1216.53 Q459.415 1216.32 458.397 1216.32 Q454.786 1216.32 452.841 1218.68 Q450.92 1221.02 450.92 1225.42 L450.92 1239.08 L446.638 1239.08 L446.638 1213.15 L450.92 1213.15 L450.92 1217.18 Q452.263 1214.82 454.415 1213.68 Q456.568 1212.52 459.647 1212.52 Q460.087 1212.52 460.619 1212.59 Q461.152 1212.64 461.8 1212.76 L461.823 1217.13 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M478.073 1226.04 Q472.911 1226.04 470.92 1227.22 Q468.929 1228.4 468.929 1231.25 Q468.929 1233.52 470.411 1234.86 Q471.915 1236.18 474.485 1236.18 Q478.026 1236.18 480.156 1233.68 Q482.309 1231.16 482.309 1226.99 L482.309 1226.04 L478.073 1226.04 M486.568 1224.28 L486.568 1239.08 L482.309 1239.08 L482.309 1235.14 Q480.851 1237.5 478.675 1238.64 Q476.499 1239.75 473.351 1239.75 Q469.369 1239.75 467.008 1237.52 Q464.67 1235.28 464.67 1231.53 Q464.67 1227.15 467.587 1224.93 Q470.527 1222.71 476.337 1222.71 L482.309 1222.71 L482.309 1222.29 Q482.309 1219.35 480.364 1217.76 Q478.443 1216.14 474.948 1216.14 Q472.726 1216.14 470.619 1216.67 Q468.513 1217.2 466.568 1218.27 L466.568 1214.33 Q468.906 1213.43 471.105 1212.99 Q473.304 1212.52 475.388 1212.52 Q481.013 1212.52 483.79 1215.44 Q486.568 1218.36 486.568 1224.28 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M495.341 1213.15 L499.6 1213.15 L499.6 1239.08 L495.341 1239.08 L495.341 1213.15 M495.341 1203.06 L499.6 1203.06 L499.6 1208.45 L495.341 1208.45 L495.341 1203.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M530.063 1223.43 L530.063 1239.08 L525.804 1239.08 L525.804 1223.57 Q525.804 1219.89 524.369 1218.06 Q522.934 1216.23 520.063 1216.23 Q516.614 1216.23 514.624 1218.43 Q512.633 1220.63 512.633 1224.42 L512.633 1239.08 L508.35 1239.08 L508.35 1213.15 L512.633 1213.15 L512.633 1217.18 Q514.161 1214.84 516.221 1213.68 Q518.304 1212.52 521.012 1212.52 Q525.48 1212.52 527.772 1215.3 Q530.063 1218.06 530.063 1223.43 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M558.258 1246.95 L558.258 1250.26 L533.628 1250.26 L533.628 1246.95 L558.258 1246.95 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M562.262 1203.06 L566.521 1203.06 L566.521 1239.08 L562.262 1239.08 L562.262 1203.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M585.48 1216.14 Q582.054 1216.14 580.063 1218.82 Q578.072 1221.48 578.072 1226.14 Q578.072 1230.79 580.04 1233.47 Q582.031 1236.14 585.48 1236.14 Q588.882 1236.14 590.873 1233.45 Q592.864 1230.77 592.864 1226.14 Q592.864 1221.53 590.873 1218.84 Q588.882 1216.14 585.48 1216.14 M585.48 1212.52 Q591.035 1212.52 594.206 1216.14 Q597.378 1219.75 597.378 1226.14 Q597.378 1232.5 594.206 1236.14 Q591.035 1239.75 585.48 1239.75 Q579.901 1239.75 576.73 1236.14 Q573.582 1232.5 573.582 1226.14 Q573.582 1219.75 576.73 1216.14 Q579.901 1212.52 585.48 1212.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M620.966 1213.91 L620.966 1217.94 Q619.16 1217.02 617.216 1216.55 Q615.271 1216.09 613.188 1216.09 Q610.017 1216.09 608.419 1217.06 Q606.845 1218.03 606.845 1219.98 Q606.845 1221.46 607.979 1222.32 Q609.114 1223.15 612.54 1223.91 L613.998 1224.24 Q618.535 1225.21 620.433 1226.99 Q622.354 1228.75 622.354 1231.92 Q622.354 1235.53 619.484 1237.64 Q616.637 1239.75 611.637 1239.75 Q609.554 1239.75 607.285 1239.33 Q605.04 1238.94 602.54 1238.13 L602.54 1233.73 Q604.901 1234.95 607.192 1235.58 Q609.484 1236.18 611.729 1236.18 Q614.739 1236.18 616.359 1235.16 Q617.979 1234.12 617.979 1232.25 Q617.979 1230.51 616.799 1229.58 Q615.641 1228.66 611.683 1227.8 L610.202 1227.46 Q606.243 1226.62 604.484 1224.91 Q602.725 1223.17 602.725 1220.16 Q602.725 1216.51 605.317 1214.52 Q607.91 1212.52 612.679 1212.52 Q615.04 1212.52 617.123 1212.87 Q619.206 1213.22 620.966 1213.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M645.664 1213.91 L645.664 1217.94 Q643.859 1217.02 641.914 1216.55 Q639.97 1216.09 637.887 1216.09 Q634.715 1216.09 633.118 1217.06 Q631.544 1218.03 631.544 1219.98 Q631.544 1221.46 632.678 1222.32 Q633.813 1223.15 637.239 1223.91 L638.697 1224.24 Q643.234 1225.21 645.132 1226.99 Q647.053 1228.75 647.053 1231.92 Q647.053 1235.53 644.183 1237.64 Q641.336 1239.75 636.336 1239.75 Q634.252 1239.75 631.984 1239.33 Q629.739 1238.94 627.239 1238.13 L627.239 1233.73 Q629.6 1234.95 631.891 1235.58 Q634.183 1236.18 636.428 1236.18 Q639.438 1236.18 641.058 1235.16 Q642.678 1234.12 642.678 1232.25 Q642.678 1230.51 641.498 1229.58 Q640.34 1228.66 636.382 1227.8 L634.901 1227.46 Q630.942 1226.62 629.183 1224.91 Q627.424 1223.17 627.424 1220.16 Q627.424 1216.51 630.016 1214.52 Q632.609 1212.52 637.377 1212.52 Q639.739 1212.52 641.822 1212.87 Q643.905 1213.22 645.664 1213.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip230)" style="stroke:#e26f46;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="254.205,1273.64 400.616,1273.64 "></polyline><path clip-path="url(#clip230)" d="M425.018 1264.99 L429.531 1264.99 L437.633 1286.75 L445.735 1264.99 L450.249 1264.99 L440.527 1290.92 L434.74 1290.92 L425.018 1264.99 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M467.911 1277.88 Q462.749 1277.88 460.758 1279.06 Q458.767 1280.24 458.767 1283.09 Q458.767 1285.36 460.249 1286.7 Q461.753 1288.02 464.323 1288.02 Q467.864 1288.02 469.994 1285.52 Q472.147 1283 472.147 1278.83 L472.147 1277.88 L467.911 1277.88 M476.406 1276.12 L476.406 1290.92 L472.147 1290.92 L472.147 1286.98 Q470.689 1289.34 468.513 1290.48 Q466.337 1291.59 463.189 1291.59 Q459.207 1291.59 456.846 1289.36 Q454.508 1287.12 454.508 1283.37 Q454.508 1278.99 457.425 1276.77 Q460.365 1274.55 466.175 1274.55 L472.147 1274.55 L472.147 1274.13 Q472.147 1271.19 470.202 1269.6 Q468.281 1267.98 464.786 1267.98 Q462.564 1267.98 460.457 1268.51 Q458.351 1269.04 456.406 1270.11 L456.406 1266.17 Q458.744 1265.27 460.943 1264.83 Q463.142 1264.36 465.226 1264.36 Q470.851 1264.36 473.628 1267.28 Q476.406 1270.2 476.406 1276.12 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M485.179 1254.9 L489.438 1254.9 L489.438 1290.92 L485.179 1290.92 L485.179 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M518.049 1298.79 L518.049 1302.1 L493.42 1302.1 L493.42 1298.79 L518.049 1298.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M522.054 1254.9 L526.313 1254.9 L526.313 1290.92 L522.054 1290.92 L522.054 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M545.271 1267.98 Q541.846 1267.98 539.855 1270.66 Q537.864 1273.32 537.864 1277.98 Q537.864 1282.63 539.832 1285.31 Q541.822 1287.98 545.271 1287.98 Q548.674 1287.98 550.665 1285.29 Q552.656 1282.61 552.656 1277.98 Q552.656 1273.37 550.665 1270.68 Q548.674 1267.98 545.271 1267.98 M545.271 1264.36 Q550.827 1264.36 553.998 1267.98 Q557.17 1271.59 557.17 1277.98 Q557.17 1284.34 553.998 1287.98 Q550.827 1291.59 545.271 1291.59 Q539.693 1291.59 536.522 1287.98 Q533.373 1284.34 533.373 1277.98 Q533.373 1271.59 536.522 1267.98 Q539.693 1264.36 545.271 1264.36 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M580.757 1265.75 L580.757 1269.78 Q578.952 1268.86 577.007 1268.39 Q575.063 1267.93 572.98 1267.93 Q569.808 1267.93 568.211 1268.9 Q566.637 1269.87 566.637 1271.82 Q566.637 1273.3 567.771 1274.16 Q568.906 1274.99 572.332 1275.75 L573.79 1276.08 Q578.327 1277.05 580.225 1278.83 Q582.146 1280.59 582.146 1283.76 Q582.146 1287.37 579.276 1289.48 Q576.429 1291.59 571.429 1291.59 Q569.345 1291.59 567.077 1291.17 Q564.832 1290.78 562.332 1289.97 L562.332 1285.57 Q564.693 1286.79 566.984 1287.42 Q569.276 1288.02 571.521 1288.02 Q574.531 1288.02 576.151 1287 Q577.771 1285.96 577.771 1284.09 Q577.771 1282.35 576.591 1281.42 Q575.433 1280.5 571.475 1279.64 L569.994 1279.3 Q566.035 1278.46 564.276 1276.75 Q562.517 1275.01 562.517 1272 Q562.517 1268.35 565.109 1266.36 Q567.702 1264.36 572.47 1264.36 Q574.832 1264.36 576.915 1264.71 Q578.998 1265.06 580.757 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M605.456 1265.75 L605.456 1269.78 Q603.651 1268.86 601.706 1268.39 Q599.762 1267.93 597.679 1267.93 Q594.507 1267.93 592.91 1268.9 Q591.336 1269.87 591.336 1271.82 Q591.336 1273.3 592.47 1274.16 Q593.605 1274.99 597.03 1275.75 L598.489 1276.08 Q603.026 1277.05 604.924 1278.83 Q606.845 1280.59 606.845 1283.76 Q606.845 1287.37 603.975 1289.48 Q601.128 1291.59 596.128 1291.59 Q594.044 1291.59 591.776 1291.17 Q589.531 1290.78 587.031 1289.97 L587.031 1285.57 Q589.392 1286.79 591.683 1287.42 Q593.975 1288.02 596.22 1288.02 Q599.23 1288.02 600.85 1287 Q602.47 1285.96 602.47 1284.09 Q602.47 1282.35 601.29 1281.42 Q600.132 1280.5 596.174 1279.64 L594.693 1279.3 Q590.734 1278.46 588.975 1276.75 Q587.216 1275.01 587.216 1272 Q587.216 1268.35 589.808 1266.36 Q592.401 1264.36 597.169 1264.36 Q599.53 1264.36 601.614 1264.71 Q603.697 1265.06 605.456 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip230)" style="stroke:#3da44d;stroke-linecap:round;stroke-linejoin:round;stroke-width:4;stroke-opacity:1;fill:none;" points="254.205,1325.48 400.616,1325.48 "></polyline><path clip-path="url(#clip230)" d="M425.018 1316.83 L429.531 1316.83 L437.633 1338.59 L445.735 1316.83 L450.249 1316.83 L440.527 1342.76 L434.74 1342.76 L425.018 1316.83 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M467.911 1329.72 Q462.749 1329.72 460.758 1330.9 Q458.767 1332.08 458.767 1334.93 Q458.767 1337.2 460.249 1338.54 Q461.753 1339.86 464.323 1339.86 Q467.864 1339.86 469.994 1337.36 Q472.147 1334.84 472.147 1330.67 L472.147 1329.72 L467.911 1329.72 M476.406 1327.96 L476.406 1342.76 L472.147 1342.76 L472.147 1338.82 Q470.689 1341.18 468.513 1342.32 Q466.337 1343.43 463.189 1343.43 Q459.207 1343.43 456.846 1341.2 Q454.508 1338.96 454.508 1335.21 Q454.508 1330.83 457.425 1328.61 Q460.365 1326.39 466.175 1326.39 L472.147 1326.39 L472.147 1325.97 Q472.147 1323.03 470.202 1321.44 Q468.281 1319.82 464.786 1319.82 Q462.564 1319.82 460.457 1320.35 Q458.351 1320.88 456.406 1321.95 L456.406 1318.01 Q458.744 1317.11 460.943 1316.67 Q463.142 1316.2 465.226 1316.2 Q470.851 1316.2 473.628 1319.12 Q476.406 1322.04 476.406 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M485.179 1306.74 L489.438 1306.74 L489.438 1342.76 L485.179 1342.76 L485.179 1306.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M518.049 1350.63 L518.049 1353.94 L493.42 1353.94 L493.42 1350.63 L518.049 1350.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M533.836 1329.72 Q528.674 1329.72 526.684 1330.9 Q524.693 1332.08 524.693 1334.93 Q524.693 1337.2 526.174 1338.54 Q527.679 1339.86 530.248 1339.86 Q533.79 1339.86 535.92 1337.36 Q538.072 1334.84 538.072 1330.67 L538.072 1329.72 L533.836 1329.72 M542.332 1327.96 L542.332 1342.76 L538.072 1342.76 L538.072 1338.82 Q536.614 1341.18 534.438 1342.32 Q532.262 1343.43 529.114 1343.43 Q525.133 1343.43 522.772 1341.2 Q520.434 1338.96 520.434 1335.21 Q520.434 1330.83 523.35 1328.61 Q526.29 1326.39 532.1 1326.39 L538.072 1326.39 L538.072 1325.97 Q538.072 1323.03 536.128 1321.44 Q534.207 1319.82 530.711 1319.82 Q528.489 1319.82 526.383 1320.35 Q524.276 1320.88 522.332 1321.95 L522.332 1318.01 Q524.67 1317.11 526.869 1316.67 Q529.068 1316.2 531.151 1316.2 Q536.776 1316.2 539.554 1319.12 Q542.332 1322.04 542.332 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M569.762 1317.82 L569.762 1321.81 Q567.957 1320.81 566.128 1320.32 Q564.322 1319.82 562.47 1319.82 Q558.327 1319.82 556.035 1322.45 Q553.744 1325.07 553.744 1329.82 Q553.744 1334.56 556.035 1337.2 Q558.327 1339.82 562.47 1339.82 Q564.322 1339.82 566.128 1339.33 Q567.957 1338.82 569.762 1337.82 L569.762 1341.76 Q567.98 1342.59 566.058 1343.01 Q564.16 1343.43 562.008 1343.43 Q556.151 1343.43 552.702 1339.75 Q549.253 1336.07 549.253 1329.82 Q549.253 1323.47 552.725 1319.84 Q556.221 1316.2 562.285 1316.2 Q564.253 1316.2 566.128 1316.62 Q568.003 1317.01 569.762 1317.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip230)" d="M595.827 1317.82 L595.827 1321.81 Q594.021 1320.81 592.193 1320.32 Q590.387 1319.82 588.535 1319.82 Q584.392 1319.82 582.1 1322.45 Q579.808 1325.07 579.808 1329.82 Q579.808 1334.56 582.1 1337.2 Q584.392 1339.82 588.535 1339.82 Q590.387 1339.82 592.193 1339.33 Q594.021 1338.82 595.827 1337.82 L595.827 1341.76 Q594.044 1342.59 592.123 1343.01 Q590.225 1343.43 588.072 1343.43 Q582.216 1343.43 578.767 1339.75 Q575.318 1336.07 575.318 1329.82 Q575.318 1323.47 578.79 1319.84 Q582.285 1316.2 588.35 1316.2 Q590.318 1316.2 592.193 1316.62 Q594.068 1317.01 595.827 1317.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path></svg><h2 id="Summary-and-Discussion" tabindex="-1">Summary and Discussion <a class="header-anchor" href="#Summary-and-Discussion" aria-label="Permalink to &quot;Summary and Discussion {#Summary-and-Discussion}&quot;">â€‹</a></h2><p>The main components that comprise DenseNet are dense blocks and transition layers. For the latter, we need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again. In terms of cross-layer connections, in contrast to ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension. Although these concatenation operations reuse features to achieve computational efficiency, unfortunately they lead to heavy GPU memory consumption. As a result, applying DenseNet may require more memory-efficient implementations that may increase training time [<a href="/d2l-julia/previews/PR1/references#pleiss2017memory">147</a>].</p><h2 id="Exercises" tabindex="-1">Exercises <a class="header-anchor" href="#Exercises" aria-label="Permalink to &quot;Exercises {#Exercises}&quot;">â€‹</a></h2><ol><li><p>Why do we use average pooling rather than max-pooling in the transition layer?</p></li><li><p>One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?</p></li><li><p>One problem for which DenseNet has been criticized is its high memory consumption.</p></li><li><p>Is this really the case? Try to change the input shape to <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0;" xmlns="http://www.w3.org/2000/svg" width="9.553ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 4222.4 677" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)" style="stroke-width:3;"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(2722.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)" style="stroke-width:3;"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>224</mn><mo>Ã—</mo><mn>224</mn></math></mjx-assistive-mml></mjx-container> to compare the actual GPU memory consumption empirically.</p></li><li><p>Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?</p></li><li><p>Implement the various DenseNet versions presented in Table 1 of the DenseNet paper [<a href="/d2l-julia/previews/PR1/references#Huang_Liu_Van-Der-Maaten_ea_2017">94</a>].</p></li><li><p>Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in :numref:<code>sec_kaggle_house</code>.</p></li></ol><h3 id="1-." tabindex="-1">1 . <a class="header-anchor" href="#1-." aria-label="Permalink to &quot;1 . {#1-.}&quot;">â€‹</a></h3><p>Since the goal is to take outputs from previous layers upto the next layer, we use mean pooling. It doesnot discard the rest of the activations in the convolutional layer, and instead takes the mean.</p><h3 id="2" tabindex="-1">2 <a class="header-anchor" href="#2" aria-label="Permalink to &quot;2 {#2}&quot;">â€‹</a></h3><p>Due to the transition layers, we effectively manage the model complexity and by extension the number of parameters.</p><h3 id="3." tabindex="-1">3. <a class="header-anchor" href="#3." aria-label="Permalink to &quot;3. {#3.}&quot;">â€‹</a></h3><p>A. 96x96 takes 2169MiB of GPU memoy. 224 / 96 = 2.33. The GPU memory is affected by the order of N^3. Therefore it will be 8 times more B. Sparse Connectivity: Instead of connecting all the layers to all the layers, randomly pick some layers and connect them.</p><h3 id="4." tabindex="-1">4. <a class="header-anchor" href="#4." aria-label="Permalink to &quot;4. {#4.}&quot;">â€‹</a></h3><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">densenet121 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; arch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">24</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">densenet169 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; arch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">densenet201 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; arch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">48</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">densenet264 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; arch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">12</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">48</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DenseNet(</span></span>
<span class="line"><span>  Chain(</span></span>
<span class="line"><span>    DenseNetB1(</span></span>
<span class="line"><span>      Chain(</span></span>
<span class="line"><span>        Conv((7, 7), 1 =&gt; 64, pad=3, stride=2),  # 3_200 parameters</span></span>
<span class="line"><span>        BatchNorm(64, relu),            # 128 parameters, plus 128</span></span>
<span class="line"><span>        MaxPool((3, 3), pad=1, stride=2),</span></span>
<span class="line"><span>      ),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 64 =&gt; 32, pad=1),  # 18_464 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 96 =&gt; 32, pad=1),  # 27_680 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 192 =&gt; 32, pad=1),  # 55_328 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 224 =&gt; 32, pad=1),  # 64_544 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    Chain(</span></span>
<span class="line"><span>      BatchNorm(256, relu),             # 512 parameters, plus 512</span></span>
<span class="line"><span>      Conv((1, 1), 256 =&gt; 128),         # 32_896 parameters</span></span>
<span class="line"><span>      MeanPool((2, 2), pad=1),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 192 =&gt; 32, pad=1),  # 55_328 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 224 =&gt; 32, pad=1),  # 64_544 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 256 =&gt; 32, pad=1),  # 73_760 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 288 =&gt; 32, pad=1),  # 82_976 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 320 =&gt; 32, pad=1),  # 92_192 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 352 =&gt; 32, pad=1),  # 101_408 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 384 =&gt; 32, pad=1),  # 110_624 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 416 =&gt; 32, pad=1),  # 119_840 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 448 =&gt; 32, pad=1),  # 129_056 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 480 =&gt; 32, pad=1),  # 138_272 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    Chain(</span></span>
<span class="line"><span>      BatchNorm(512, relu),             # 1_024 parameters, plus 1_024</span></span>
<span class="line"><span>      Conv((1, 1), 512 =&gt; 256),         # 131_328 parameters</span></span>
<span class="line"><span>      MeanPool((2, 2), pad=1),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 256 =&gt; 32, pad=1),  # 73_760 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 288 =&gt; 32, pad=1),  # 82_976 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 320 =&gt; 32, pad=1),  # 92_192 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 352 =&gt; 32, pad=1),  # 101_408 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 384 =&gt; 32, pad=1),  # 110_624 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 416 =&gt; 32, pad=1),  # 119_840 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 448 =&gt; 32, pad=1),  # 129_056 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 480 =&gt; 32, pad=1),  # 138_272 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 512 =&gt; 32, pad=1),  # 147_488 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 544 =&gt; 32, pad=1),  # 156_704 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 576 =&gt; 32, pad=1),  # 165_920 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 608 =&gt; 32, pad=1),  # 175_136 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 640 =&gt; 32, pad=1),  # 184_352 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 672 =&gt; 32, pad=1),  # 193_568 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 704 =&gt; 32, pad=1),  # 202_784 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 736 =&gt; 32, pad=1),  # 212_000 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 768 =&gt; 32, pad=1),  # 221_216 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 800 =&gt; 32, pad=1),  # 230_432 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 832 =&gt; 32, pad=1),  # 239_648 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 864 =&gt; 32, pad=1),  # 248_864 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 896 =&gt; 32, pad=1),  # 258_080 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 928 =&gt; 32, pad=1),  # 267_296 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 960 =&gt; 32, pad=1),  # 276_512 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 992 =&gt; 32, pad=1),  # 285_728 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1024 =&gt; 32, pad=1),  # 294_944 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1056 =&gt; 32, pad=1),  # 304_160 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1088 =&gt; 32, pad=1),  # 313_376 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1120 =&gt; 32, pad=1),  # 322_592 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1152 =&gt; 32, pad=1),  # 331_808 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1184 =&gt; 32, pad=1),  # 341_024 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1216 =&gt; 32, pad=1),  # 350_240 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1248 =&gt; 32, pad=1),  # 359_456 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1280 =&gt; 32, pad=1),  # 368_672 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1312 =&gt; 32, pad=1),  # 377_888 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1344 =&gt; 32, pad=1),  # 387_104 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1376 =&gt; 32, pad=1),  # 396_320 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1408 =&gt; 32, pad=1),  # 405_536 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1440 =&gt; 32, pad=1),  # 414_752 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1472 =&gt; 32, pad=1),  # 423_968 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1504 =&gt; 32, pad=1),  # 433_184 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1536 =&gt; 32, pad=1),  # 442_400 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1568 =&gt; 32, pad=1),  # 451_616 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1600 =&gt; 32, pad=1),  # 460_832 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1632 =&gt; 32, pad=1),  # 470_048 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1664 =&gt; 32, pad=1),  # 479_264 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1696 =&gt; 32, pad=1),  # 488_480 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1728 =&gt; 32, pad=1),  # 497_696 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1760 =&gt; 32, pad=1),  # 506_912 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1792 =&gt; 32, pad=1),  # 516_128 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1824 =&gt; 32, pad=1),  # 525_344 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1856 =&gt; 32, pad=1),  # 534_560 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1888 =&gt; 32, pad=1),  # 543_776 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1920 =&gt; 32, pad=1),  # 552_992 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1952 =&gt; 32, pad=1),  # 562_208 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1984 =&gt; 32, pad=1),  # 571_424 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2016 =&gt; 32, pad=1),  # 580_640 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2048 =&gt; 32, pad=1),  # 589_856 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2080 =&gt; 32, pad=1),  # 599_072 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2112 =&gt; 32, pad=1),  # 608_288 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2144 =&gt; 32, pad=1),  # 617_504 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2176 =&gt; 32, pad=1),  # 626_720 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2208 =&gt; 32, pad=1),  # 635_936 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2240 =&gt; 32, pad=1),  # 645_152 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2272 =&gt; 32, pad=1),  # 654_368 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    Chain(</span></span>
<span class="line"><span>      BatchNorm(2304, relu),            # 4_608 parameters, plus 4_608</span></span>
<span class="line"><span>      Conv((1, 1), 2304 =&gt; 1152),       # 2_655_360 parameters</span></span>
<span class="line"><span>      MeanPool((2, 2), pad=1),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    DenseBlock(</span></span>
<span class="line"><span>      [</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1152 =&gt; 32, pad=1),  # 331_808 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1184 =&gt; 32, pad=1),  # 341_024 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1216 =&gt; 32, pad=1),  # 350_240 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1248 =&gt; 32, pad=1),  # 359_456 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1280 =&gt; 32, pad=1),  # 368_672 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1312 =&gt; 32, pad=1),  # 377_888 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1344 =&gt; 32, pad=1),  # 387_104 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1376 =&gt; 32, pad=1),  # 396_320 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1408 =&gt; 32, pad=1),  # 405_536 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1440 =&gt; 32, pad=1),  # 414_752 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1472 =&gt; 32, pad=1),  # 423_968 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1504 =&gt; 32, pad=1),  # 433_184 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1536 =&gt; 32, pad=1),  # 442_400 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1568 =&gt; 32, pad=1),  # 451_616 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1600 =&gt; 32, pad=1),  # 460_832 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1632 =&gt; 32, pad=1),  # 470_048 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1664 =&gt; 32, pad=1),  # 479_264 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1696 =&gt; 32, pad=1),  # 488_480 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1728 =&gt; 32, pad=1),  # 497_696 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1760 =&gt; 32, pad=1),  # 506_912 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1792 =&gt; 32, pad=1),  # 516_128 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1824 =&gt; 32, pad=1),  # 525_344 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1856 =&gt; 32, pad=1),  # 534_560 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1888 =&gt; 32, pad=1),  # 543_776 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1920 =&gt; 32, pad=1),  # 552_992 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1952 =&gt; 32, pad=1),  # 562_208 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 1984 =&gt; 32, pad=1),  # 571_424 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2016 =&gt; 32, pad=1),  # 580_640 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2048 =&gt; 32, pad=1),  # 589_856 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2080 =&gt; 32, pad=1),  # 599_072 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2112 =&gt; 32, pad=1),  # 608_288 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2144 =&gt; 32, pad=1),  # 617_504 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2176 =&gt; 32, pad=1),  # 626_720 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2208 =&gt; 32, pad=1),  # 635_936 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2240 =&gt; 32, pad=1),  # 645_152 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2272 =&gt; 32, pad=1),  # 654_368 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2304 =&gt; 32, pad=1),  # 663_584 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2336 =&gt; 32, pad=1),  # 672_800 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2368 =&gt; 32, pad=1),  # 682_016 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2400 =&gt; 32, pad=1),  # 691_232 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2432 =&gt; 32, pad=1),  # 700_448 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2464 =&gt; 32, pad=1),  # 709_664 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2496 =&gt; 32, pad=1),  # 718_880 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2528 =&gt; 32, pad=1),  # 728_096 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2560 =&gt; 32, pad=1),  # 737_312 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2592 =&gt; 32, pad=1),  # 746_528 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2624 =&gt; 32, pad=1),  # 755_744 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        Chain(</span></span>
<span class="line"><span>          Conv((3, 3), 2656 =&gt; 32, pad=1),  # 764_960 parameters</span></span>
<span class="line"><span>          BatchNorm(32, relu),          # 64 parameters, plus 64</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>      ],</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>    GlobalMeanPool(),</span></span>
<span class="line"><span>    Flux.flatten,</span></span>
<span class="line"><span>    Dense(2688 =&gt; 10),                  # 26_890 parameters</span></span>
<span class="line"><span>    NNlib.softmax,</span></span>
<span class="line"><span>  ),</span></span>
<span class="line"><span>)         # Total: 538 trainable arrays, 53_786_826 parameters,</span></span>
<span class="line"><span>          # plus 268 non-trainable, 14_592 parameters, summarysize 205.290 MiB.</span></span></code></pre></div><h3 id="5." tabindex="-1">5. <a class="header-anchor" href="#5." aria-label="Permalink to &quot;5. {#5.}&quot;">â€‹</a></h3><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## 5.</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetMLPBlock{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseNetMLPBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(features_in, num_features, num_dense, return_output_features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    prev_features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> features_in</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    blocks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">num_dense</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        push!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(blocks, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dense</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prev_features, num_features))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        prev_features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_features</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        push!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(blocks, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dropout</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNetMLPBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(blocks)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> return_output_features </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> block, prev_features </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    else</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> block</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DenseNetMLPBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)(x)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> block </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> block</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> vcat</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x,y)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DenseNetMLP{N} </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AbstractModel</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    net</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">N</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DenseNetMLP</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(num_features; num_classes </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, arch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    prev_features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 64</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    layers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> map</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(arch) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">do</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_dense </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        block, prev_features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> DenseNetMLPBlock</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prev_features, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, num_dense; return_output_featuers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> block</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    Chain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Dense</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(num_features </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, relu),</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Dropout</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        layers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        Dense</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(_ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_classes),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        softmax</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://https://github.com/ashutosh-b-b/d2l-julia/edit/master/docs/src/CH7.ModernConvolutionalNeuralNetworks/MCNN_7.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_6" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>Residual Networks (ResNet) and ResNeXt</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/d2l-julia/previews/PR1/CH7.ModernConvolutionalNeuralNetworks/MCNN_8" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>Designing Convolution Network Architectures</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://luxdl.github.io/DocumenterVitepress.jl/dev/" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br></p><p class="copyright" data-v-c970a860>Â© Copyright 2025.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"ch10.attention_mechanisms_and_transformers_attn_1.md\":\"BPTHOId7\",\"ch10.attention_mechanisms_and_transformers_attn_2.md\":\"CSvHAO07\",\"ch10.attention_mechanisms_and_transformers_attn_3.md\":\"B6P-XMmW\",\"ch10.attention_mechanisms_and_transformers_attn_4.md\":\"CB5DEaB1\",\"ch10.attention_mechanisms_and_transformers_attn_5.md\":\"DQHNoxkK\",\"ch10.attention_mechanisms_and_transformers_attn_6.md\":\"DJp_Cd3H\",\"ch10.attention_mechanisms_and_transformers_untitled.md\":\"AZxRWAaB\",\"ch3.linear_regression_lnn_1.md\":\"DTK1fX6I\",\"ch3.linear_regression_lnn_2.md\":\"BD8w6uR8\",\"ch3.linear_regression_lnn_3.md\":\"C4MB6zPT\",\"ch3.linear_regression_lnn_4.md\":\"B7J5MW04\",\"ch3.linear_regression_lnn_5.md\":\"CfF2CfNK\",\"ch3.linear_regression_lnn_6.md\":\"uj6xm8fY\",\"ch3.linear_regression_lnn_7.md\":\"B_zpSpiN\",\"ch4.linear_classification_lcn_1.md\":\"Boi1cDsh\",\"ch4.linear_classification_lcn_2.md\":\"CkOR2Iia\",\"ch4.linear_classification_lcn_3.md\":\"V-0rtib8\",\"ch4.linear_classification_lcn_4.md\":\"BX5UP2HZ\",\"ch4.linear_classification_lcn_5.md\":\"C-7KZE9h\",\"ch4.linear_classification_lcn_6.md\":\"DNDvKaZ3\",\"ch5.mlp_mlp_1.md\":\"DNcZmrDZ\",\"ch5.mlp_mlp_2.md\":\"MI21_tyz\",\"ch5.mlp_mlp_3.md\":\"DVB63m8H\",\"ch5.mlp_mlp_4.md\":\"BYjKXVG9\",\"ch5.mlp_mlp_5.md\":\"CxEzVy5G\",\"ch5.mlp_mlp_6.md\":\"CpygNHoD\",\"ch6.convolutional_neural_networks_cnn_2.md\":\"pAndzyT8\",\"ch6.convolutional_neural_networks_cnn_3.md\":\"CyHC0BXV\",\"ch6.convolutional_neural_networks_cnn_4.md\":\"-fYwA9iM\",\"ch6.convolutional_neural_networks_cnn_5.md\":\"DKCH7I6h\",\"ch6.convolutional_neural_networks_cnn_6.md\":\"BxTNLj1_\",\"ch7.modernconvolutionalneuralnetworks_mcnn_0.md\":\"Do42Pcb-\",\"ch7.modernconvolutionalneuralnetworks_mcnn_1.md\":\"DyrfEGE0\",\"ch7.modernconvolutionalneuralnetworks_mcnn_2.md\":\"BdK6e3J7\",\"ch7.modernconvolutionalneuralnetworks_mcnn_3.md\":\"CvJU8raL\",\"ch7.modernconvolutionalneuralnetworks_mcnn_4.md\":\"DfMUvHOO\",\"ch7.modernconvolutionalneuralnetworks_mcnn_5.md\":\"CVkY2ACS\",\"ch7.modernconvolutionalneuralnetworks_mcnn_6.md\":\"Cin-j3ht\",\"ch7.modernconvolutionalneuralnetworks_mcnn_7.md\":\"BY0qhW6e\",\"ch7.modernconvolutionalneuralnetworks_mcnn_8.md\":\"CfFt59lS\",\"ch8.recurrent_neural_networks_rnn_0.md\":\"CIW34d_V\",\"ch8.recurrent_neural_networks_rnn_1.md\":\"Dqwc86d0\",\"ch8.recurrent_neural_networks_rnn_2.md\":\"DVOnaLjw\",\"ch8.recurrent_neural_networks_rnn_3.md\":\"BzfHLTyo\",\"ch8.recurrent_neural_networks_rnn_4.md\":\"DnQjCEE3\",\"ch8.recurrent_neural_networks_rnn_5.md\":\"6IPEYW7M\",\"ch8.recurrent_neural_networks_rnn_6.md\":\"GS6Ynndo\",\"ch8.recurrent_neural_networks_rnn_7.md\":\"B3soXDbW\",\"ch9.modern_recurrent_neural_networks_mrnn_1.md\":\"zGzQsIzl\",\"ch9.modern_recurrent_neural_networks_mrnn_2.md\":\"CV4lwlnE\",\"ch9.modern_recurrent_neural_networks_mrnn_3.md\":\"DI9bj1mT\",\"ch9.modern_recurrent_neural_networks_mrnn_4.md\":\"CvMg8EjP\",\"ch9.modern_recurrent_neural_networks_mrnn_5.md\":\"CuOfLj02\",\"ch9.modern_recurrent_neural_networks_mrnn_6.md\":\"BrLASjSi\",\"ch9.modern_recurrent_neural_networks_mrnn_7.md\":\"D5R7Lv49\",\"chapters.md\":\"BCfSINH3\",\"index.md\":\"C9fKtYo7\",\"references.md\":\"Dwf5fRK0\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"d2l Julia\",\"description\":\"Documentation for d2l-julia\",\"base\":\"/d2l-julia/previews/PR1/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"src\":\"/logo.png\",\"width\":24,\"height\":24},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Chapters\",\"collapsed\":false,\"items\":[{\"text\":\"ðŸ“˜ Chapters Overview\",\"link\":\"/chapters\"},{\"text\":\"Linear Neural Networks for Regression\",\"collapsed\":false,\"items\":[{\"text\":\"Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_1\"},{\"text\":\"Multiple Dispatch Design for Implementation\",\"link\":\"/CH3.Linear_Regression/LNN_2\"},{\"text\":\"Synthetic Regression Data\",\"link\":\"/CH3.Linear_Regression/LNN_3\"},{\"text\":\"Linear Regression Implementation from Scratch\",\"link\":\"/CH3.Linear_Regression/LNN_4\"},{\"text\":\"Concise Implementation of Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_5\"},{\"text\":\"Generalization\",\"link\":\"/CH3.Linear_Regression/LNN_6\"},{\"text\":\"Weight Decay\",\"link\":\"/CH3.Linear_Regression/LNN_7\"}]},{\"text\":\"Linear Neural Networks for Classification\",\"collapsed\":false,\"items\":[{\"text\":\"Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_1\"},{\"text\":\"The Image Classification Dataset\",\"link\":\"/CH4.Linear_Classification/LCN_2\"},{\"text\":\"Softmax Regression Implementation from Scratch\",\"link\":\"/CH4.Linear_Classification/LCN_3\"},{\"text\":\"Concise Implementation of Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_4\"},{\"text\":\"Generalization in Classification\",\"link\":\"/CH4.Linear_Classification/LCN_5\"},{\"text\":\"Environment and Distribution Shift\",\"link\":\"/CH4.Linear_Classification/LCN_6\"}]},{\"text\":\"Multilayer Perceptron\",\"collapsed\":false,\"items\":[{\"text\":\"Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_1\"},{\"text\":\"Implementation of Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_2\"},{\"text\":\"Forward Propagation, Backward Propagation, and Computational Graphs\",\"link\":\"/CH5.MLP/MLP_3\"},{\"text\":\"Numerical Stability and Initialization\",\"link\":\"/CH5.MLP/MLP_4\"},{\"text\":\"Generalization in Deep Learning\",\"link\":\"/CH5.MLP/MLP_5\"},{\"text\":\"Dropout\",\"link\":\"/CH5.MLP/MLP_6\"}]},{\"text\":\"Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Convolutions for Images\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_2\"},{\"text\":\"Padding and Stride\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_3\"},{\"text\":\"Multiple Input and Multiple Output Channels\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_4\"},{\"text\":\"Pooling\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_5\"},{\"text\":\"Convolutional Neural Networks (LeNet)\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_6\"}]},{\"text\":\"Modern Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Modern Convolutional Neural Networks\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_0\"},{\"text\":\"Deep Convolutional Neural Networks (AlexNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_1\"},{\"text\":\"Networks Using Blocks (VGG)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_2\"},{\"text\":\"Network in Network (NiN)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_3\"},{\"text\":\"Multi-Branch Networks  (GoogLeNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_4\"},{\"text\":\"Batch Normalization\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_5\"},{\"text\":\"Residual Networks (ResNet) and ResNeXt\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_6\"},{\"text\":\"Densely Connected Networks (DenseNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_7\"},{\"text\":\"Designing Convolution Network Architectures\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_8\"}]},{\"text\":\"Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_0\"},{\"text\":\"Working with Sequences\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_1\"},{\"text\":\"Converting Raw Text into Sequence Data\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_2\"},{\"text\":\"Language Models\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_3\"},{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_4\"},{\"text\":\"Recurrent Neural Network Implementation from Scratch\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_5\"},{\"text\":\"Concise Implementation of Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_6\"},{\"text\":\"Backpropagation Through Time\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_7\"}]},{\"text\":\"Modern Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Long Short-Term Memory (LSTM)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_1\"},{\"text\":\"Gated Recurrent Units (GRU)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_2\"},{\"text\":\"Deep Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_3\"},{\"text\":\"Bidirectional Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_4\"},{\"text\":\"Machine Translation and the Dataset\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_5\"},{\"text\":\"The Encoderâ€“Decoder Architecture\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_6\"},{\"text\":\"Sequence-to-Sequence Learning for Machine Translation\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_7\"}]},{\"text\":\"Attention Mechanisms and Transformers\",\"collapsed\":false,\"items\":[{\"text\":\"Queries, Keys, and Values\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_1\"},{\"text\":\"Attention Pooling by Similarity\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_2\"},{\"text\":\"Attention Scoring Functions\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_3\"},{\"text\":\"The Bahdanau Attention Mechanism\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_4\"},{\"text\":\"Multi-Head Attention\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_5\"},{\"text\":\"Self-Attention and Positional Encoding\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_6\"},{\"text\":\"CH10.Attention_Mechanisms_and_Transformers/Untitled\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/Untitled\"}]}]},{\"text\":\"References\",\"link\":\"/references\"},{\"component\":\"VersionPicker\"}],\"sidebar\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Chapters\",\"collapsed\":false,\"items\":[{\"text\":\"ðŸ“˜ Chapters Overview\",\"link\":\"/chapters\"},{\"text\":\"Linear Neural Networks for Regression\",\"collapsed\":false,\"items\":[{\"text\":\"Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_1\"},{\"text\":\"Multiple Dispatch Design for Implementation\",\"link\":\"/CH3.Linear_Regression/LNN_2\"},{\"text\":\"Synthetic Regression Data\",\"link\":\"/CH3.Linear_Regression/LNN_3\"},{\"text\":\"Linear Regression Implementation from Scratch\",\"link\":\"/CH3.Linear_Regression/LNN_4\"},{\"text\":\"Concise Implementation of Linear Regression\",\"link\":\"/CH3.Linear_Regression/LNN_5\"},{\"text\":\"Generalization\",\"link\":\"/CH3.Linear_Regression/LNN_6\"},{\"text\":\"Weight Decay\",\"link\":\"/CH3.Linear_Regression/LNN_7\"}]},{\"text\":\"Linear Neural Networks for Classification\",\"collapsed\":false,\"items\":[{\"text\":\"Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_1\"},{\"text\":\"The Image Classification Dataset\",\"link\":\"/CH4.Linear_Classification/LCN_2\"},{\"text\":\"Softmax Regression Implementation from Scratch\",\"link\":\"/CH4.Linear_Classification/LCN_3\"},{\"text\":\"Concise Implementation of Softmax Regression\",\"link\":\"/CH4.Linear_Classification/LCN_4\"},{\"text\":\"Generalization in Classification\",\"link\":\"/CH4.Linear_Classification/LCN_5\"},{\"text\":\"Environment and Distribution Shift\",\"link\":\"/CH4.Linear_Classification/LCN_6\"}]},{\"text\":\"Multilayer Perceptron\",\"collapsed\":false,\"items\":[{\"text\":\"Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_1\"},{\"text\":\"Implementation of Multilayer Perceptrons\",\"link\":\"/CH5.MLP/MLP_2\"},{\"text\":\"Forward Propagation, Backward Propagation, and Computational Graphs\",\"link\":\"/CH5.MLP/MLP_3\"},{\"text\":\"Numerical Stability and Initialization\",\"link\":\"/CH5.MLP/MLP_4\"},{\"text\":\"Generalization in Deep Learning\",\"link\":\"/CH5.MLP/MLP_5\"},{\"text\":\"Dropout\",\"link\":\"/CH5.MLP/MLP_6\"}]},{\"text\":\"Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Convolutions for Images\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_2\"},{\"text\":\"Padding and Stride\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_3\"},{\"text\":\"Multiple Input and Multiple Output Channels\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_4\"},{\"text\":\"Pooling\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_5\"},{\"text\":\"Convolutional Neural Networks (LeNet)\",\"link\":\"/CH6.Convolutional_Neural_Networks/CNN_6\"}]},{\"text\":\"Modern Convolutional Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Modern Convolutional Neural Networks\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_0\"},{\"text\":\"Deep Convolutional Neural Networks (AlexNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_1\"},{\"text\":\"Networks Using Blocks (VGG)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_2\"},{\"text\":\"Network in Network (NiN)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_3\"},{\"text\":\"Multi-Branch Networks  (GoogLeNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_4\"},{\"text\":\"Batch Normalization\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_5\"},{\"text\":\"Residual Networks (ResNet) and ResNeXt\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_6\"},{\"text\":\"Densely Connected Networks (DenseNet)\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_7\"},{\"text\":\"Designing Convolution Network Architectures\",\"link\":\"/CH7.ModernConvolutionalNeuralNetworks/MCNN_8\"}]},{\"text\":\"Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_0\"},{\"text\":\"Working with Sequences\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_1\"},{\"text\":\"Converting Raw Text into Sequence Data\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_2\"},{\"text\":\"Language Models\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_3\"},{\"text\":\"Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_4\"},{\"text\":\"Recurrent Neural Network Implementation from Scratch\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_5\"},{\"text\":\"Concise Implementation of Recurrent Neural Networks\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_6\"},{\"text\":\"Backpropagation Through Time\",\"link\":\"/CH8.Recurrent_Neural_Networks/RNN_7\"}]},{\"text\":\"Modern Recurrent Neural Networks\",\"collapsed\":false,\"items\":[{\"text\":\"Long Short-Term Memory (LSTM)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_1\"},{\"text\":\"Gated Recurrent Units (GRU)\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_2\"},{\"text\":\"Deep Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_3\"},{\"text\":\"Bidirectional Recurrent Neural Networks\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_4\"},{\"text\":\"Machine Translation and the Dataset\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_5\"},{\"text\":\"The Encoderâ€“Decoder Architecture\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_6\"},{\"text\":\"Sequence-to-Sequence Learning for Machine Translation\",\"link\":\"/CH9.Modern_Recurrent_Neural_Networks/MRNN_7\"}]},{\"text\":\"Attention Mechanisms and Transformers\",\"collapsed\":false,\"items\":[{\"text\":\"Queries, Keys, and Values\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_1\"},{\"text\":\"Attention Pooling by Similarity\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_2\"},{\"text\":\"Attention Scoring Functions\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_3\"},{\"text\":\"The Bahdanau Attention Mechanism\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_4\"},{\"text\":\"Multi-Head Attention\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_5\"},{\"text\":\"Self-Attention and Positional Encoding\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/ATTN_6\"},{\"text\":\"CH10.Attention_Mechanisms_and_Transformers/Untitled\",\"link\":\"/CH10.Attention_Mechanisms_and_Transformers/Untitled\"}]}]},{\"text\":\"References\",\"link\":\"/references\"}],\"editLink\":{\"pattern\":\"https://https://github.com/ashutosh-b-b/d2l-julia/edit/master/docs/src/:path\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/ashutosh-b-b/d2l-julia\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/dev/\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>\",\"copyright\":\"Â© Copyright 2025.\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>