<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Densely Connected Networks (DenseNet) · d2l Julia</title><meta name="title" content="Densely Connected Networks (DenseNet) · d2l Julia"/><meta property="og:title" content="Densely Connected Networks (DenseNet) · d2l Julia"/><meta property="twitter:title" content="Densely Connected Networks (DenseNet) · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../MCNN_3/">-</a></li><li><a class="tocitem" href="../MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../MCNN_5/">-</a></li><li><a class="tocitem" href="../MCNN_6/">Residual Networks (ResNet) and ResNeXt</a></li><li class="is-active"><a class="tocitem" href>Densely Connected Networks (DenseNet)</a><ul class="internal"><li><a class="tocitem" href="#From-ResNet-to-DenseNet"><span>From ResNet to DenseNet</span></a></li><li><a class="tocitem" href="#Dense-Blocks"><span>Dense Blocks</span></a></li><li><a class="tocitem" href="#Transition-Layers"><span>Transition Layers</span></a></li><li><a class="tocitem" href="#DenseNet-Model"><span>DenseNet Model</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Summary-and-Discussion"><span>Summary and Discussion</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Modern Convolutional Neural Networks</a></li><li class="is-active"><a href>Densely Connected Networks (DenseNet)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Densely Connected Networks (DenseNet)</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Densely-Connected-Networks-(DenseNet)"><a class="docs-heading-anchor" href="#Densely-Connected-Networks-(DenseNet)">Densely Connected Networks (DenseNet)</a><a id="Densely-Connected-Networks-(DenseNet)-1"></a><a class="docs-heading-anchor-permalink" href="#Densely-Connected-Networks-(DenseNet)" title="Permalink"></a></h1><p>ResNet significantly changed the view of how to parametrize the functions in deep networks. <em>DenseNet</em> (dense convolutional network) is to some extent the logical extension of this :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>. DenseNet is characterized by both the connectivity pattern where each layer connects to all the preceding layers and the concatenation operation (rather than the addition operator in ResNet) to preserve and reuse features from earlier layers. To understand how to arrive at it, let&#39;s take a small detour to mathematics.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Flux 
using CUDA, cuDNN</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/d2l-julia/d2lai`</code></pre><h2 id="From-ResNet-to-DenseNet"><a class="docs-heading-anchor" href="#From-ResNet-to-DenseNet">From ResNet to DenseNet</a><a id="From-ResNet-to-DenseNet-1"></a><a class="docs-heading-anchor-permalink" href="#From-ResNet-to-DenseNet" title="Permalink"></a></h2><p>Recall the Taylor expansion for functions. At the point <span>$x = 0$</span> it can be written as</p><p class="math-container">\[f(x) = f(0) + x \cdot \left[f&#39;(0) + x \cdot \left[\frac{f&#39;&#39;(0)}{2!}  + x \cdot \left[\frac{f&#39;&#39;&#39;(0)}{3!}  + \cdots \right]\right]\right].\]</p><p>The key point is that it decomposes a function into terms of increasingly higher order. In a similar vein, ResNet decomposes functions into</p><p class="math-container">\[f(\mathbf{x}) = \mathbf{x} + g(\mathbf{x}).\]</p><p>That is, ResNet decomposes <span>$f$</span> into a simple linear term and a more complex nonlinear one. What if we wanted to capture (not necessarily add) information beyond two terms? One such solution is DenseNet :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>.</p><p><img src="../../img/densenet-block.svg" alt="The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation. "/> :label:<code>fig_densenet_block</code></p><p>As shown in :numref:<code>fig_densenet_block</code>, the key difference between ResNet and DenseNet is that in the latter case outputs are <em>concatenated</em> (denoted by <span>$[,]$</span>) rather than added. As a result, we perform a mapping from <span>$\mathbf{x}$</span> to its values after applying an increasingly complex sequence of functions:</p><p class="math-container">\[\mathbf{x} \to \left[
\mathbf{x},
f_1(\mathbf{x}),
f_2\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right)\right]\right), f_3\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right), f_2\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right)\right]\right)\right]\right), \ldots\right].\]</p><p>In the end, all these functions are combined in MLP to reduce the number of features again. In terms of implementation this is quite simple: rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The final layer of such a chain is densely connected to all previous layers. The dense connections are shown in :numref:<code>fig_densenet</code>.</p><p><img src="../../img/densenet.svg" alt="Dense connections in DenseNet. Note how the dimensionality increases with depth."/> :label:<code>fig_densenet</code></p><p>The main components that comprise a DenseNet are <em>dense blocks</em> and <em>transition layers</em>. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large,  since the expansion <span>$\mathbf{x} \to \left[\mathbf{x}, f_1(\mathbf{x}), f_2\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right)\right]\right), \ldots \right]$</span> can be quite high-dimensional.</p><h2 id="Dense-Blocks"><a class="docs-heading-anchor" href="#Dense-Blocks">Dense Blocks</a><a id="Dense-Blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-Blocks" title="Permalink"></a></h2><p>DenseNet uses the modified &quot;batch normalization, activation, and convolution&quot; structure of ResNet (see the exercise in :numref:<code>sec_resnet</code>). First, we implement this convolution block structure.</p><pre><code class="language-julia hljs">struct DenseNetConvBlock{N} &lt;: AbstractModel 
    net::N 
end 

Flux.@layer DenseNetConvBlock

(d::DenseNetConvBlock)(x) = d.net(x)

function DenseNetConvBlock(channel_in::Int, num_channels::Int)
    net = Chain(
        Conv((3,3), channel_in =&gt; num_channels, pad = 1),
        BatchNorm(num_channels, relu)
    )
end</code></pre><pre><code class="nohighlight hljs">DenseNetConvBlock</code></pre><p>A <em>dense block</em> consists of multiple convolution blocks, each using the same number of output channels. In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension. Lazy evaluation allows us to adjust the dimensionality automatically.</p><pre><code class="language-julia hljs">struct DenseBlock{N} &lt;: AbstractModel 
    net::N 
end 
Flux.@layer DenseBlock

function DenseBlock(channel_in::Int, num_convs, num_channels; return_output_channels = false)
    prev_channels = channel_in
    conv_layers = map(1:num_convs) do i 
        block = DenseNetConvBlock(prev_channels, num_channels)
        prev_channels += num_channels 
        return block
    end
    net = DenseBlock(conv_layers)
    if return_output_channels
        return net, prev_channels 
    else 
        return net
    end
end

function (d::DenseBlock)(x)
    for block in d.net
        y = block(x)
        x = cat(x, y; dims = 3)
    end
    return x
end</code></pre><p>In the following example, we define a <code>DenseBlock</code> instance with two convolution blocks of 10 output channels. When using an input with three channels, we will get an output with  <span>$3 + 10 + 10=23$</span> channels. The number of convolution block channels controls the growth in the number of output channels relative to the number of input channels. This is also referred to as the <em>growth rate</em>.</p><pre><code class="language-julia hljs">block = DenseBlock(3, 2, 10)
block(rand(8,8,3,16)) |&gt; size</code></pre><pre><code class="nohighlight hljs">(8, 8, 23, 16)</code></pre><h2 id="Transition-Layers"><a class="docs-heading-anchor" href="#Transition-Layers">Transition Layers</a><a id="Transition-Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Transition-Layers" title="Permalink"></a></h2><p>Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A <em>transition layer</em> is used to control the complexity of the model. It reduces the number of channels by using a <span>$1\times 1$</span> convolution. Moreover, it halves the height and width via average pooling with a stride of 2.</p><pre><code class="language-julia hljs">struct DenseNetTransitionBlock{N} &lt;: AbstractModel 
    net::N
end
Flux.@layer DenseNetTransitionBlock

function DenseNetTransitionBlock(channel_in, num_channels)
    net = Chain(
        BatchNorm(channel_in, relu),
        Conv((1,1), channel_in =&gt; num_channels),
        MeanPool((2,2), pad = 1, stride = 2)
    )
end

(dtb::DenseNetTransitionBlock)(x) = dtb.net(x)</code></pre><p>Apply a transition layer with 10 channels to the output of the dense block in the previous example.  This reduces the number of output channels to 10, and halves the height and width.</p><pre><code class="language-julia hljs">block = DenseNetTransitionBlock(23, 10)
block(rand(8, 8, 23, 16)) |&gt; size</code></pre><pre><code class="nohighlight hljs">(5, 5, 10, 16)</code></pre><h2 id="DenseNet-Model"><a class="docs-heading-anchor" href="#DenseNet-Model">DenseNet Model</a><a id="DenseNet-Model-1"></a><a class="docs-heading-anchor-permalink" href="#DenseNet-Model" title="Permalink"></a></h2><p>Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and max-pooling layer as in ResNet.</p><pre><code class="language-julia hljs">struct DenseNetB1{N} &lt;: AbstractModel
    net::N 
end
Flux.@layer DenseNetB1
(b1::DenseNetB1)(x) = b1.net(x)

function DenseNetB1(channel_in::Int = 1)
    net = Chain(
        Conv((7,7), channel_in =&gt; 64, pad = 3, stride =2),
        BatchNorm(64, relu),
        MaxPool((3,3), stride = 2, pad = 1)
    )
    DenseNetB1(net)
end</code></pre><pre><code class="nohighlight hljs">DenseNetB1</code></pre><p>Then, similar to the four modules made up of residual blocks that ResNet uses, DenseNet uses four dense blocks. As with ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4, consistent with the ResNet-18 model in :numref:<code>sec_resnet</code>. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block.</p><p>In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. Here, we use the transition layer to halve the height and width and halve the number of channels. Similar to ResNet, a global pooling layer and a fully connected layer are connected at the end to produce the output.</p><pre><code class="language-julia hljs">struct DenseNet{N} &lt;:AbstractClassifier
    net::N
end
Flux.@layer DenseNet 
(dn::DenseNet)(x) = dn.net(x)

function DenseNet(channel_in::Int = 1; growth_rate = 32, arch = (4,4,4,4), num_classes = 10)
    prev_channels = 64
    layers = []
    for i in 1:length(arch)
        block, prev_channels = DenseBlock(prev_channels, arch[i], growth_rate; return_output_channels = true)
        push!(layers, block)
        if i != length(arch)
            transition_layer = DenseNetTransitionBlock(prev_channels, prev_channels ÷ 2)
            prev_channels = prev_channels ÷ 2
            push!(layers, transition_layer)
        end
    end
    net = Flux.@autosize (96, 96, 1, 1) Chain(
        DenseNetB1(channel_in),
        layers...,
        GlobalMeanPool(),
        Flux.flatten,
        Dense(_ =&gt; num_classes),
        softmax

    )
    DenseNet(net)
end</code></pre><pre><code class="nohighlight hljs">DenseNet</code></pre><pre><code class="language-julia hljs">model = DenseNet()
</code></pre><pre><code class="nohighlight hljs">DenseNet(
  Chain(
    DenseNetB1(
      Chain(
        Conv((7, 7), 1 =&gt; 64, pad=3, stride=2),  # 3_200 parameters
        BatchNorm(64, relu),            # 128 parameters, plus 128
        MaxPool((3, 3), pad=1, stride=2),
      ),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 64 =&gt; 32, pad=1),  # 18_464 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 96 =&gt; 32, pad=1),  # 27_680 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    Chain(
      BatchNorm(192, relu),             # 384 parameters, plus 384
      Conv((1, 1), 192 =&gt; 96),          # 18_528 parameters
      MeanPool((2, 2), pad=1),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 96 =&gt; 32, pad=1),  # 27_680 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 192 =&gt; 32, pad=1),  # 55_328 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    Chain(
      BatchNorm(224, relu),             # 448 parameters, plus 448
      Conv((1, 1), 224 =&gt; 112),         # 25_200 parameters
      MeanPool((2, 2), pad=1),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 112 =&gt; 32, pad=1),  # 32_288 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 144 =&gt; 32, pad=1),  # 41_504 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 176 =&gt; 32, pad=1),  # 50_720 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 208 =&gt; 32, pad=1),  # 59_936 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    Chain(
      BatchNorm(240, relu),             # 480 parameters, plus 480
      Conv((1, 1), 240 =&gt; 120),         # 28_920 parameters
      MeanPool((2, 2), pad=1),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 120 =&gt; 32, pad=1),  # 34_592 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 152 =&gt; 32, pad=1),  # 43_808 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 184 =&gt; 32, pad=1),  # 53_024 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 216 =&gt; 32, pad=1),  # 62_240 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    GlobalMeanPool(),
    Flux.flatten,
    Dense(248 =&gt; 10),                   # 2_490 parameters
    NNlib.softmax,
  ),
)         # Total: 82 trainable arrays, 754_082 parameters,
          # plus 40 non-trainable, 2_464 parameters, summarysize 2.894 MiB.</code></pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>Since we are using a deeper network here, in this section, we will reduce the input height and width from 224 to 96 to simplify the computation.</p><pre><code class="language-julia hljs">model = DenseNet()
data = d2lai.FashionMNISTData(batchsize = 128, resize = (96, 96))
opt = Descent(0.01)
trainer = Trainer(model, data, opt; max_epochs = 10, gpu = true, board_yscale = :identity)
d2lai.fit(trainer);</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 0.63423675, Val Loss: 0.44235954, Val Acc: 0.875
    [ Info: Train Loss: 0.26092514, Val Loss: 0.35349703, Val Acc: 0.9375
    [ Info: Train Loss: 0.23870032, Val Loss: 0.54305226, Val Acc: 0.8125
    [ Info: Train Loss: 0.36317107, Val Loss: 0.43822515, Val Acc: 0.875
    [ Info: Train Loss: 0.3757471, Val Loss: 0.3553314, Val Acc: 0.875
    [ Info: Train Loss: 0.16658472, Val Loss: 0.48415565, Val Acc: 0.875
    [ Info: Train Loss: 0.16947578, Val Loss: 0.3061001, Val Acc: 0.875
    [ Info: Train Loss: 0.25147822, Val Loss: 0.4952318, Val Acc: 0.8125
    [ Info: Train Loss: 0.14965303, Val Loss: 0.32668728, Val Acc: 0.875
    [ Info: Train Loss: 0.14355798, Val Loss: 0.32147193, Val Acc: 0.875</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip230">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip230)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip231">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip230)" d="M156.598 1423.18 L2352.76 1423.18 L2352.76 47.2441 L156.598 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip232">
    <rect x="156" y="47" width="2197" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="448.959,1423.18 448.959,47.2441 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="909.369,1423.18 909.369,47.2441 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1369.78,1423.18 1369.78,47.2441 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1830.19,1423.18 1830.19,47.2441 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2290.6,1423.18 2290.6,47.2441 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,1408.07 2352.76,1408.07 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,1045.43 2352.76,1045.43 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,682.786 2352.76,682.786 "/>
<polyline clip-path="url(#clip232)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,320.145 2352.76,320.145 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="448.959,1423.18 448.959,1404.28 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="909.369,1423.18 909.369,1404.28 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1369.78,1423.18 1369.78,1404.28 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1830.19,1423.18 1830.19,1404.28 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2290.6,1423.18 2290.6,1404.28 "/>
<path clip-path="url(#clip230)" d="M443.612 1481.64 L459.931 1481.64 L459.931 1485.58 L437.987 1485.58 L437.987 1481.64 Q440.649 1478.89 445.232 1474.26 Q449.838 1469.61 451.019 1468.27 Q453.264 1465.74 454.144 1464.01 Q455.047 1462.25 455.047 1460.56 Q455.047 1457.8 453.102 1456.07 Q451.181 1454.33 448.079 1454.33 Q445.88 1454.33 443.426 1455.09 Q440.996 1455.86 438.218 1457.41 L438.218 1452.69 Q441.042 1451.55 443.496 1450.97 Q445.95 1450.39 447.987 1450.39 Q453.357 1450.39 456.551 1453.08 Q459.746 1455.77 459.746 1460.26 Q459.746 1462.39 458.936 1464.31 Q458.149 1466.2 456.042 1468.8 Q455.463 1469.47 452.362 1472.69 Q449.26 1475.88 443.612 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M912.379 1455.09 L900.573 1473.54 L912.379 1473.54 L912.379 1455.09 M911.152 1451.02 L917.031 1451.02 L917.031 1473.54 L921.962 1473.54 L921.962 1477.43 L917.031 1477.43 L917.031 1485.58 L912.379 1485.58 L912.379 1477.43 L896.777 1477.43 L896.777 1472.92 L911.152 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1370.18 1466.44 Q1367.04 1466.44 1365.18 1468.59 Q1363.36 1470.74 1363.36 1474.49 Q1363.36 1478.22 1365.18 1480.39 Q1367.04 1482.55 1370.18 1482.55 Q1373.33 1482.55 1375.16 1480.39 Q1377.01 1478.22 1377.01 1474.49 Q1377.01 1470.74 1375.16 1468.59 Q1373.33 1466.44 1370.18 1466.44 M1379.47 1451.78 L1379.47 1456.04 Q1377.71 1455.21 1375.9 1454.77 Q1374.12 1454.33 1372.36 1454.33 Q1367.73 1454.33 1365.28 1457.45 Q1362.85 1460.58 1362.5 1466.9 Q1363.87 1464.89 1365.93 1463.82 Q1367.99 1462.73 1370.46 1462.73 Q1375.67 1462.73 1378.68 1465.9 Q1381.71 1469.05 1381.71 1474.49 Q1381.71 1479.82 1378.56 1483.03 Q1375.42 1486.25 1370.18 1486.25 Q1364.19 1486.25 1361.02 1481.67 Q1357.85 1477.06 1357.85 1468.33 Q1357.85 1460.14 1361.74 1455.28 Q1365.62 1450.39 1372.18 1450.39 Q1373.93 1450.39 1375.72 1450.74 Q1377.52 1451.09 1379.47 1451.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1830.19 1469.17 Q1826.86 1469.17 1824.94 1470.95 Q1823.04 1472.73 1823.04 1475.86 Q1823.04 1478.98 1824.94 1480.77 Q1826.86 1482.55 1830.19 1482.55 Q1833.52 1482.55 1835.44 1480.77 Q1837.37 1478.96 1837.37 1475.86 Q1837.37 1472.73 1835.44 1470.95 Q1833.55 1469.17 1830.19 1469.17 M1825.51 1467.18 Q1822.5 1466.44 1820.82 1464.38 Q1819.15 1462.32 1819.15 1459.35 Q1819.15 1455.21 1822.09 1452.8 Q1825.05 1450.39 1830.19 1450.39 Q1835.35 1450.39 1838.29 1452.8 Q1841.23 1455.21 1841.23 1459.35 Q1841.23 1462.32 1839.54 1464.38 Q1837.88 1466.44 1834.89 1467.18 Q1838.27 1467.96 1840.14 1470.26 Q1842.04 1472.55 1842.04 1475.86 Q1842.04 1480.88 1838.96 1483.57 Q1835.91 1486.25 1830.19 1486.25 Q1824.47 1486.25 1821.39 1483.57 Q1818.34 1480.88 1818.34 1475.86 Q1818.34 1472.55 1820.24 1470.26 Q1822.13 1467.96 1825.51 1467.18 M1823.8 1459.79 Q1823.8 1462.48 1825.47 1463.98 Q1827.16 1465.49 1830.19 1465.49 Q1833.2 1465.49 1834.89 1463.98 Q1836.6 1462.48 1836.6 1459.79 Q1836.6 1457.11 1834.89 1455.6 Q1833.2 1454.1 1830.19 1454.1 Q1827.16 1454.1 1825.47 1455.6 Q1823.8 1457.11 1823.8 1459.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M2265.29 1481.64 L2272.93 1481.64 L2272.93 1455.28 L2264.62 1456.95 L2264.62 1452.69 L2272.88 1451.02 L2277.56 1451.02 L2277.56 1481.64 L2285.2 1481.64 L2285.2 1485.58 L2265.29 1485.58 L2265.29 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M2304.64 1454.1 Q2301.03 1454.1 2299.2 1457.66 Q2297.39 1461.2 2297.39 1468.33 Q2297.39 1475.44 2299.2 1479.01 Q2301.03 1482.55 2304.64 1482.55 Q2308.27 1482.55 2310.08 1479.01 Q2311.91 1475.44 2311.91 1468.33 Q2311.91 1461.2 2310.08 1457.66 Q2308.27 1454.1 2304.64 1454.1 M2304.64 1450.39 Q2310.45 1450.39 2313.51 1455 Q2316.58 1459.58 2316.58 1468.33 Q2316.58 1477.06 2313.51 1481.67 Q2310.45 1486.25 2304.64 1486.25 Q2298.83 1486.25 2295.75 1481.67 Q2292.7 1477.06 2292.7 1468.33 Q2292.7 1459.58 2295.75 1455 Q2298.83 1450.39 2304.64 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1174.87 1548.76 L1174.87 1551.62 L1147.94 1551.62 Q1148.32 1557.67 1151.57 1560.85 Q1154.85 1564 1160.67 1564 Q1164.05 1564 1167.2 1563.17 Q1170.38 1562.35 1173.5 1560.69 L1173.5 1566.23 Q1170.35 1567.57 1167.04 1568.27 Q1163.73 1568.97 1160.32 1568.97 Q1151.79 1568.97 1146.79 1564 Q1141.83 1559.04 1141.83 1550.57 Q1141.83 1541.82 1146.54 1536.69 Q1151.28 1531.54 1159.3 1531.54 Q1166.5 1531.54 1170.67 1536.18 Q1174.87 1540.8 1174.87 1548.76 M1169.01 1547.04 Q1168.95 1542.23 1166.31 1539.37 Q1163.7 1536.5 1159.37 1536.5 Q1154.46 1536.5 1151.5 1539.27 Q1148.58 1542.04 1148.13 1547.07 L1169.01 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1190.14 1562.7 L1190.14 1581.6 L1184.26 1581.6 L1184.26 1532.4 L1190.14 1532.4 L1190.14 1537.81 Q1191.99 1534.62 1194.79 1533.1 Q1197.62 1531.54 1201.54 1531.54 Q1208.03 1531.54 1212.07 1536.69 Q1216.15 1541.85 1216.15 1550.25 Q1216.15 1558.65 1212.07 1563.81 Q1208.03 1568.97 1201.54 1568.97 Q1197.62 1568.97 1194.79 1567.44 Q1191.99 1565.88 1190.14 1562.7 M1210.07 1550.25 Q1210.07 1543.79 1207.4 1540.13 Q1204.75 1536.44 1200.11 1536.44 Q1195.46 1536.44 1192.79 1540.13 Q1190.14 1543.79 1190.14 1550.25 Q1190.14 1556.71 1192.79 1560.4 Q1195.46 1564.07 1200.11 1564.07 Q1204.75 1564.07 1207.4 1560.4 Q1210.07 1556.71 1210.07 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1239.67 1536.5 Q1234.96 1536.5 1232.22 1540.19 Q1229.48 1543.85 1229.48 1550.25 Q1229.48 1556.65 1232.19 1560.34 Q1234.93 1564 1239.67 1564 Q1244.35 1564 1247.09 1560.31 Q1249.82 1556.62 1249.82 1550.25 Q1249.82 1543.92 1247.09 1540.23 Q1244.35 1536.5 1239.67 1536.5 M1239.67 1531.54 Q1247.31 1531.54 1251.67 1536.5 Q1256.03 1541.47 1256.03 1550.25 Q1256.03 1559 1251.67 1564 Q1247.31 1568.97 1239.67 1568.97 Q1232 1568.97 1227.64 1564 Q1223.31 1559 1223.31 1550.25 Q1223.31 1541.47 1227.64 1536.5 Q1232 1531.54 1239.67 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1291.39 1533.76 L1291.39 1539.24 Q1288.91 1537.87 1286.39 1537.2 Q1283.91 1536.5 1281.37 1536.5 Q1275.67 1536.5 1272.52 1540.13 Q1269.37 1543.73 1269.37 1550.25 Q1269.37 1556.78 1272.52 1560.4 Q1275.67 1564 1281.37 1564 Q1283.91 1564 1286.39 1563.33 Q1288.91 1562.63 1291.39 1561.26 L1291.39 1566.68 Q1288.94 1567.82 1286.3 1568.39 Q1283.69 1568.97 1280.73 1568.97 Q1272.68 1568.97 1267.93 1563.91 Q1263.19 1558.85 1263.19 1550.25 Q1263.19 1541.53 1267.97 1536.53 Q1272.77 1531.54 1281.11 1531.54 Q1283.82 1531.54 1286.39 1532.11 Q1288.97 1532.65 1291.39 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1331.21 1546.53 L1331.21 1568.04 L1325.35 1568.04 L1325.35 1546.72 Q1325.35 1541.66 1323.38 1539.14 Q1321.41 1536.63 1317.46 1536.63 Q1312.72 1536.63 1309.98 1539.65 Q1307.24 1542.68 1307.24 1547.9 L1307.24 1568.04 L1301.35 1568.04 L1301.35 1518.52 L1307.24 1518.52 L1307.24 1537.93 Q1309.34 1534.72 1312.18 1533.13 Q1315.04 1531.54 1318.76 1531.54 Q1324.91 1531.54 1328.06 1535.36 Q1331.21 1539.14 1331.21 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M1365.62 1533.45 L1365.62 1538.98 Q1363.13 1537.71 1360.46 1537.07 Q1357.79 1536.44 1354.92 1536.44 Q1350.56 1536.44 1348.36 1537.77 Q1346.2 1539.11 1346.2 1541.79 Q1346.2 1543.82 1347.76 1545 Q1349.32 1546.15 1354.03 1547.2 L1356.04 1547.64 Q1362.27 1548.98 1364.88 1551.43 Q1367.53 1553.85 1367.53 1558.21 Q1367.53 1563.17 1363.58 1566.07 Q1359.66 1568.97 1352.79 1568.97 Q1349.92 1568.97 1346.8 1568.39 Q1343.72 1567.85 1340.28 1566.74 L1340.28 1560.69 Q1343.53 1562.38 1346.68 1563.24 Q1349.83 1564.07 1352.92 1564.07 Q1357.05 1564.07 1359.28 1562.66 Q1361.51 1561.23 1361.51 1558.65 Q1361.51 1556.27 1359.89 1554.99 Q1358.29 1553.72 1352.85 1552.54 L1350.82 1552.07 Q1345.37 1550.92 1342.95 1548.56 Q1340.53 1546.18 1340.53 1542.04 Q1340.53 1537.01 1344.1 1534.27 Q1347.66 1531.54 1354.22 1531.54 Q1357.47 1531.54 1360.33 1532.01 Q1363.2 1532.49 1365.62 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1423.18 156.598,47.2441 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1408.07 175.496,1408.07 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1045.43 175.496,1045.43 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,682.786 175.496,682.786 "/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,320.145 175.496,320.145 "/>
<path clip-path="url(#clip230)" d="M65.0198 1393.87 Q61.4087 1393.87 59.58 1397.43 Q57.7745 1400.97 57.7745 1408.1 Q57.7745 1415.21 59.58 1418.77 Q61.4087 1422.32 65.0198 1422.32 Q68.6541 1422.32 70.4596 1418.77 Q72.2883 1415.21 72.2883 1408.1 Q72.2883 1400.97 70.4596 1397.43 Q68.6541 1393.87 65.0198 1393.87 M65.0198 1390.16 Q70.83 1390.16 73.8855 1394.77 Q76.9642 1399.35 76.9642 1408.1 Q76.9642 1416.83 73.8855 1421.44 Q70.83 1426.02 65.0198 1426.02 Q59.2097 1426.02 56.131 1421.44 Q53.0754 1416.83 53.0754 1408.1 Q53.0754 1399.35 56.131 1394.77 Q59.2097 1390.16 65.0198 1390.16 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M85.1818 1419.47 L90.066 1419.47 L90.066 1425.35 L85.1818 1425.35 L85.1818 1419.47 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M104.279 1421.41 L120.598 1421.41 L120.598 1425.35 L98.6539 1425.35 L98.6539 1421.41 Q101.316 1418.66 105.899 1414.03 Q110.506 1409.38 111.686 1408.03 Q113.932 1405.51 114.811 1403.77 Q115.714 1402.01 115.714 1400.32 Q115.714 1397.57 113.77 1395.83 Q111.848 1394.1 108.746 1394.1 Q106.547 1394.1 104.094 1394.86 Q101.663 1395.63 98.8854 1397.18 L98.8854 1392.45 Q101.709 1391.32 104.163 1390.74 Q106.617 1390.16 108.654 1390.16 Q114.024 1390.16 117.219 1392.85 Q120.413 1395.53 120.413 1400.02 Q120.413 1402.15 119.603 1404.07 Q118.816 1405.97 116.709 1408.57 Q116.131 1409.24 113.029 1412.45 Q109.927 1415.65 104.279 1421.41 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M62.9365 1031.23 Q59.3254 1031.23 57.4967 1034.79 Q55.6912 1038.33 55.6912 1045.46 Q55.6912 1052.57 57.4967 1056.13 Q59.3254 1059.67 62.9365 1059.67 Q66.5707 1059.67 68.3763 1056.13 Q70.205 1052.57 70.205 1045.46 Q70.205 1038.33 68.3763 1034.79 Q66.5707 1031.23 62.9365 1031.23 M62.9365 1027.52 Q68.7467 1027.52 71.8022 1032.13 Q74.8809 1036.71 74.8809 1045.46 Q74.8809 1054.19 71.8022 1058.79 Q68.7467 1063.38 62.9365 1063.38 Q57.1264 1063.38 54.0477 1058.79 Q50.9921 1054.19 50.9921 1045.46 Q50.9921 1036.71 54.0477 1032.13 Q57.1264 1027.52 62.9365 1027.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M83.0984 1056.83 L87.9827 1056.83 L87.9827 1062.71 L83.0984 1062.71 L83.0984 1056.83 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M111.015 1032.22 L99.2095 1050.67 L111.015 1050.67 L111.015 1032.22 M109.788 1028.15 L115.668 1028.15 L115.668 1050.67 L120.598 1050.67 L120.598 1054.56 L115.668 1054.56 L115.668 1062.71 L111.015 1062.71 L111.015 1054.56 L95.4132 1054.56 L95.4132 1050.04 L109.788 1028.15 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M63.2606 668.585 Q59.6495 668.585 57.8208 672.149 Q56.0152 675.691 56.0152 682.821 Q56.0152 689.927 57.8208 693.492 Q59.6495 697.033 63.2606 697.033 Q66.8948 697.033 68.7004 693.492 Q70.5291 689.927 70.5291 682.821 Q70.5291 675.691 68.7004 672.149 Q66.8948 668.585 63.2606 668.585 M63.2606 664.881 Q69.0707 664.881 72.1263 669.487 Q75.205 674.071 75.205 682.821 Q75.205 691.547 72.1263 696.154 Q69.0707 700.737 63.2606 700.737 Q57.4504 700.737 54.3717 696.154 Q51.3162 691.547 51.3162 682.821 Q51.3162 674.071 54.3717 669.487 Q57.4504 664.881 63.2606 664.881 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M83.4225 694.186 L88.3067 694.186 L88.3067 700.066 L83.4225 700.066 L83.4225 694.186 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M109.071 680.922 Q105.922 680.922 104.071 683.075 Q102.242 685.228 102.242 688.978 Q102.242 692.705 104.071 694.881 Q105.922 697.033 109.071 697.033 Q112.219 697.033 114.047 694.881 Q115.899 692.705 115.899 688.978 Q115.899 685.228 114.047 683.075 Q112.219 680.922 109.071 680.922 M118.353 666.27 L118.353 670.529 Q116.594 669.696 114.788 669.256 Q113.006 668.816 111.246 668.816 Q106.617 668.816 104.163 671.941 Q101.733 675.066 101.385 681.385 Q102.751 679.371 104.811 678.307 Q106.871 677.219 109.348 677.219 Q114.557 677.219 117.566 680.39 Q120.598 683.538 120.598 688.978 Q120.598 694.302 117.45 697.52 Q114.302 700.737 109.071 700.737 Q103.075 700.737 99.9039 696.154 Q96.7326 691.547 96.7326 682.821 Q96.7326 674.626 100.621 669.765 Q104.51 664.881 111.061 664.881 Q112.82 664.881 114.603 665.228 Q116.408 665.575 118.353 666.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M63.5152 305.944 Q59.9041 305.944 58.0754 309.508 Q56.2699 313.05 56.2699 320.18 Q56.2699 327.286 58.0754 330.851 Q59.9041 334.392 63.5152 334.392 Q67.1494 334.392 68.955 330.851 Q70.7837 327.286 70.7837 320.18 Q70.7837 313.05 68.955 309.508 Q67.1494 305.944 63.5152 305.944 M63.5152 302.24 Q69.3254 302.24 72.3809 306.846 Q75.4596 311.43 75.4596 320.18 Q75.4596 328.906 72.3809 333.513 Q69.3254 338.096 63.5152 338.096 Q57.7051 338.096 54.6264 333.513 Q51.5708 328.906 51.5708 320.18 Q51.5708 311.43 54.6264 306.846 Q57.7051 302.24 63.5152 302.24 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M83.6771 331.545 L88.5614 331.545 L88.5614 337.425 L83.6771 337.425 L83.6771 331.545 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M108.746 321.013 Q105.413 321.013 103.492 322.795 Q101.594 324.578 101.594 327.703 Q101.594 330.828 103.492 332.61 Q105.413 334.392 108.746 334.392 Q112.08 334.392 114.001 332.61 Q115.922 330.805 115.922 327.703 Q115.922 324.578 114.001 322.795 Q112.103 321.013 108.746 321.013 M104.071 319.022 Q101.061 318.281 99.3715 316.221 Q97.7048 314.161 97.7048 311.198 Q97.7048 307.055 100.645 304.647 Q103.608 302.24 108.746 302.24 Q113.908 302.24 116.848 304.647 Q119.788 307.055 119.788 311.198 Q119.788 314.161 118.098 316.221 Q116.432 318.281 113.445 319.022 Q116.825 319.809 118.7 322.101 Q120.598 324.393 120.598 327.703 Q120.598 332.726 117.52 335.411 Q114.464 338.096 108.746 338.096 Q103.029 338.096 99.9502 335.411 Q96.8947 332.726 96.8947 327.703 Q96.8947 324.393 98.7928 322.101 Q100.691 319.809 104.071 319.022 M102.358 311.638 Q102.358 314.323 104.024 315.828 Q105.714 317.332 108.746 317.332 Q111.756 317.332 113.445 315.828 Q115.158 314.323 115.158 311.638 Q115.158 308.953 113.445 307.448 Q111.756 305.944 108.746 305.944 Q105.714 305.944 104.024 307.448 Q102.358 308.953 102.358 311.638 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip232)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,86.1857 448.959,880.167 679.164,1054.28 909.369,1132.1 1139.57,1216.05 1369.78,1245.25 1599.98,1299.49 1830.19,1323.73 2060.4,1347.9 2290.6,1384.24 "/>
<polyline clip-path="url(#clip232)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,658.425 448.959,1002.99 679.164,1050.43 909.369,725.134 1139.57,1035.94 1369.78,1221.19 1599.98,1266.95 1830.19,1088.64 2060.4,1202.21 2290.6,1157.27 "/>
<polyline clip-path="url(#clip232)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,332.804 448.959,222.169 679.164,209.258 909.369,314.156 1139.57,223.783 1369.78,146.14 1599.98,138.071 1830.19,197.782 2060.4,153.851 2290.6,172.858 "/>
<path clip-path="url(#clip230)" d="M229.803 1377.32 L671.455 1377.32 L671.455 1169.96 L229.803 1169.96  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip230)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="229.803,1377.32 671.455,1377.32 671.455,1169.96 229.803,1169.96 229.803,1377.32 "/>
<polyline clip-path="url(#clip230)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,1221.8 400.616,1221.8 "/>
<path clip-path="url(#clip230)" d="M432.425 1205.79 L432.425 1213.15 L441.198 1213.15 L441.198 1216.46 L432.425 1216.46 L432.425 1230.53 Q432.425 1233.7 433.281 1234.61 Q434.161 1235.51 436.823 1235.51 L441.198 1235.51 L441.198 1239.08 L436.823 1239.08 Q431.892 1239.08 430.017 1237.25 Q428.142 1235.39 428.142 1230.53 L428.142 1216.46 L425.018 1216.46 L425.018 1213.15 L428.142 1213.15 L428.142 1205.79 L432.425 1205.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M461.823 1217.13 Q461.105 1216.71 460.249 1216.53 Q459.415 1216.32 458.397 1216.32 Q454.786 1216.32 452.841 1218.68 Q450.92 1221.02 450.92 1225.42 L450.92 1239.08 L446.638 1239.08 L446.638 1213.15 L450.92 1213.15 L450.92 1217.18 Q452.263 1214.82 454.415 1213.68 Q456.568 1212.52 459.647 1212.52 Q460.087 1212.52 460.619 1212.59 Q461.152 1212.64 461.8 1212.76 L461.823 1217.13 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M478.073 1226.04 Q472.911 1226.04 470.92 1227.22 Q468.929 1228.4 468.929 1231.25 Q468.929 1233.52 470.411 1234.86 Q471.915 1236.18 474.485 1236.18 Q478.026 1236.18 480.156 1233.68 Q482.309 1231.16 482.309 1226.99 L482.309 1226.04 L478.073 1226.04 M486.568 1224.28 L486.568 1239.08 L482.309 1239.08 L482.309 1235.14 Q480.851 1237.5 478.675 1238.64 Q476.499 1239.75 473.351 1239.75 Q469.369 1239.75 467.008 1237.52 Q464.67 1235.28 464.67 1231.53 Q464.67 1227.15 467.587 1224.93 Q470.527 1222.71 476.337 1222.71 L482.309 1222.71 L482.309 1222.29 Q482.309 1219.35 480.364 1217.76 Q478.443 1216.14 474.948 1216.14 Q472.726 1216.14 470.619 1216.67 Q468.513 1217.2 466.568 1218.27 L466.568 1214.33 Q468.906 1213.43 471.105 1212.99 Q473.304 1212.52 475.388 1212.52 Q481.013 1212.52 483.79 1215.44 Q486.568 1218.36 486.568 1224.28 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M495.341 1213.15 L499.6 1213.15 L499.6 1239.08 L495.341 1239.08 L495.341 1213.15 M495.341 1203.06 L499.6 1203.06 L499.6 1208.45 L495.341 1208.45 L495.341 1203.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M530.063 1223.43 L530.063 1239.08 L525.804 1239.08 L525.804 1223.57 Q525.804 1219.89 524.369 1218.06 Q522.934 1216.23 520.063 1216.23 Q516.614 1216.23 514.624 1218.43 Q512.633 1220.63 512.633 1224.42 L512.633 1239.08 L508.35 1239.08 L508.35 1213.15 L512.633 1213.15 L512.633 1217.18 Q514.161 1214.84 516.221 1213.68 Q518.304 1212.52 521.012 1212.52 Q525.48 1212.52 527.772 1215.3 Q530.063 1218.06 530.063 1223.43 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M558.258 1246.95 L558.258 1250.26 L533.628 1250.26 L533.628 1246.95 L558.258 1246.95 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M562.262 1203.06 L566.521 1203.06 L566.521 1239.08 L562.262 1239.08 L562.262 1203.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M585.48 1216.14 Q582.054 1216.14 580.063 1218.82 Q578.072 1221.48 578.072 1226.14 Q578.072 1230.79 580.04 1233.47 Q582.031 1236.14 585.48 1236.14 Q588.882 1236.14 590.873 1233.45 Q592.864 1230.77 592.864 1226.14 Q592.864 1221.53 590.873 1218.84 Q588.882 1216.14 585.48 1216.14 M585.48 1212.52 Q591.035 1212.52 594.206 1216.14 Q597.378 1219.75 597.378 1226.14 Q597.378 1232.5 594.206 1236.14 Q591.035 1239.75 585.48 1239.75 Q579.901 1239.75 576.73 1236.14 Q573.582 1232.5 573.582 1226.14 Q573.582 1219.75 576.73 1216.14 Q579.901 1212.52 585.48 1212.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M620.966 1213.91 L620.966 1217.94 Q619.16 1217.02 617.216 1216.55 Q615.271 1216.09 613.188 1216.09 Q610.017 1216.09 608.419 1217.06 Q606.845 1218.03 606.845 1219.98 Q606.845 1221.46 607.979 1222.32 Q609.114 1223.15 612.54 1223.91 L613.998 1224.24 Q618.535 1225.21 620.433 1226.99 Q622.354 1228.75 622.354 1231.92 Q622.354 1235.53 619.484 1237.64 Q616.637 1239.75 611.637 1239.75 Q609.554 1239.75 607.285 1239.33 Q605.04 1238.94 602.54 1238.13 L602.54 1233.73 Q604.901 1234.95 607.192 1235.58 Q609.484 1236.18 611.729 1236.18 Q614.739 1236.18 616.359 1235.16 Q617.979 1234.12 617.979 1232.25 Q617.979 1230.51 616.799 1229.58 Q615.641 1228.66 611.683 1227.8 L610.202 1227.46 Q606.243 1226.62 604.484 1224.91 Q602.725 1223.17 602.725 1220.16 Q602.725 1216.51 605.317 1214.52 Q607.91 1212.52 612.679 1212.52 Q615.04 1212.52 617.123 1212.87 Q619.206 1213.22 620.966 1213.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M645.664 1213.91 L645.664 1217.94 Q643.859 1217.02 641.914 1216.55 Q639.97 1216.09 637.887 1216.09 Q634.715 1216.09 633.118 1217.06 Q631.544 1218.03 631.544 1219.98 Q631.544 1221.46 632.678 1222.32 Q633.813 1223.15 637.239 1223.91 L638.697 1224.24 Q643.234 1225.21 645.132 1226.99 Q647.053 1228.75 647.053 1231.92 Q647.053 1235.53 644.183 1237.64 Q641.336 1239.75 636.336 1239.75 Q634.252 1239.75 631.984 1239.33 Q629.739 1238.94 627.239 1238.13 L627.239 1233.73 Q629.6 1234.95 631.891 1235.58 Q634.183 1236.18 636.428 1236.18 Q639.438 1236.18 641.058 1235.16 Q642.678 1234.12 642.678 1232.25 Q642.678 1230.51 641.498 1229.58 Q640.34 1228.66 636.382 1227.8 L634.901 1227.46 Q630.942 1226.62 629.183 1224.91 Q627.424 1223.17 627.424 1220.16 Q627.424 1216.51 630.016 1214.52 Q632.609 1212.52 637.377 1212.52 Q639.739 1212.52 641.822 1212.87 Q643.905 1213.22 645.664 1213.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip230)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,1273.64 400.616,1273.64 "/>
<path clip-path="url(#clip230)" d="M425.018 1264.99 L429.531 1264.99 L437.633 1286.75 L445.735 1264.99 L450.249 1264.99 L440.527 1290.92 L434.74 1290.92 L425.018 1264.99 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M467.911 1277.88 Q462.749 1277.88 460.758 1279.06 Q458.767 1280.24 458.767 1283.09 Q458.767 1285.36 460.249 1286.7 Q461.753 1288.02 464.323 1288.02 Q467.864 1288.02 469.994 1285.52 Q472.147 1283 472.147 1278.83 L472.147 1277.88 L467.911 1277.88 M476.406 1276.12 L476.406 1290.92 L472.147 1290.92 L472.147 1286.98 Q470.689 1289.34 468.513 1290.48 Q466.337 1291.59 463.189 1291.59 Q459.207 1291.59 456.846 1289.36 Q454.508 1287.12 454.508 1283.37 Q454.508 1278.99 457.425 1276.77 Q460.365 1274.55 466.175 1274.55 L472.147 1274.55 L472.147 1274.13 Q472.147 1271.19 470.202 1269.6 Q468.281 1267.98 464.786 1267.98 Q462.564 1267.98 460.457 1268.51 Q458.351 1269.04 456.406 1270.11 L456.406 1266.17 Q458.744 1265.27 460.943 1264.83 Q463.142 1264.36 465.226 1264.36 Q470.851 1264.36 473.628 1267.28 Q476.406 1270.2 476.406 1276.12 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M485.179 1254.9 L489.438 1254.9 L489.438 1290.92 L485.179 1290.92 L485.179 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M518.049 1298.79 L518.049 1302.1 L493.42 1302.1 L493.42 1298.79 L518.049 1298.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M522.054 1254.9 L526.313 1254.9 L526.313 1290.92 L522.054 1290.92 L522.054 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M545.271 1267.98 Q541.846 1267.98 539.855 1270.66 Q537.864 1273.32 537.864 1277.98 Q537.864 1282.63 539.832 1285.31 Q541.822 1287.98 545.271 1287.98 Q548.674 1287.98 550.665 1285.29 Q552.656 1282.61 552.656 1277.98 Q552.656 1273.37 550.665 1270.68 Q548.674 1267.98 545.271 1267.98 M545.271 1264.36 Q550.827 1264.36 553.998 1267.98 Q557.17 1271.59 557.17 1277.98 Q557.17 1284.34 553.998 1287.98 Q550.827 1291.59 545.271 1291.59 Q539.693 1291.59 536.522 1287.98 Q533.373 1284.34 533.373 1277.98 Q533.373 1271.59 536.522 1267.98 Q539.693 1264.36 545.271 1264.36 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M580.757 1265.75 L580.757 1269.78 Q578.952 1268.86 577.007 1268.39 Q575.063 1267.93 572.98 1267.93 Q569.808 1267.93 568.211 1268.9 Q566.637 1269.87 566.637 1271.82 Q566.637 1273.3 567.771 1274.16 Q568.906 1274.99 572.332 1275.75 L573.79 1276.08 Q578.327 1277.05 580.225 1278.83 Q582.146 1280.59 582.146 1283.76 Q582.146 1287.37 579.276 1289.48 Q576.429 1291.59 571.429 1291.59 Q569.345 1291.59 567.077 1291.17 Q564.832 1290.78 562.332 1289.97 L562.332 1285.57 Q564.693 1286.79 566.984 1287.42 Q569.276 1288.02 571.521 1288.02 Q574.531 1288.02 576.151 1287 Q577.771 1285.96 577.771 1284.09 Q577.771 1282.35 576.591 1281.42 Q575.433 1280.5 571.475 1279.64 L569.994 1279.3 Q566.035 1278.46 564.276 1276.75 Q562.517 1275.01 562.517 1272 Q562.517 1268.35 565.109 1266.36 Q567.702 1264.36 572.47 1264.36 Q574.832 1264.36 576.915 1264.71 Q578.998 1265.06 580.757 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M605.456 1265.75 L605.456 1269.78 Q603.651 1268.86 601.706 1268.39 Q599.762 1267.93 597.679 1267.93 Q594.507 1267.93 592.91 1268.9 Q591.336 1269.87 591.336 1271.82 Q591.336 1273.3 592.47 1274.16 Q593.605 1274.99 597.03 1275.75 L598.489 1276.08 Q603.026 1277.05 604.924 1278.83 Q606.845 1280.59 606.845 1283.76 Q606.845 1287.37 603.975 1289.48 Q601.128 1291.59 596.128 1291.59 Q594.044 1291.59 591.776 1291.17 Q589.531 1290.78 587.031 1289.97 L587.031 1285.57 Q589.392 1286.79 591.683 1287.42 Q593.975 1288.02 596.22 1288.02 Q599.23 1288.02 600.85 1287 Q602.47 1285.96 602.47 1284.09 Q602.47 1282.35 601.29 1281.42 Q600.132 1280.5 596.174 1279.64 L594.693 1279.3 Q590.734 1278.46 588.975 1276.75 Q587.216 1275.01 587.216 1272 Q587.216 1268.35 589.808 1266.36 Q592.401 1264.36 597.169 1264.36 Q599.53 1264.36 601.614 1264.71 Q603.697 1265.06 605.456 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip230)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,1325.48 400.616,1325.48 "/>
<path clip-path="url(#clip230)" d="M425.018 1316.83 L429.531 1316.83 L437.633 1338.59 L445.735 1316.83 L450.249 1316.83 L440.527 1342.76 L434.74 1342.76 L425.018 1316.83 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M467.911 1329.72 Q462.749 1329.72 460.758 1330.9 Q458.767 1332.08 458.767 1334.93 Q458.767 1337.2 460.249 1338.54 Q461.753 1339.86 464.323 1339.86 Q467.864 1339.86 469.994 1337.36 Q472.147 1334.84 472.147 1330.67 L472.147 1329.72 L467.911 1329.72 M476.406 1327.96 L476.406 1342.76 L472.147 1342.76 L472.147 1338.82 Q470.689 1341.18 468.513 1342.32 Q466.337 1343.43 463.189 1343.43 Q459.207 1343.43 456.846 1341.2 Q454.508 1338.96 454.508 1335.21 Q454.508 1330.83 457.425 1328.61 Q460.365 1326.39 466.175 1326.39 L472.147 1326.39 L472.147 1325.97 Q472.147 1323.03 470.202 1321.44 Q468.281 1319.82 464.786 1319.82 Q462.564 1319.82 460.457 1320.35 Q458.351 1320.88 456.406 1321.95 L456.406 1318.01 Q458.744 1317.11 460.943 1316.67 Q463.142 1316.2 465.226 1316.2 Q470.851 1316.2 473.628 1319.12 Q476.406 1322.04 476.406 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M485.179 1306.74 L489.438 1306.74 L489.438 1342.76 L485.179 1342.76 L485.179 1306.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M518.049 1350.63 L518.049 1353.94 L493.42 1353.94 L493.42 1350.63 L518.049 1350.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M533.836 1329.72 Q528.674 1329.72 526.684 1330.9 Q524.693 1332.08 524.693 1334.93 Q524.693 1337.2 526.174 1338.54 Q527.679 1339.86 530.248 1339.86 Q533.79 1339.86 535.92 1337.36 Q538.072 1334.84 538.072 1330.67 L538.072 1329.72 L533.836 1329.72 M542.332 1327.96 L542.332 1342.76 L538.072 1342.76 L538.072 1338.82 Q536.614 1341.18 534.438 1342.32 Q532.262 1343.43 529.114 1343.43 Q525.133 1343.43 522.772 1341.2 Q520.434 1338.96 520.434 1335.21 Q520.434 1330.83 523.35 1328.61 Q526.29 1326.39 532.1 1326.39 L538.072 1326.39 L538.072 1325.97 Q538.072 1323.03 536.128 1321.44 Q534.207 1319.82 530.711 1319.82 Q528.489 1319.82 526.383 1320.35 Q524.276 1320.88 522.332 1321.95 L522.332 1318.01 Q524.67 1317.11 526.869 1316.67 Q529.068 1316.2 531.151 1316.2 Q536.776 1316.2 539.554 1319.12 Q542.332 1322.04 542.332 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M569.762 1317.82 L569.762 1321.81 Q567.957 1320.81 566.128 1320.32 Q564.322 1319.82 562.47 1319.82 Q558.327 1319.82 556.035 1322.45 Q553.744 1325.07 553.744 1329.82 Q553.744 1334.56 556.035 1337.2 Q558.327 1339.82 562.47 1339.82 Q564.322 1339.82 566.128 1339.33 Q567.957 1338.82 569.762 1337.82 L569.762 1341.76 Q567.98 1342.59 566.058 1343.01 Q564.16 1343.43 562.008 1343.43 Q556.151 1343.43 552.702 1339.75 Q549.253 1336.07 549.253 1329.82 Q549.253 1323.47 552.725 1319.84 Q556.221 1316.2 562.285 1316.2 Q564.253 1316.2 566.128 1316.62 Q568.003 1317.01 569.762 1317.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip230)" d="M595.827 1317.82 L595.827 1321.81 Q594.021 1320.81 592.193 1320.32 Q590.387 1319.82 588.535 1319.82 Q584.392 1319.82 582.1 1322.45 Q579.808 1325.07 579.808 1329.82 Q579.808 1334.56 582.1 1337.2 Q584.392 1339.82 588.535 1339.82 Q590.387 1339.82 592.193 1339.33 Q594.021 1338.82 595.827 1337.82 L595.827 1341.76 Q594.044 1342.59 592.123 1343.01 Q590.225 1343.43 588.072 1343.43 Q582.216 1343.43 578.767 1339.75 Q575.318 1336.07 575.318 1329.82 Q575.318 1323.47 578.79 1319.84 Q582.285 1316.2 588.35 1316.2 Q590.318 1316.2 592.193 1316.62 Q594.068 1317.01 595.827 1317.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><h2 id="Summary-and-Discussion"><a class="docs-heading-anchor" href="#Summary-and-Discussion">Summary and Discussion</a><a id="Summary-and-Discussion-1"></a><a class="docs-heading-anchor-permalink" href="#Summary-and-Discussion" title="Permalink"></a></h2><p>The main components that comprise DenseNet are dense blocks and transition layers. For the latter, we need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again. In terms of cross-layer connections, in contrast to ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension. Although these concatenation operations reuse features to achieve computational efficiency, unfortunately they lead to heavy GPU memory consumption. As a result, applying DenseNet may require more memory-efficient implementations that may increase training time :cite:<code>pleiss2017memory</code>.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>Why do we use average pooling rather than max-pooling in the transition layer?</li><li>One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?</li><li>One problem for which DenseNet has been criticized is its high memory consumption.<ol><li>Is this really the case? Try to change the input shape to <span>$224\times 224$</span> to compare the actual GPU memory consumption empirically.</li><li>Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?</li></ol></li><li>Implement the various DenseNet versions presented in Table 1 of the DenseNet paper :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>.</li><li>Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in :numref:<code>sec_kaggle_house</code>.</li></ol><h3 id="1-."><a class="docs-heading-anchor" href="#1-.">1 .</a><a id="1-.-1"></a><a class="docs-heading-anchor-permalink" href="#1-." title="Permalink"></a></h3><p>Since the goal is to take outputs from previous layers upto the next layer, we use mean pooling. It doesnot discard the rest of the activations in the convolutional layer, and instead takes the mean.</p><h3 id="2"><a class="docs-heading-anchor" href="#2">2</a><a id="2-1"></a><a class="docs-heading-anchor-permalink" href="#2" title="Permalink"></a></h3><p>Due to the transition layers, we effectively manage the model complexity and by extension the number of parameters.</p><h3 id="3."><a class="docs-heading-anchor" href="#3.">3.</a><a id="3.-1"></a><a class="docs-heading-anchor-permalink" href="#3." title="Permalink"></a></h3><p>A. 96x96 takes 2169MiB of GPU memoy. 224 / 96 = 2.33. The GPU memory is affected by the order of N^3. Therefore it will be 8 times more B. Sparse Connectivity: Instead of connecting all the layers to all the layers, randomly pick some layers and connect them.</p><h3 id="4."><a class="docs-heading-anchor" href="#4.">4.</a><a id="4.-1"></a><a class="docs-heading-anchor-permalink" href="#4." title="Permalink"></a></h3><pre><code class="language-julia hljs">densenet121 = DenseNet(; arch = (6, 12, 24, 16))
densenet169 = DenseNet(; arch = (6, 12, 32, 32))
densenet201 = DenseNet(; arch = (6, 12, 48, 32))
densenet264 = DenseNet(; arch = (6, 12, 64, 48))</code></pre><pre><code class="nohighlight hljs">DenseNet(
  Chain(
    DenseNetB1(
      Chain(
        Conv((7, 7), 1 =&gt; 64, pad=3, stride=2),  # 3_200 parameters
        BatchNorm(64, relu),            # 128 parameters, plus 128
        MaxPool((3, 3), pad=1, stride=2),
      ),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 64 =&gt; 32, pad=1),  # 18_464 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 96 =&gt; 32, pad=1),  # 27_680 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 192 =&gt; 32, pad=1),  # 55_328 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 224 =&gt; 32, pad=1),  # 64_544 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    Chain(
      BatchNorm(256, relu),             # 512 parameters, plus 512
      Conv((1, 1), 256 =&gt; 128),         # 32_896 parameters
      MeanPool((2, 2), pad=1),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 128 =&gt; 32, pad=1),  # 36_896 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 160 =&gt; 32, pad=1),  # 46_112 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 192 =&gt; 32, pad=1),  # 55_328 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 224 =&gt; 32, pad=1),  # 64_544 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 256 =&gt; 32, pad=1),  # 73_760 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 288 =&gt; 32, pad=1),  # 82_976 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 320 =&gt; 32, pad=1),  # 92_192 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 352 =&gt; 32, pad=1),  # 101_408 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 384 =&gt; 32, pad=1),  # 110_624 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 416 =&gt; 32, pad=1),  # 119_840 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 448 =&gt; 32, pad=1),  # 129_056 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 480 =&gt; 32, pad=1),  # 138_272 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    Chain(
      BatchNorm(512, relu),             # 1_024 parameters, plus 1_024
      Conv((1, 1), 512 =&gt; 256),         # 131_328 parameters
      MeanPool((2, 2), pad=1),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 256 =&gt; 32, pad=1),  # 73_760 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 288 =&gt; 32, pad=1),  # 82_976 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 320 =&gt; 32, pad=1),  # 92_192 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 352 =&gt; 32, pad=1),  # 101_408 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 384 =&gt; 32, pad=1),  # 110_624 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 416 =&gt; 32, pad=1),  # 119_840 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 448 =&gt; 32, pad=1),  # 129_056 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 480 =&gt; 32, pad=1),  # 138_272 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 512 =&gt; 32, pad=1),  # 147_488 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 544 =&gt; 32, pad=1),  # 156_704 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 576 =&gt; 32, pad=1),  # 165_920 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 608 =&gt; 32, pad=1),  # 175_136 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 640 =&gt; 32, pad=1),  # 184_352 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 672 =&gt; 32, pad=1),  # 193_568 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 704 =&gt; 32, pad=1),  # 202_784 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 736 =&gt; 32, pad=1),  # 212_000 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 768 =&gt; 32, pad=1),  # 221_216 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 800 =&gt; 32, pad=1),  # 230_432 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 832 =&gt; 32, pad=1),  # 239_648 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 864 =&gt; 32, pad=1),  # 248_864 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 896 =&gt; 32, pad=1),  # 258_080 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 928 =&gt; 32, pad=1),  # 267_296 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 960 =&gt; 32, pad=1),  # 276_512 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 992 =&gt; 32, pad=1),  # 285_728 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1024 =&gt; 32, pad=1),  # 294_944 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1056 =&gt; 32, pad=1),  # 304_160 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1088 =&gt; 32, pad=1),  # 313_376 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1120 =&gt; 32, pad=1),  # 322_592 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1152 =&gt; 32, pad=1),  # 331_808 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1184 =&gt; 32, pad=1),  # 341_024 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1216 =&gt; 32, pad=1),  # 350_240 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1248 =&gt; 32, pad=1),  # 359_456 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1280 =&gt; 32, pad=1),  # 368_672 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1312 =&gt; 32, pad=1),  # 377_888 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1344 =&gt; 32, pad=1),  # 387_104 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1376 =&gt; 32, pad=1),  # 396_320 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1408 =&gt; 32, pad=1),  # 405_536 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1440 =&gt; 32, pad=1),  # 414_752 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1472 =&gt; 32, pad=1),  # 423_968 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1504 =&gt; 32, pad=1),  # 433_184 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1536 =&gt; 32, pad=1),  # 442_400 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1568 =&gt; 32, pad=1),  # 451_616 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1600 =&gt; 32, pad=1),  # 460_832 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1632 =&gt; 32, pad=1),  # 470_048 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1664 =&gt; 32, pad=1),  # 479_264 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1696 =&gt; 32, pad=1),  # 488_480 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1728 =&gt; 32, pad=1),  # 497_696 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1760 =&gt; 32, pad=1),  # 506_912 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1792 =&gt; 32, pad=1),  # 516_128 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1824 =&gt; 32, pad=1),  # 525_344 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1856 =&gt; 32, pad=1),  # 534_560 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1888 =&gt; 32, pad=1),  # 543_776 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1920 =&gt; 32, pad=1),  # 552_992 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1952 =&gt; 32, pad=1),  # 562_208 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1984 =&gt; 32, pad=1),  # 571_424 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2016 =&gt; 32, pad=1),  # 580_640 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2048 =&gt; 32, pad=1),  # 589_856 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2080 =&gt; 32, pad=1),  # 599_072 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2112 =&gt; 32, pad=1),  # 608_288 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2144 =&gt; 32, pad=1),  # 617_504 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2176 =&gt; 32, pad=1),  # 626_720 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2208 =&gt; 32, pad=1),  # 635_936 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2240 =&gt; 32, pad=1),  # 645_152 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2272 =&gt; 32, pad=1),  # 654_368 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    Chain(
      BatchNorm(2304, relu),            # 4_608 parameters, plus 4_608
      Conv((1, 1), 2304 =&gt; 1152),       # 2_655_360 parameters
      MeanPool((2, 2), pad=1),
    ),
    DenseBlock(
      [
        Chain(
          Conv((3, 3), 1152 =&gt; 32, pad=1),  # 331_808 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1184 =&gt; 32, pad=1),  # 341_024 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1216 =&gt; 32, pad=1),  # 350_240 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1248 =&gt; 32, pad=1),  # 359_456 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1280 =&gt; 32, pad=1),  # 368_672 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1312 =&gt; 32, pad=1),  # 377_888 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1344 =&gt; 32, pad=1),  # 387_104 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1376 =&gt; 32, pad=1),  # 396_320 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1408 =&gt; 32, pad=1),  # 405_536 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1440 =&gt; 32, pad=1),  # 414_752 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1472 =&gt; 32, pad=1),  # 423_968 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1504 =&gt; 32, pad=1),  # 433_184 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1536 =&gt; 32, pad=1),  # 442_400 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1568 =&gt; 32, pad=1),  # 451_616 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1600 =&gt; 32, pad=1),  # 460_832 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1632 =&gt; 32, pad=1),  # 470_048 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1664 =&gt; 32, pad=1),  # 479_264 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1696 =&gt; 32, pad=1),  # 488_480 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1728 =&gt; 32, pad=1),  # 497_696 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1760 =&gt; 32, pad=1),  # 506_912 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1792 =&gt; 32, pad=1),  # 516_128 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1824 =&gt; 32, pad=1),  # 525_344 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1856 =&gt; 32, pad=1),  # 534_560 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1888 =&gt; 32, pad=1),  # 543_776 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1920 =&gt; 32, pad=1),  # 552_992 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1952 =&gt; 32, pad=1),  # 562_208 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 1984 =&gt; 32, pad=1),  # 571_424 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2016 =&gt; 32, pad=1),  # 580_640 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2048 =&gt; 32, pad=1),  # 589_856 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2080 =&gt; 32, pad=1),  # 599_072 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2112 =&gt; 32, pad=1),  # 608_288 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2144 =&gt; 32, pad=1),  # 617_504 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2176 =&gt; 32, pad=1),  # 626_720 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2208 =&gt; 32, pad=1),  # 635_936 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2240 =&gt; 32, pad=1),  # 645_152 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2272 =&gt; 32, pad=1),  # 654_368 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2304 =&gt; 32, pad=1),  # 663_584 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2336 =&gt; 32, pad=1),  # 672_800 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2368 =&gt; 32, pad=1),  # 682_016 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2400 =&gt; 32, pad=1),  # 691_232 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2432 =&gt; 32, pad=1),  # 700_448 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2464 =&gt; 32, pad=1),  # 709_664 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2496 =&gt; 32, pad=1),  # 718_880 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2528 =&gt; 32, pad=1),  # 728_096 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2560 =&gt; 32, pad=1),  # 737_312 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2592 =&gt; 32, pad=1),  # 746_528 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2624 =&gt; 32, pad=1),  # 755_744 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
        Chain(
          Conv((3, 3), 2656 =&gt; 32, pad=1),  # 764_960 parameters
          BatchNorm(32, relu),          # 64 parameters, plus 64
        ),
      ],
    ),
    GlobalMeanPool(),
    Flux.flatten,
    Dense(2688 =&gt; 10),                  # 26_890 parameters
    NNlib.softmax,
  ),
)         # Total: 538 trainable arrays, 53_786_826 parameters,
          # plus 268 non-trainable, 14_592 parameters, summarysize 205.290 MiB.</code></pre><h3 id="5."><a class="docs-heading-anchor" href="#5.">5.</a><a id="5.-1"></a><a class="docs-heading-anchor-permalink" href="#5." title="Permalink"></a></h3><pre><code class="language-julia hljs">## 5.
struct DenseNetMLPBlock{N} &lt;: AbstractModel 
    net::N 
end 

function DenseNetMLPBlock(features_in, num_features, num_dense, return_output_features = false)
    prev_features = features_in
    blocks = []
    for i in 1:num_dense
        push!(blocks, Dense(prev_features, num_features))
        prev_features += num_features
        push!(blocks, Dropout(0.4),
    end
    block = DenseNetMLPBlock(blocks)
    if return_output_features 
        return block, prev_features 
    else
        return block
    end
            
end

function (d::DenseNetMLPBlock)(x)
    for block in d.net
        y = block(x)
        x = vcat(x,y)
    end
end

struct DenseNetMLP{N} &lt;: AbstractModel 
    net::N
end
</code></pre><pre><code class="language-julia hljs">function DenseNetMLP(num_features; num_classes = 10, arch = (4,4,4,4))
    prev_features = 64
    layers = map(arch) do num_dense 
        block, prev_features = DenseNetMLPBlock(prev_features, 64, num_dense; return_output_featuers = true)
        return block
    end
    Chain(
        Dense(num_features =&gt; 64, relu),
        Dropout(0.2),
        layers...,
        Dense(_ =&gt; num_classes),
        softmax
    )
end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MCNN_6/">« Residual Networks (ResNet) and ResNeXt</a><a class="docs-footer-nextpage" href="../MCNN_8/">Designing Convolution Network Architectures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
