<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Residual Networks (ResNet) and ResNeXt · d2l Julia</title><meta name="title" content="Residual Networks (ResNet) and ResNeXt · d2l Julia"/><meta property="og:title" content="Residual Networks (ResNet) and ResNeXt · d2l Julia"/><meta property="twitter:title" content="Residual Networks (ResNet) and ResNeXt · d2l Julia"/><meta name="description" content="Documentation for d2l Julia."/><meta property="og:description" content="Documentation for d2l Julia."/><meta property="twitter:description" content="Documentation for d2l Julia."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../CH3.Linear_Regression/LNN_1/">d2l Julia</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Linear Neural Networks for Regression</span><ul><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_1/">Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_2/">Multiple Dispatch Design for Implementation</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_3/">Synthetic Regression Data</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_4/">Linear Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_5/">Concise Implementation of Linear Regression</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_6/">Generalization</a></li><li><a class="tocitem" href="../../CH3.Linear_Regression/LNN_7/">Weight Decay</a></li></ul></li><li><span class="tocitem">Linear Neural Networks for Classification</span><ul><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_1/">Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_2/">The Image Classification Dataset</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_3/">Softmax Regression Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_4/">Concise Implementation of Softmax Regression</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_5/">Generalization in Classification</a></li><li><a class="tocitem" href="../../CH4.Linear_Classification/LCN_6/">Environment and Distribution Shift</a></li></ul></li><li><span class="tocitem">Multilayer Perceptron</span><ul><li><a class="tocitem" href="../../CH5.MLP/MLP_1/">Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_2/">Implementation of Multilayer Perceptrons</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_3/">Forward Propagation, Backward Propagation, and Computational Graphs</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_4/">Numerical Stability and Initialization</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_5/">Generalization in Deep Learning</a></li><li><a class="tocitem" href="../../CH5.MLP/MLP_6/">Dropout</a></li></ul></li><li><span class="tocitem">Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_2/">Convolutions for Images</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_3/">Padding and Stride</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_4/">Multiple Input and Multiple Output Channels</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_5/">Pooling</a></li><li><a class="tocitem" href="../../CH6.Convolutional_Neural_Networks/CNN_6/">Convolutional Neural Networks (LeNet)</a></li></ul></li><li><span class="tocitem">Modern Convolutional Neural Networks</span><ul><li><a class="tocitem" href="../MCNN_0/">Modern Convolutional Neural Networks</a></li><li><a class="tocitem" href="../MCNN_1/">Deep Convolutional Neural Networks (AlexNet)</a></li><li><a class="tocitem" href="../MCNN_2/">Networks Using Blocks (VGG)</a></li><li><a class="tocitem" href="../MCNN_3/">-</a></li><li><a class="tocitem" href="../MCNN_4/">Multi-Branch Networks  (GoogLeNet)</a></li><li><a class="tocitem" href="../MCNN_5/">-</a></li><li class="is-active"><a class="tocitem" href>Residual Networks (ResNet) and ResNeXt</a><ul class="internal"><li><a class="tocitem" href="#Function-Classes"><span>Function Classes</span></a></li><li><a class="tocitem" href="#**Residual-Blocks**"><span><strong>Residual Blocks</strong></span></a></li><li><a class="tocitem" href="#ResNet-Model"><span>ResNet Model</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#ResNeXt"><span>ResNeXt</span></a></li><li><a class="tocitem" href="#Summary-and-Discussion"><span>Summary and Discussion</span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../MCNN_7/">Densely Connected Networks (DenseNet)</a></li><li><a class="tocitem" href="../MCNN_8/">Designing Convolution Network Architectures</a></li></ul></li><li><span class="tocitem">Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_0/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_1/">Working with Sequences</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_2/">Converting Raw Text into Sequence Data</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_3/">Language Models</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_4/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_5/">Recurrent Neural Network Implementation from Scratch</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_6/">Concise Implementation of Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH8.Recurrent_Neural_Networks/RNN_7/">Backpropagation Through Time</a></li></ul></li><li><span class="tocitem">Modern Recurrent Neural Networks</span><ul><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN7/">Sequence-to-Sequence Learning for Machine Translation</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_1/">Long Short-Term Memory (LSTM)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_2/">Gated Recurrent Units (GRU)</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_3/">-</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_4/">Bidirectional Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_5/">Machine Translation and the Dataset</a></li><li><a class="tocitem" href="../../CH9.Modern_Recurrent_Neural_Networks/MRNN_6/">The Encoder–Decoder Architecture</a></li></ul></li><li><span class="tocitem">Attention Mechanisms and Transformers</span><ul><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_1/">Queries, Keys, and Values</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_2/">Attention Pooling by Similarity</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_3/">Attention Scoring Functions</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_4/">The Bahdanau Attention Mechanism</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_5/">Multi-Head Attention</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/ATTN_6/">Self-Attention and Positional Encoding</a></li><li><a class="tocitem" href="../../CH10.Attention_Mechanisms_and_Transformers/Untitled/">-</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Modern Convolutional Neural Networks</a></li><li class="is-active"><a href>Residual Networks (ResNet) and ResNeXt</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Residual Networks (ResNet) and ResNeXt</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Residual-Networks-(ResNet)-and-ResNeXt"><a class="docs-heading-anchor" href="#Residual-Networks-(ResNet)-and-ResNeXt">Residual Networks (ResNet) and ResNeXt</a><a id="Residual-Networks-(ResNet)-and-ResNeXt-1"></a><a class="docs-heading-anchor-permalink" href="#Residual-Networks-(ResNet)-and-ResNeXt" title="Permalink"></a></h1><p>As we design ever deeper networks it becomes imperative to understand how adding layers can increase the complexity and expressiveness of the network. Even more important is the ability to design networks where adding layers makes networks strictly more expressive rather than just different. To make some progress we need a bit of mathematics.</p><pre><code class="language-julia hljs">using Pkg; Pkg.activate(&quot;../../d2lai&quot;)
using d2lai
using Flux 
using CUDA, cuDNN
using Statistics, Flux.Zygote</code></pre><pre><code class="nohighlight hljs">  Activating project at `/workspace/d2l-julia/d2lai`</code></pre><h2 id="Function-Classes"><a class="docs-heading-anchor" href="#Function-Classes">Function Classes</a><a id="Function-Classes-1"></a><a class="docs-heading-anchor-permalink" href="#Function-Classes" title="Permalink"></a></h2><p>Consider <span>$\mathcal{F}$</span>, the class of functions that a specific network architecture (together with learning rates and other hyperparameter settings) can reach. That is, for all <span>$f \in \mathcal{F}$</span> there exists some set of parameters (e.g., weights and biases) that can be obtained through training on a suitable dataset. Let&#39;s assume that <span>$f^*$</span> is the &quot;truth&quot; function that we really would like to find. If it is in <span>$\mathcal{F}$</span>, we are in good shape but typically we will not be quite so lucky. Instead, we will try to find some <span>$f^*_\mathcal{F}$</span> which is our best bet within <span>$\mathcal{F}$</span>. For instance, given a dataset with features <span>$\mathbf{X}$</span> and labels <span>$\mathbf{y}$</span>, we might try finding it by solving the following optimization problem:</p><p class="math-container">\[f^*_\mathcal{F} \stackrel{\textrm{def}}{=} \mathop{\mathrm{argmin}}_f L(\mathbf{X}, \mathbf{y}, f) \textrm{ subject to } f \in \mathcal{F}.\]</p><p>We know that regularization :cite:<code>tikhonov1977solutions,morozov2012methods</code> may control complexity of <span>$\mathcal{F}$</span> and achieve consistency, so a larger size of training data generally leads to better <span>$f^*_\mathcal{F}$</span>. It is only reasonable to assume that if we design a different and more powerful architecture <span>$\mathcal{F}&#39;$</span> we should arrive at a better outcome. In other words, we would expect that <span>$f^*_{\mathcal{F}&#39;}$</span> is &quot;better&quot; than <span>$f^*_{\mathcal{F}}$</span>. However, if <span>$\mathcal{F} \not\subseteq \mathcal{F}&#39;$</span> there is no guarantee that this should even happen. In fact, <span>$f^*_{\mathcal{F}&#39;}$</span> might well be worse. As illustrated by :numref:<code>fig_functionclasses</code>, for non-nested function classes, a larger function class does not always move closer to the &quot;truth&quot; function <span>$f^*$</span>. For instance, on the left of :numref:<code>fig_functionclasses</code>, though <span>$\mathcal{F}_3$</span> is closer to <span>$f^*$</span> than <span>$\mathcal{F}_1$</span>, <span>$\mathcal{F}_6$</span> moves away and there is no guarantee that further increasing the complexity can reduce the distance from <span>$f^*$</span>. With nested function classes where <span>$\mathcal{F}_1 \subseteq \cdots \subseteq \mathcal{F}_6$</span> on the right of :numref:<code>fig_functionclasses</code>, we can avoid the aforementioned issue from the non-nested function classes.</p><p><img src="../../img/functionclasses.svg" alt="For non-nested function classes, a larger (indicated by area) function class does not guarantee we will get closer to the &quot;truth&quot; function (\$\\mathit{f}^*\$). This does not happen in nested function classes."/> :label:<code>fig_functionclasses</code></p><p>Thus, only if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network. For deep neural networks, if we can train the newly-added layer into an identity function <span>$f(\mathbf{x}) = \mathbf{x}$</span>, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors.</p><p>This is the question that :citet:<code>He.Zhang.Ren.ea.2016</code> considered when working on very deep computer vision models. At the heart of their proposed <em>residual network</em> (<em>ResNet</em>) is the idea that every additional layer should more easily contain the identity function as one of its elements. These considerations are rather profound but they led to a surprisingly simple solution, a <em>residual block</em>. With it, ResNet won the ImageNet Large Scale Visual Recognition Challenge in 2015. The design had a profound influence on how to build deep neural networks. For instance, residual blocks have been added to recurrent networks :cite:<code>prakash2016neural,kim2017residual</code>. Likewise, Transformers :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code> use them to stack many layers of networks efficiently. It is also used in graph neural networks :cite:<code>Kipf.Welling.2016</code> and, as a basic concept, it has been used extensively in computer vision :cite:<code>Redmon.Farhadi.2018,Ren.He.Girshick.ea.2015</code>.  Note that residual networks are predated by highway networks :cite:<code>srivastava2015highway</code> that share some of the motivation, albeit without the elegant parametrization around the identity function.</p><h2 id="**Residual-Blocks**"><a class="docs-heading-anchor" href="#**Residual-Blocks**"><strong>Residual Blocks</strong></a><a id="**Residual-Blocks**-1"></a><a class="docs-heading-anchor-permalink" href="#**Residual-Blocks**" title="Permalink"></a></h2><p>:label:<code>subsec_residual-blks</code></p><p>Let&#39;s focus on a local part of a neural network, as depicted in :numref:<code>fig_residual_block</code>. Denote the input by <span>$\mathbf{x}$</span>. We assume that <span>$f(\mathbf{x})$</span>, the desired underlying mapping we want to obtain by learning, is to be used as input to the activation function on the top. On the left, the portion within the dotted-line box must directly learn <span>$f(\mathbf{x})$</span>. On the right, the portion within the dotted-line box needs to learn the <em>residual mapping</em> <span>$g(\mathbf{x}) = f(\mathbf{x}) - \mathbf{x}$</span>, which is how the residual block derives its name. If the identity mapping <span>$f(\mathbf{x}) = \mathbf{x}$</span> is the desired underlying mapping, the residual mapping amounts to <span>$g(\mathbf{x}) = 0$</span> and it is thus easier to learn: we only need to push the weights and biases of the upper weight layer (e.g., fully connected layer and convolutional layer) within the dotted-line box to zero. The right figure illustrates the <em>residual block</em> of ResNet, where the solid line carrying the layer input <span>$\mathbf{x}$</span> to the addition operator is called a <em>residual connection</em> (or <em>shortcut connection</em>). With residual blocks, inputs can forward propagate faster through the residual connections across layers. In fact, the residual block can be thought of as a special case of the multi-branch Inception block: it has two branches one of which is the identity mapping.</p><p><img src="../../img/residual-block.svg" alt="In a regular block (left), the portion within the dotted-line box must directly learn the mapping \$\\mathit{f}(\\mathbf{x})\$. In a residual block (right), the portion within the dotted-line box needs to learn the residual mapping \$\\mathit{g}(\\mathbf{x}) = \\mathit{f}(\\mathbf{x}) - \\mathbf{x}\$, making the identity mapping \$\\mathit{f}(\\mathbf{x}) = \\mathbf{x}\$ easier to learn."/> :label:<code>fig_residual_block</code></p><p>ResNet has VGG&#39;s full <span>$3\times 3$</span> convolutional layer design. The residual block has two <span>$3\times 3$</span> convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional <span>$1\times 1$</span> convolutional layer to transform the input into the desired shape for the addition operation. Let&#39;s have a look at the code below.</p><pre><code class="language-julia hljs">struct Residual{N} &lt;: AbstractModel
    net::N
end 

function Residual(channels_in::Int; num_channels::Int = channels_in, use_1x1conv = !isequal(channels_in, num_channels), stride = 1)
    conv_chain = Chain(
            Conv((3,3) , channels_in=&gt;num_channels, pad = 1, stride = stride),
            BatchNorm(num_channels, relu),
            Conv((3,3) , num_channels=&gt;num_channels, pad = 1),
            BatchNorm(num_channels),
        )
    
    net = use_1x1conv ? Parallel(+, conv_chain, Conv((1,1), channels_in=&gt;num_channels, stride = stride)) : Parallel(+, conv_chain, Flux.identity)
    Residual(net)
end

(r::Residual)(x) = relu.(r.net(x))</code></pre><p>This code generates two types of networks: one where we add the input to the output before applying the ReLU nonlinearity whenever <code>use_1x1conv=False</code>; and one where we adjust channels and resolution by means of a <span>$1 \times 1$</span> convolution before adding. :numref:<code>fig_resnet_block</code> illustrates this.</p><p><img src="../../img/resnet-block.svg" alt="ResNet block with and without \$1 \\times 1\$ convolution, which transforms the input into the desired shape for the addition operation."/> :label:<code>fig_resnet_block</code></p><p>Now let&#39;s look at [<strong>a situation where the input and output are of the same shape</strong>], where <span>$1 \times 1$</span> convolution is not needed.</p><pre><code class="language-julia hljs">r = Residual(3)
r(rand(4, 5, 3, 32)) |&gt; size</code></pre><pre><code class="nohighlight hljs">┌ Warning: Layer with Float32 parameters got Float64 input.
│   The input will be converted, but any earlier layers may be very slow.
│   layer = Conv((3, 3), 3 =&gt; 3, pad=1)  # 84 parameters
│   summary(x) = &quot;4×5×3×32 Array{Float64, 4}&quot;
└ @ Flux ~/.julia/packages/Flux/3711C/src/layers/stateless.jl:60





(4, 5, 3, 32)</code></pre><p>We also have the option to halve the output height and width while increasing the number of output channels. In this case we use <span>$1 \times 1$</span> convolutions via <code>use_1x1conv=True</code>. This comes in handy at the beginning of each ResNet block to reduce the spatial dimensionality via <code>strides=2</code>.</p><pre><code class="language-julia hljs">r = Residual(3; num_channels = 6)
r(rand(4, 5, 3, 32)) |&gt; size</code></pre><pre><code class="nohighlight hljs">(4, 5, 6, 32)</code></pre><h2 id="ResNet-Model"><a class="docs-heading-anchor" href="#ResNet-Model">ResNet Model</a><a id="ResNet-Model-1"></a><a class="docs-heading-anchor-permalink" href="#ResNet-Model" title="Permalink"></a></h2><p>The first two layers of ResNet are the same as those of the GoogLeNet we described before: the <span>$7\times 7$</span> convolutional layer with 64 output channels and a stride of 2 is followed by the <span>$3\times 3$</span> max-pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.</p><pre><code class="language-julia hljs">struct ResNetB1{N} &lt;: AbstractModel
    net::N
end

function ResNetB1()
    net = Chain(
        Conv((7,7), 1 =&gt; 64, pad = 3 , stride = 2),
        BatchNorm(64, relu),
        MaxPool((3,3), pad = 1, stride = 2)
    )
    ResNetB1(net)
end 
(r::ResNetB1)(x) = r.net(x)
Flux.@layer ResNetB1</code></pre><p>GoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a max-pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.</p><pre><code class="language-julia hljs">struct ResNetBlock{N} &lt;: AbstractModel 
    net::N 
end

function ResNetBlock(channel_in, num_residuals, num_channels; first_block = false)
    block = if first_block
        blocks = map(1:num_residuals) do i
            Residual(channel_in)
        end |&gt; Chain
    else
        blocks = map(1:num_residuals) do i
            if i == 1
                return Residual(channel_in; num_channels, stride = 2)
            else
                return Residual(num_channels)
            end
        end |&gt; Chain
    end 
    ResNetBlock(block)
end
Flux.@layer ResNetBlock
(r::ResNetBlock)(x) = r.net(x)</code></pre><p>Then, we add all the modules to ResNet. Here, two residual blocks are used for each module. Lastly, just like GoogLeNet, we add a global average pooling layer, followed by the fully connected layer output.</p><pre><code class="language-julia hljs">struct ResNet{N} &lt;: AbstractClassifier 
    net::N
end
Flux.@layer ResNet 

function ResNet(arch::Tuple, num_classes::Int = 10)
    channel_ins = last.(arch[1:end-1]) 
    net = Flux.@autosize (96, 96, 1, 1) Chain(
        ResNetB1(),
        ResNetBlock(64, arch[1]..., first_block = true),
        map(arch[2:end], channel_ins) do (num_residuals, num_channels), channel_in 
            ResNetBlock(channel_in, num_residuals, num_channels)
        end |&gt; Chain, 
        GlobalMeanPool(),
        Flux.flatten,
        Dense(_ =&gt; num_classes),
        softmax
        
    )
    ResNet(net)
end
(r::ResNet)(x) = r.net(x)</code></pre><p>There are four convolutional layers in each module (excluding the <span>$1\times 1$</span> convolutional layer). Together with the first <span>$7\times 7$</span> convolutional layer and the final fully connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture of ResNet is similar to that of GoogLeNet, ResNet&#39;s structure is simpler and easier to modify. All these factors have resulted in the rapid and widespread use of ResNet. :numref:<code>fig_resnet18</code> depicts the full ResNet-18.</p><p><img src="../../img/resnet18-90.svg" alt="The ResNet-18 architecture."/> :label:<code>fig_resnet18</code></p><p>Before training ResNet, let&#39;s [<strong>observe how the input shape changes across different modules in ResNet</strong>]. As in all the previous architectures, the resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features.</p><pre><code class="language-julia hljs">arch = ((2, 64), (2, 128), (2, 256), (2, 512))
model = ResNet(arch)</code></pre><pre><code class="nohighlight hljs">ResNet(
  Chain(
    ResNetB1(
      Chain(
        Conv((7, 7), 1 =&gt; 64, pad=3, stride=2),  # 3_200 parameters
        BatchNorm(64, relu),            # 128 parameters, plus 128
        MaxPool((3, 3), pad=1, stride=2),
      ),
    ),
    ResNetBlock(
      Chain(
        [
          Residual(
            Parallel(
              +,
              Chain(
                Conv((3, 3), 64 =&gt; 64, pad=1),  # 36_928 parameters
                BatchNorm(64, relu),    # 128 parameters, plus 128
                Conv((3, 3), 64 =&gt; 64, pad=1),  # 36_928 parameters
                BatchNorm(64),          # 128 parameters, plus 128
              ),
              identity,
            ),
          ),
          Residual(
            Parallel(
              +,
              Chain(
                Conv((3, 3), 64 =&gt; 64, pad=1),  # 36_928 parameters
                BatchNorm(64, relu),    # 128 parameters, plus 128
                Conv((3, 3), 64 =&gt; 64, pad=1),  # 36_928 parameters
                BatchNorm(64),          # 128 parameters, plus 128
              ),
              identity,
            ),
          ),
        ],
      ),
    ),
    Chain(
      ResNetBlock(
        Chain(
          [
            Residual(
              Parallel(
                +,
                Chain(
                  Conv((3, 3), 64 =&gt; 128, pad=1, stride=2),  # 73_856 parameters
                  BatchNorm(128, relu),  # 256 parameters, plus 256
                  Conv((3, 3), 128 =&gt; 128, pad=1),  # 147_584 parameters
                  BatchNorm(128),       # 256 parameters, plus 256
                ),
                Conv((1, 1), 64 =&gt; 128, stride=2),  # 8_320 parameters
              ),
            ),
            Residual(
              Parallel(
                +,
                Chain(
                  Conv((3, 3), 128 =&gt; 128, pad=1),  # 147_584 parameters
                  BatchNorm(128, relu),  # 256 parameters, plus 256
                  Conv((3, 3), 128 =&gt; 128, pad=1),  # 147_584 parameters
                  BatchNorm(128),       # 256 parameters, plus 256
                ),
                identity,
              ),
            ),
          ],
        ),
      ),
      ResNetBlock(
        Chain(
          [
            Residual(
              Parallel(
                +,
                Chain(
                  Conv((3, 3), 128 =&gt; 256, pad=1, stride=2),  # 295_168 parameters
                  BatchNorm(256, relu),  # 512 parameters, plus 512
                  Conv((3, 3), 256 =&gt; 256, pad=1),  # 590_080 parameters
                  BatchNorm(256),       # 512 parameters, plus 512
                ),
                Conv((1, 1), 128 =&gt; 256, stride=2),  # 33_024 parameters
              ),
            ),
            Residual(
              Parallel(
                +,
                Chain(
                  Conv((3, 3), 256 =&gt; 256, pad=1),  # 590_080 parameters
                  BatchNorm(256, relu),  # 512 parameters, plus 512
                  Conv((3, 3), 256 =&gt; 256, pad=1),  # 590_080 parameters
                  BatchNorm(256),       # 512 parameters, plus 512
                ),
                identity,
              ),
            ),
          ],
        ),
      ),
      ResNetBlock(
        Chain(
          [
            Residual(
              Parallel(
                +,
                Chain(
                  Conv((3, 3), 256 =&gt; 512, pad=1, stride=2),  # 1_180_160 parameters
                  BatchNorm(512, relu),  # 1_024 parameters, plus 1_024
                  Conv((3, 3), 512 =&gt; 512, pad=1),  # 2_359_808 parameters
                  BatchNorm(512),       # 1_024 parameters, plus 1_024
                ),
                Conv((1, 1), 256 =&gt; 512, stride=2),  # 131_584 parameters
              ),
            ),
            Residual(
              Parallel(
                +,
                Chain(
                  Conv((3, 3), 512 =&gt; 512, pad=1),  # 2_359_808 parameters
                  BatchNorm(512, relu),  # 1_024 parameters, plus 1_024
                  Conv((3, 3), 512 =&gt; 512, pad=1),  # 2_359_808 parameters
                  BatchNorm(512),       # 1_024 parameters, plus 1_024
                ),
                identity,
              ),
            ),
          ],
        ),
      ),
    ),
    GlobalMeanPool(),
    Flux.flatten,
    Dense(512 =&gt; 10),                   # 5_130 parameters
    NNlib.softmax,
  ),
)         # Total: 76 trainable arrays, 11_178_378 parameters,
          # plus 34 non-trainable, 7_808 parameters, summarysize 42.680 MiB.</code></pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>We train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a powerful and flexible architecture. The plot capturing training and validation loss illustrates a significant gap between both graphs, with the training loss being considerably lower. For a network of this flexibility, more training data would offer distinct benefit in closing the gap and improving accuracy.</p><pre><code class="language-julia hljs">data = d2lai.FashionMNISTData(batchsize = 128, resize = (96,96));
opt = Descent(0.01)
trainer = Trainer(model, data, opt; max_epochs = 10, gpu = true, board_yscale = :identity)
d2lai.fit(trainer);</code></pre><div style="max-height:300px; overflow-y:auto; background:#111; color:#eee; padding:1em; border-radius:5px;">
<pre>    [ Info: Train Loss: 0.19277047, Val Loss: 0.19109984, Val Acc: 0.9375
    [ Info: Train Loss: 0.12554948, Val Loss: 0.14957266, Val Acc: 0.9375
    [ Info: Train Loss: 0.22092374, Val Loss: 0.14626466, Val Acc: 1.0
    [ Info: Train Loss: 0.04200567, Val Loss: 0.088903934, Val Acc: 1.0
    [ Info: Train Loss: 0.19900467, Val Loss: 0.1520779, Val Acc: 0.9375
    [ Info: Train Loss: 0.032146264, Val Loss: 0.08688806, Val Acc: 1.0
    [ Info: Train Loss: 0.06265808, Val Loss: 0.07228865, Val Acc: 1.0
    [ Info: Train Loss: 0.008486914, Val Loss: 0.22081932, Val Acc: 0.9375
    [ Info: Train Loss: 0.0028302989, Val Loss: 0.14812489, Val Acc: 0.9375
    [ Info: Train Loss: 0.008302619, Val Loss: 0.14797957, Val Acc: 0.9375</pre>
</div><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip140">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip140)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip141">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip140)" d="M156.598 1423.18 L2352.76 1423.18 L2352.76 47.2441 L156.598 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip142">
    <rect x="156" y="47" width="2197" height="1377"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="448.959,1423.18 448.959,47.2441 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="909.369,1423.18 909.369,47.2441 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1369.78,1423.18 1369.78,47.2441 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="1830.19,1423.18 1830.19,47.2441 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="2290.6,1423.18 2290.6,47.2441 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,1409.03 2352.76,1409.03 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,1119.65 2352.76,1119.65 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,830.271 2352.76,830.271 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,540.89 2352.76,540.89 "/>
<polyline clip-path="url(#clip142)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="156.598,251.509 2352.76,251.509 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1423.18 2352.76,1423.18 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="448.959,1423.18 448.959,1404.28 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="909.369,1423.18 909.369,1404.28 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1369.78,1423.18 1369.78,1404.28 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1830.19,1423.18 1830.19,1404.28 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="2290.6,1423.18 2290.6,1404.28 "/>
<path clip-path="url(#clip140)" d="M443.612 1481.64 L459.931 1481.64 L459.931 1485.58 L437.987 1485.58 L437.987 1481.64 Q440.649 1478.89 445.232 1474.26 Q449.838 1469.61 451.019 1468.27 Q453.264 1465.74 454.144 1464.01 Q455.047 1462.25 455.047 1460.56 Q455.047 1457.8 453.102 1456.07 Q451.181 1454.33 448.079 1454.33 Q445.88 1454.33 443.426 1455.09 Q440.996 1455.86 438.218 1457.41 L438.218 1452.69 Q441.042 1451.55 443.496 1450.97 Q445.95 1450.39 447.987 1450.39 Q453.357 1450.39 456.551 1453.08 Q459.746 1455.77 459.746 1460.26 Q459.746 1462.39 458.936 1464.31 Q458.149 1466.2 456.042 1468.8 Q455.463 1469.47 452.362 1472.69 Q449.26 1475.88 443.612 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M912.379 1455.09 L900.573 1473.54 L912.379 1473.54 L912.379 1455.09 M911.152 1451.02 L917.031 1451.02 L917.031 1473.54 L921.962 1473.54 L921.962 1477.43 L917.031 1477.43 L917.031 1485.58 L912.379 1485.58 L912.379 1477.43 L896.777 1477.43 L896.777 1472.92 L911.152 1451.02 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1370.18 1466.44 Q1367.04 1466.44 1365.18 1468.59 Q1363.36 1470.74 1363.36 1474.49 Q1363.36 1478.22 1365.18 1480.39 Q1367.04 1482.55 1370.18 1482.55 Q1373.33 1482.55 1375.16 1480.39 Q1377.01 1478.22 1377.01 1474.49 Q1377.01 1470.74 1375.16 1468.59 Q1373.33 1466.44 1370.18 1466.44 M1379.47 1451.78 L1379.47 1456.04 Q1377.71 1455.21 1375.9 1454.77 Q1374.12 1454.33 1372.36 1454.33 Q1367.73 1454.33 1365.28 1457.45 Q1362.85 1460.58 1362.5 1466.9 Q1363.87 1464.89 1365.93 1463.82 Q1367.99 1462.73 1370.46 1462.73 Q1375.67 1462.73 1378.68 1465.9 Q1381.71 1469.05 1381.71 1474.49 Q1381.71 1479.82 1378.56 1483.03 Q1375.42 1486.25 1370.18 1486.25 Q1364.19 1486.25 1361.02 1481.67 Q1357.85 1477.06 1357.85 1468.33 Q1357.85 1460.14 1361.74 1455.28 Q1365.62 1450.39 1372.18 1450.39 Q1373.93 1450.39 1375.72 1450.74 Q1377.52 1451.09 1379.47 1451.78 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1830.19 1469.17 Q1826.86 1469.17 1824.94 1470.95 Q1823.04 1472.73 1823.04 1475.86 Q1823.04 1478.98 1824.94 1480.77 Q1826.86 1482.55 1830.19 1482.55 Q1833.52 1482.55 1835.44 1480.77 Q1837.37 1478.96 1837.37 1475.86 Q1837.37 1472.73 1835.44 1470.95 Q1833.55 1469.17 1830.19 1469.17 M1825.51 1467.18 Q1822.5 1466.44 1820.82 1464.38 Q1819.15 1462.32 1819.15 1459.35 Q1819.15 1455.21 1822.09 1452.8 Q1825.05 1450.39 1830.19 1450.39 Q1835.35 1450.39 1838.29 1452.8 Q1841.23 1455.21 1841.23 1459.35 Q1841.23 1462.32 1839.54 1464.38 Q1837.88 1466.44 1834.89 1467.18 Q1838.27 1467.96 1840.14 1470.26 Q1842.04 1472.55 1842.04 1475.86 Q1842.04 1480.88 1838.96 1483.57 Q1835.91 1486.25 1830.19 1486.25 Q1824.47 1486.25 1821.39 1483.57 Q1818.34 1480.88 1818.34 1475.86 Q1818.34 1472.55 1820.24 1470.26 Q1822.13 1467.96 1825.51 1467.18 M1823.8 1459.79 Q1823.8 1462.48 1825.47 1463.98 Q1827.16 1465.49 1830.19 1465.49 Q1833.2 1465.49 1834.89 1463.98 Q1836.6 1462.48 1836.6 1459.79 Q1836.6 1457.11 1834.89 1455.6 Q1833.2 1454.1 1830.19 1454.1 Q1827.16 1454.1 1825.47 1455.6 Q1823.8 1457.11 1823.8 1459.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M2265.29 1481.64 L2272.93 1481.64 L2272.93 1455.28 L2264.62 1456.95 L2264.62 1452.69 L2272.88 1451.02 L2277.56 1451.02 L2277.56 1481.64 L2285.2 1481.64 L2285.2 1485.58 L2265.29 1485.58 L2265.29 1481.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M2304.64 1454.1 Q2301.03 1454.1 2299.2 1457.66 Q2297.39 1461.2 2297.39 1468.33 Q2297.39 1475.44 2299.2 1479.01 Q2301.03 1482.55 2304.64 1482.55 Q2308.27 1482.55 2310.08 1479.01 Q2311.91 1475.44 2311.91 1468.33 Q2311.91 1461.2 2310.08 1457.66 Q2308.27 1454.1 2304.64 1454.1 M2304.64 1450.39 Q2310.45 1450.39 2313.51 1455 Q2316.58 1459.58 2316.58 1468.33 Q2316.58 1477.06 2313.51 1481.67 Q2310.45 1486.25 2304.64 1486.25 Q2298.83 1486.25 2295.75 1481.67 Q2292.7 1477.06 2292.7 1468.33 Q2292.7 1459.58 2295.75 1455 Q2298.83 1450.39 2304.64 1450.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1174.87 1548.76 L1174.87 1551.62 L1147.94 1551.62 Q1148.32 1557.67 1151.57 1560.85 Q1154.85 1564 1160.67 1564 Q1164.05 1564 1167.2 1563.17 Q1170.38 1562.35 1173.5 1560.69 L1173.5 1566.23 Q1170.35 1567.57 1167.04 1568.27 Q1163.73 1568.97 1160.32 1568.97 Q1151.79 1568.97 1146.79 1564 Q1141.83 1559.04 1141.83 1550.57 Q1141.83 1541.82 1146.54 1536.69 Q1151.28 1531.54 1159.3 1531.54 Q1166.5 1531.54 1170.67 1536.18 Q1174.87 1540.8 1174.87 1548.76 M1169.01 1547.04 Q1168.95 1542.23 1166.31 1539.37 Q1163.7 1536.5 1159.37 1536.5 Q1154.46 1536.5 1151.5 1539.27 Q1148.58 1542.04 1148.13 1547.07 L1169.01 1547.04 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1190.14 1562.7 L1190.14 1581.6 L1184.26 1581.6 L1184.26 1532.4 L1190.14 1532.4 L1190.14 1537.81 Q1191.99 1534.62 1194.79 1533.1 Q1197.62 1531.54 1201.54 1531.54 Q1208.03 1531.54 1212.07 1536.69 Q1216.15 1541.85 1216.15 1550.25 Q1216.15 1558.65 1212.07 1563.81 Q1208.03 1568.97 1201.54 1568.97 Q1197.62 1568.97 1194.79 1567.44 Q1191.99 1565.88 1190.14 1562.7 M1210.07 1550.25 Q1210.07 1543.79 1207.4 1540.13 Q1204.75 1536.44 1200.11 1536.44 Q1195.46 1536.44 1192.79 1540.13 Q1190.14 1543.79 1190.14 1550.25 Q1190.14 1556.71 1192.79 1560.4 Q1195.46 1564.07 1200.11 1564.07 Q1204.75 1564.07 1207.4 1560.4 Q1210.07 1556.71 1210.07 1550.25 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1239.67 1536.5 Q1234.96 1536.5 1232.22 1540.19 Q1229.48 1543.85 1229.48 1550.25 Q1229.48 1556.65 1232.19 1560.34 Q1234.93 1564 1239.67 1564 Q1244.35 1564 1247.09 1560.31 Q1249.82 1556.62 1249.82 1550.25 Q1249.82 1543.92 1247.09 1540.23 Q1244.35 1536.5 1239.67 1536.5 M1239.67 1531.54 Q1247.31 1531.54 1251.67 1536.5 Q1256.03 1541.47 1256.03 1550.25 Q1256.03 1559 1251.67 1564 Q1247.31 1568.97 1239.67 1568.97 Q1232 1568.97 1227.64 1564 Q1223.31 1559 1223.31 1550.25 Q1223.31 1541.47 1227.64 1536.5 Q1232 1531.54 1239.67 1531.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1291.39 1533.76 L1291.39 1539.24 Q1288.91 1537.87 1286.39 1537.2 Q1283.91 1536.5 1281.37 1536.5 Q1275.67 1536.5 1272.52 1540.13 Q1269.37 1543.73 1269.37 1550.25 Q1269.37 1556.78 1272.52 1560.4 Q1275.67 1564 1281.37 1564 Q1283.91 1564 1286.39 1563.33 Q1288.91 1562.63 1291.39 1561.26 L1291.39 1566.68 Q1288.94 1567.82 1286.3 1568.39 Q1283.69 1568.97 1280.73 1568.97 Q1272.68 1568.97 1267.93 1563.91 Q1263.19 1558.85 1263.19 1550.25 Q1263.19 1541.53 1267.97 1536.53 Q1272.77 1531.54 1281.11 1531.54 Q1283.82 1531.54 1286.39 1532.11 Q1288.97 1532.65 1291.39 1533.76 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1331.21 1546.53 L1331.21 1568.04 L1325.35 1568.04 L1325.35 1546.72 Q1325.35 1541.66 1323.38 1539.14 Q1321.41 1536.63 1317.46 1536.63 Q1312.72 1536.63 1309.98 1539.65 Q1307.24 1542.68 1307.24 1547.9 L1307.24 1568.04 L1301.35 1568.04 L1301.35 1518.52 L1307.24 1518.52 L1307.24 1537.93 Q1309.34 1534.72 1312.18 1533.13 Q1315.04 1531.54 1318.76 1531.54 Q1324.91 1531.54 1328.06 1535.36 Q1331.21 1539.14 1331.21 1546.53 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M1365.62 1533.45 L1365.62 1538.98 Q1363.13 1537.71 1360.46 1537.07 Q1357.79 1536.44 1354.92 1536.44 Q1350.56 1536.44 1348.36 1537.77 Q1346.2 1539.11 1346.2 1541.79 Q1346.2 1543.82 1347.76 1545 Q1349.32 1546.15 1354.03 1547.2 L1356.04 1547.64 Q1362.27 1548.98 1364.88 1551.43 Q1367.53 1553.85 1367.53 1558.21 Q1367.53 1563.17 1363.58 1566.07 Q1359.66 1568.97 1352.79 1568.97 Q1349.92 1568.97 1346.8 1568.39 Q1343.72 1567.85 1340.28 1566.74 L1340.28 1560.69 Q1343.53 1562.38 1346.68 1563.24 Q1349.83 1564.07 1352.92 1564.07 Q1357.05 1564.07 1359.28 1562.66 Q1361.51 1561.23 1361.51 1558.65 Q1361.51 1556.27 1359.89 1554.99 Q1358.29 1553.72 1352.85 1552.54 L1350.82 1552.07 Q1345.37 1550.92 1342.95 1548.56 Q1340.53 1546.18 1340.53 1542.04 Q1340.53 1537.01 1344.1 1534.27 Q1347.66 1531.54 1354.22 1531.54 Q1357.47 1531.54 1360.33 1532.01 Q1363.2 1532.49 1365.62 1533.45 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1423.18 156.598,47.2441 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1409.03 175.496,1409.03 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,1119.65 175.496,1119.65 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,830.271 175.496,830.271 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,540.89 175.496,540.89 "/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="156.598,251.509 175.496,251.509 "/>
<path clip-path="url(#clip140)" d="M63.4226 1394.83 Q59.8115 1394.83 57.9828 1398.4 Q56.1773 1401.94 56.1773 1409.07 Q56.1773 1416.17 57.9828 1419.74 Q59.8115 1423.28 63.4226 1423.28 Q67.0569 1423.28 68.8624 1419.74 Q70.6911 1416.17 70.6911 1409.07 Q70.6911 1401.94 68.8624 1398.4 Q67.0569 1394.83 63.4226 1394.83 M63.4226 1391.13 Q69.2328 1391.13 72.2883 1395.73 Q75.367 1400.32 75.367 1409.07 Q75.367 1417.79 72.2883 1422.4 Q69.2328 1426.98 63.4226 1426.98 Q57.6125 1426.98 54.5338 1422.4 Q51.4782 1417.79 51.4782 1409.07 Q51.4782 1400.32 54.5338 1395.73 Q57.6125 1391.13 63.4226 1391.13 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M83.5845 1420.43 L88.4688 1420.43 L88.4688 1426.31 L83.5845 1426.31 L83.5845 1420.43 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M108.654 1394.83 Q105.043 1394.83 103.214 1398.4 Q101.409 1401.94 101.409 1409.07 Q101.409 1416.17 103.214 1419.74 Q105.043 1423.28 108.654 1423.28 Q112.288 1423.28 114.094 1419.74 Q115.922 1416.17 115.922 1409.07 Q115.922 1401.94 114.094 1398.4 Q112.288 1394.83 108.654 1394.83 M108.654 1391.13 Q114.464 1391.13 117.52 1395.73 Q120.598 1400.32 120.598 1409.07 Q120.598 1417.79 117.52 1422.4 Q114.464 1426.98 108.654 1426.98 Q102.844 1426.98 99.765 1422.4 Q96.7095 1417.79 96.7095 1409.07 Q96.7095 1400.32 99.765 1395.73 Q102.844 1391.13 108.654 1391.13 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M65.0198 1105.45 Q61.4087 1105.45 59.58 1109.02 Q57.7745 1112.56 57.7745 1119.69 Q57.7745 1126.79 59.58 1130.36 Q61.4087 1133.9 65.0198 1133.9 Q68.6541 1133.9 70.4596 1130.36 Q72.2883 1126.79 72.2883 1119.69 Q72.2883 1112.56 70.4596 1109.02 Q68.6541 1105.45 65.0198 1105.45 M65.0198 1101.75 Q70.83 1101.75 73.8855 1106.35 Q76.9642 1110.94 76.9642 1119.69 Q76.9642 1128.41 73.8855 1133.02 Q70.83 1137.6 65.0198 1137.6 Q59.2097 1137.6 56.131 1133.02 Q53.0754 1128.41 53.0754 1119.69 Q53.0754 1110.94 56.131 1106.35 Q59.2097 1101.75 65.0198 1101.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M85.1818 1131.05 L90.066 1131.05 L90.066 1136.93 L85.1818 1136.93 L85.1818 1131.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M104.279 1133 L120.598 1133 L120.598 1136.93 L98.6539 1136.93 L98.6539 1133 Q101.316 1130.24 105.899 1125.61 Q110.506 1120.96 111.686 1119.62 Q113.932 1117.09 114.811 1115.36 Q115.714 1113.6 115.714 1111.91 Q115.714 1109.15 113.77 1107.42 Q111.848 1105.68 108.746 1105.68 Q106.547 1105.68 104.094 1106.45 Q101.663 1107.21 98.8854 1108.76 L98.8854 1104.04 Q101.709 1102.9 104.163 1102.33 Q106.617 1101.75 108.654 1101.75 Q114.024 1101.75 117.219 1104.43 Q120.413 1107.12 120.413 1111.61 Q120.413 1113.74 119.603 1115.66 Q118.816 1117.56 116.709 1120.15 Q116.131 1120.82 113.029 1124.04 Q109.927 1127.23 104.279 1133 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M62.9365 816.07 Q59.3254 816.07 57.4967 819.634 Q55.6912 823.176 55.6912 830.306 Q55.6912 837.412 57.4967 840.977 Q59.3254 844.519 62.9365 844.519 Q66.5707 844.519 68.3763 840.977 Q70.205 837.412 70.205 830.306 Q70.205 823.176 68.3763 819.634 Q66.5707 816.07 62.9365 816.07 M62.9365 812.366 Q68.7467 812.366 71.8022 816.972 Q74.8809 821.556 74.8809 830.306 Q74.8809 839.032 71.8022 843.639 Q68.7467 848.222 62.9365 848.222 Q57.1264 848.222 54.0477 843.639 Q50.9921 839.032 50.9921 830.306 Q50.9921 821.556 54.0477 816.972 Q57.1264 812.366 62.9365 812.366 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M83.0984 841.671 L87.9827 841.671 L87.9827 847.551 L83.0984 847.551 L83.0984 841.671 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M111.015 817.065 L99.2095 835.514 L111.015 835.514 L111.015 817.065 M109.788 812.991 L115.668 812.991 L115.668 835.514 L120.598 835.514 L120.598 839.403 L115.668 839.403 L115.668 847.551 L111.015 847.551 L111.015 839.403 L95.4132 839.403 L95.4132 834.889 L109.788 812.991 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M63.2606 526.689 Q59.6495 526.689 57.8208 530.254 Q56.0152 533.795 56.0152 540.925 Q56.0152 548.031 57.8208 551.596 Q59.6495 555.138 63.2606 555.138 Q66.8948 555.138 68.7004 551.596 Q70.5291 548.031 70.5291 540.925 Q70.5291 533.795 68.7004 530.254 Q66.8948 526.689 63.2606 526.689 M63.2606 522.985 Q69.0707 522.985 72.1263 527.592 Q75.205 532.175 75.205 540.925 Q75.205 549.652 72.1263 554.258 Q69.0707 558.841 63.2606 558.841 Q57.4504 558.841 54.3717 554.258 Q51.3162 549.652 51.3162 540.925 Q51.3162 532.175 54.3717 527.592 Q57.4504 522.985 63.2606 522.985 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M83.4225 552.291 L88.3067 552.291 L88.3067 558.17 L83.4225 558.17 L83.4225 552.291 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M109.071 539.027 Q105.922 539.027 104.071 541.179 Q102.242 543.332 102.242 547.082 Q102.242 550.809 104.071 552.985 Q105.922 555.138 109.071 555.138 Q112.219 555.138 114.047 552.985 Q115.899 550.809 115.899 547.082 Q115.899 543.332 114.047 541.179 Q112.219 539.027 109.071 539.027 M118.353 524.374 L118.353 528.633 Q116.594 527.8 114.788 527.36 Q113.006 526.92 111.246 526.92 Q106.617 526.92 104.163 530.045 Q101.733 533.17 101.385 539.49 Q102.751 537.476 104.811 536.411 Q106.871 535.323 109.348 535.323 Q114.557 535.323 117.566 538.494 Q120.598 541.642 120.598 547.082 Q120.598 552.406 117.45 555.624 Q114.302 558.841 109.071 558.841 Q103.075 558.841 99.9039 554.258 Q96.7326 549.652 96.7326 540.925 Q96.7326 532.73 100.621 527.869 Q104.51 522.985 111.061 522.985 Q112.82 522.985 114.603 523.332 Q116.408 523.68 118.353 524.374 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M63.5152 237.308 Q59.9041 237.308 58.0754 240.873 Q56.2699 244.414 56.2699 251.544 Q56.2699 258.651 58.0754 262.215 Q59.9041 265.757 63.5152 265.757 Q67.1494 265.757 68.955 262.215 Q70.7837 258.651 70.7837 251.544 Q70.7837 244.414 68.955 240.873 Q67.1494 237.308 63.5152 237.308 M63.5152 233.604 Q69.3254 233.604 72.3809 238.211 Q75.4596 242.794 75.4596 251.544 Q75.4596 260.271 72.3809 264.877 Q69.3254 269.461 63.5152 269.461 Q57.7051 269.461 54.6264 264.877 Q51.5708 260.271 51.5708 251.544 Q51.5708 242.794 54.6264 238.211 Q57.7051 233.604 63.5152 233.604 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M83.6771 262.91 L88.5614 262.91 L88.5614 268.789 L83.6771 268.789 L83.6771 262.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M108.746 252.377 Q105.413 252.377 103.492 254.16 Q101.594 255.942 101.594 259.067 Q101.594 262.192 103.492 263.975 Q105.413 265.757 108.746 265.757 Q112.08 265.757 114.001 263.975 Q115.922 262.169 115.922 259.067 Q115.922 255.942 114.001 254.16 Q112.103 252.377 108.746 252.377 M104.071 250.387 Q101.061 249.646 99.3715 247.586 Q97.7048 245.526 97.7048 242.563 Q97.7048 238.419 100.645 236.012 Q103.608 233.604 108.746 233.604 Q113.908 233.604 116.848 236.012 Q119.788 238.419 119.788 242.563 Q119.788 245.526 118.098 247.586 Q116.432 249.646 113.445 250.387 Q116.825 251.174 118.7 253.465 Q120.598 255.757 120.598 259.067 Q120.598 264.09 117.52 266.775 Q114.464 269.461 108.746 269.461 Q103.029 269.461 99.9502 266.775 Q96.8947 264.09 96.8947 259.067 Q96.8947 255.757 98.7928 253.465 Q100.691 251.174 104.071 250.387 M102.358 243.002 Q102.358 245.688 104.024 247.192 Q105.714 248.697 108.746 248.697 Q111.756 248.697 113.445 247.192 Q115.158 245.688 115.158 243.002 Q115.158 240.317 113.445 238.813 Q111.756 237.308 108.746 237.308 Q105.714 237.308 104.024 238.813 Q102.358 240.317 102.358 243.002 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip142)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,703.273 448.959,1152.46 679.164,1222.63 909.369,1279.27 1139.57,1314.21 1369.78,1335.18 1599.98,1355.2 1830.19,1364.14 2060.4,1374.94 2290.6,1384.24 "/>
<polyline clip-path="url(#clip142)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,795.425 448.959,908.69 679.164,876.237 909.369,926.585 1139.57,852.786 1369.78,957.572 1599.98,911.944 1830.19,895.45 2060.4,964.959 2290.6,953.456 "/>
<polyline clip-path="url(#clip142)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="218.754,193.931 448.959,144.566 679.164,157.014 909.369,128.397 1139.57,161.45 1369.78,112.085 1599.98,111.512 1830.19,109.366 2060.4,89.906 2290.6,86.1857 "/>
<path clip-path="url(#clip140)" d="M229.803 1377.32 L671.455 1377.32 L671.455 1169.96 L229.803 1169.96  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip140)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="229.803,1377.32 671.455,1377.32 671.455,1169.96 229.803,1169.96 229.803,1377.32 "/>
<polyline clip-path="url(#clip140)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,1221.8 400.616,1221.8 "/>
<path clip-path="url(#clip140)" d="M432.425 1205.79 L432.425 1213.15 L441.198 1213.15 L441.198 1216.46 L432.425 1216.46 L432.425 1230.53 Q432.425 1233.7 433.281 1234.61 Q434.161 1235.51 436.823 1235.51 L441.198 1235.51 L441.198 1239.08 L436.823 1239.08 Q431.892 1239.08 430.017 1237.25 Q428.142 1235.39 428.142 1230.53 L428.142 1216.46 L425.018 1216.46 L425.018 1213.15 L428.142 1213.15 L428.142 1205.79 L432.425 1205.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M461.823 1217.13 Q461.105 1216.71 460.249 1216.53 Q459.415 1216.32 458.397 1216.32 Q454.786 1216.32 452.841 1218.68 Q450.92 1221.02 450.92 1225.42 L450.92 1239.08 L446.638 1239.08 L446.638 1213.15 L450.92 1213.15 L450.92 1217.18 Q452.263 1214.82 454.415 1213.68 Q456.568 1212.52 459.647 1212.52 Q460.087 1212.52 460.619 1212.59 Q461.152 1212.64 461.8 1212.76 L461.823 1217.13 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M478.073 1226.04 Q472.911 1226.04 470.92 1227.22 Q468.929 1228.4 468.929 1231.25 Q468.929 1233.52 470.411 1234.86 Q471.915 1236.18 474.485 1236.18 Q478.026 1236.18 480.156 1233.68 Q482.309 1231.16 482.309 1226.99 L482.309 1226.04 L478.073 1226.04 M486.568 1224.28 L486.568 1239.08 L482.309 1239.08 L482.309 1235.14 Q480.851 1237.5 478.675 1238.64 Q476.499 1239.75 473.351 1239.75 Q469.369 1239.75 467.008 1237.52 Q464.67 1235.28 464.67 1231.53 Q464.67 1227.15 467.587 1224.93 Q470.527 1222.71 476.337 1222.71 L482.309 1222.71 L482.309 1222.29 Q482.309 1219.35 480.364 1217.76 Q478.443 1216.14 474.948 1216.14 Q472.726 1216.14 470.619 1216.67 Q468.513 1217.2 466.568 1218.27 L466.568 1214.33 Q468.906 1213.43 471.105 1212.99 Q473.304 1212.52 475.388 1212.52 Q481.013 1212.52 483.79 1215.44 Q486.568 1218.36 486.568 1224.28 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M495.341 1213.15 L499.6 1213.15 L499.6 1239.08 L495.341 1239.08 L495.341 1213.15 M495.341 1203.06 L499.6 1203.06 L499.6 1208.45 L495.341 1208.45 L495.341 1203.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M530.063 1223.43 L530.063 1239.08 L525.804 1239.08 L525.804 1223.57 Q525.804 1219.89 524.369 1218.06 Q522.934 1216.23 520.063 1216.23 Q516.614 1216.23 514.624 1218.43 Q512.633 1220.63 512.633 1224.42 L512.633 1239.08 L508.35 1239.08 L508.35 1213.15 L512.633 1213.15 L512.633 1217.18 Q514.161 1214.84 516.221 1213.68 Q518.304 1212.52 521.012 1212.52 Q525.48 1212.52 527.772 1215.3 Q530.063 1218.06 530.063 1223.43 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M558.258 1246.95 L558.258 1250.26 L533.628 1250.26 L533.628 1246.95 L558.258 1246.95 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M562.262 1203.06 L566.521 1203.06 L566.521 1239.08 L562.262 1239.08 L562.262 1203.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M585.48 1216.14 Q582.054 1216.14 580.063 1218.82 Q578.072 1221.48 578.072 1226.14 Q578.072 1230.79 580.04 1233.47 Q582.031 1236.14 585.48 1236.14 Q588.882 1236.14 590.873 1233.45 Q592.864 1230.77 592.864 1226.14 Q592.864 1221.53 590.873 1218.84 Q588.882 1216.14 585.48 1216.14 M585.48 1212.52 Q591.035 1212.52 594.206 1216.14 Q597.378 1219.75 597.378 1226.14 Q597.378 1232.5 594.206 1236.14 Q591.035 1239.75 585.48 1239.75 Q579.901 1239.75 576.73 1236.14 Q573.582 1232.5 573.582 1226.14 Q573.582 1219.75 576.73 1216.14 Q579.901 1212.52 585.48 1212.52 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M620.966 1213.91 L620.966 1217.94 Q619.16 1217.02 617.216 1216.55 Q615.271 1216.09 613.188 1216.09 Q610.017 1216.09 608.419 1217.06 Q606.845 1218.03 606.845 1219.98 Q606.845 1221.46 607.979 1222.32 Q609.114 1223.15 612.54 1223.91 L613.998 1224.24 Q618.535 1225.21 620.433 1226.99 Q622.354 1228.75 622.354 1231.92 Q622.354 1235.53 619.484 1237.64 Q616.637 1239.75 611.637 1239.75 Q609.554 1239.75 607.285 1239.33 Q605.04 1238.94 602.54 1238.13 L602.54 1233.73 Q604.901 1234.95 607.192 1235.58 Q609.484 1236.18 611.729 1236.18 Q614.739 1236.18 616.359 1235.16 Q617.979 1234.12 617.979 1232.25 Q617.979 1230.51 616.799 1229.58 Q615.641 1228.66 611.683 1227.8 L610.202 1227.46 Q606.243 1226.62 604.484 1224.91 Q602.725 1223.17 602.725 1220.16 Q602.725 1216.51 605.317 1214.52 Q607.91 1212.52 612.679 1212.52 Q615.04 1212.52 617.123 1212.87 Q619.206 1213.22 620.966 1213.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M645.664 1213.91 L645.664 1217.94 Q643.859 1217.02 641.914 1216.55 Q639.97 1216.09 637.887 1216.09 Q634.715 1216.09 633.118 1217.06 Q631.544 1218.03 631.544 1219.98 Q631.544 1221.46 632.678 1222.32 Q633.813 1223.15 637.239 1223.91 L638.697 1224.24 Q643.234 1225.21 645.132 1226.99 Q647.053 1228.75 647.053 1231.92 Q647.053 1235.53 644.183 1237.64 Q641.336 1239.75 636.336 1239.75 Q634.252 1239.75 631.984 1239.33 Q629.739 1238.94 627.239 1238.13 L627.239 1233.73 Q629.6 1234.95 631.891 1235.58 Q634.183 1236.18 636.428 1236.18 Q639.438 1236.18 641.058 1235.16 Q642.678 1234.12 642.678 1232.25 Q642.678 1230.51 641.498 1229.58 Q640.34 1228.66 636.382 1227.8 L634.901 1227.46 Q630.942 1226.62 629.183 1224.91 Q627.424 1223.17 627.424 1220.16 Q627.424 1216.51 630.016 1214.52 Q632.609 1212.52 637.377 1212.52 Q639.739 1212.52 641.822 1212.87 Q643.905 1213.22 645.664 1213.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip140)" style="stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,1273.64 400.616,1273.64 "/>
<path clip-path="url(#clip140)" d="M425.018 1264.99 L429.531 1264.99 L437.633 1286.75 L445.735 1264.99 L450.249 1264.99 L440.527 1290.92 L434.74 1290.92 L425.018 1264.99 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M467.911 1277.88 Q462.749 1277.88 460.758 1279.06 Q458.767 1280.24 458.767 1283.09 Q458.767 1285.36 460.249 1286.7 Q461.753 1288.02 464.323 1288.02 Q467.864 1288.02 469.994 1285.52 Q472.147 1283 472.147 1278.83 L472.147 1277.88 L467.911 1277.88 M476.406 1276.12 L476.406 1290.92 L472.147 1290.92 L472.147 1286.98 Q470.689 1289.34 468.513 1290.48 Q466.337 1291.59 463.189 1291.59 Q459.207 1291.59 456.846 1289.36 Q454.508 1287.12 454.508 1283.37 Q454.508 1278.99 457.425 1276.77 Q460.365 1274.55 466.175 1274.55 L472.147 1274.55 L472.147 1274.13 Q472.147 1271.19 470.202 1269.6 Q468.281 1267.98 464.786 1267.98 Q462.564 1267.98 460.457 1268.51 Q458.351 1269.04 456.406 1270.11 L456.406 1266.17 Q458.744 1265.27 460.943 1264.83 Q463.142 1264.36 465.226 1264.36 Q470.851 1264.36 473.628 1267.28 Q476.406 1270.2 476.406 1276.12 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M485.179 1254.9 L489.438 1254.9 L489.438 1290.92 L485.179 1290.92 L485.179 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M518.049 1298.79 L518.049 1302.1 L493.42 1302.1 L493.42 1298.79 L518.049 1298.79 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M522.054 1254.9 L526.313 1254.9 L526.313 1290.92 L522.054 1290.92 L522.054 1254.9 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M545.271 1267.98 Q541.846 1267.98 539.855 1270.66 Q537.864 1273.32 537.864 1277.98 Q537.864 1282.63 539.832 1285.31 Q541.822 1287.98 545.271 1287.98 Q548.674 1287.98 550.665 1285.29 Q552.656 1282.61 552.656 1277.98 Q552.656 1273.37 550.665 1270.68 Q548.674 1267.98 545.271 1267.98 M545.271 1264.36 Q550.827 1264.36 553.998 1267.98 Q557.17 1271.59 557.17 1277.98 Q557.17 1284.34 553.998 1287.98 Q550.827 1291.59 545.271 1291.59 Q539.693 1291.59 536.522 1287.98 Q533.373 1284.34 533.373 1277.98 Q533.373 1271.59 536.522 1267.98 Q539.693 1264.36 545.271 1264.36 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M580.757 1265.75 L580.757 1269.78 Q578.952 1268.86 577.007 1268.39 Q575.063 1267.93 572.98 1267.93 Q569.808 1267.93 568.211 1268.9 Q566.637 1269.87 566.637 1271.82 Q566.637 1273.3 567.771 1274.16 Q568.906 1274.99 572.332 1275.75 L573.79 1276.08 Q578.327 1277.05 580.225 1278.83 Q582.146 1280.59 582.146 1283.76 Q582.146 1287.37 579.276 1289.48 Q576.429 1291.59 571.429 1291.59 Q569.345 1291.59 567.077 1291.17 Q564.832 1290.78 562.332 1289.97 L562.332 1285.57 Q564.693 1286.79 566.984 1287.42 Q569.276 1288.02 571.521 1288.02 Q574.531 1288.02 576.151 1287 Q577.771 1285.96 577.771 1284.09 Q577.771 1282.35 576.591 1281.42 Q575.433 1280.5 571.475 1279.64 L569.994 1279.3 Q566.035 1278.46 564.276 1276.75 Q562.517 1275.01 562.517 1272 Q562.517 1268.35 565.109 1266.36 Q567.702 1264.36 572.47 1264.36 Q574.832 1264.36 576.915 1264.71 Q578.998 1265.06 580.757 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M605.456 1265.75 L605.456 1269.78 Q603.651 1268.86 601.706 1268.39 Q599.762 1267.93 597.679 1267.93 Q594.507 1267.93 592.91 1268.9 Q591.336 1269.87 591.336 1271.82 Q591.336 1273.3 592.47 1274.16 Q593.605 1274.99 597.03 1275.75 L598.489 1276.08 Q603.026 1277.05 604.924 1278.83 Q606.845 1280.59 606.845 1283.76 Q606.845 1287.37 603.975 1289.48 Q601.128 1291.59 596.128 1291.59 Q594.044 1291.59 591.776 1291.17 Q589.531 1290.78 587.031 1289.97 L587.031 1285.57 Q589.392 1286.79 591.683 1287.42 Q593.975 1288.02 596.22 1288.02 Q599.23 1288.02 600.85 1287 Q602.47 1285.96 602.47 1284.09 Q602.47 1282.35 601.29 1281.42 Q600.132 1280.5 596.174 1279.64 L594.693 1279.3 Q590.734 1278.46 588.975 1276.75 Q587.216 1275.01 587.216 1272 Q587.216 1268.35 589.808 1266.36 Q592.401 1264.36 597.169 1264.36 Q599.53 1264.36 601.614 1264.71 Q603.697 1265.06 605.456 1265.75 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip140)" style="stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="254.205,1325.48 400.616,1325.48 "/>
<path clip-path="url(#clip140)" d="M425.018 1316.83 L429.531 1316.83 L437.633 1338.59 L445.735 1316.83 L450.249 1316.83 L440.527 1342.76 L434.74 1342.76 L425.018 1316.83 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M467.911 1329.72 Q462.749 1329.72 460.758 1330.9 Q458.767 1332.08 458.767 1334.93 Q458.767 1337.2 460.249 1338.54 Q461.753 1339.86 464.323 1339.86 Q467.864 1339.86 469.994 1337.36 Q472.147 1334.84 472.147 1330.67 L472.147 1329.72 L467.911 1329.72 M476.406 1327.96 L476.406 1342.76 L472.147 1342.76 L472.147 1338.82 Q470.689 1341.18 468.513 1342.32 Q466.337 1343.43 463.189 1343.43 Q459.207 1343.43 456.846 1341.2 Q454.508 1338.96 454.508 1335.21 Q454.508 1330.83 457.425 1328.61 Q460.365 1326.39 466.175 1326.39 L472.147 1326.39 L472.147 1325.97 Q472.147 1323.03 470.202 1321.44 Q468.281 1319.82 464.786 1319.82 Q462.564 1319.82 460.457 1320.35 Q458.351 1320.88 456.406 1321.95 L456.406 1318.01 Q458.744 1317.11 460.943 1316.67 Q463.142 1316.2 465.226 1316.2 Q470.851 1316.2 473.628 1319.12 Q476.406 1322.04 476.406 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M485.179 1306.74 L489.438 1306.74 L489.438 1342.76 L485.179 1342.76 L485.179 1306.74 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M518.049 1350.63 L518.049 1353.94 L493.42 1353.94 L493.42 1350.63 L518.049 1350.63 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M533.836 1329.72 Q528.674 1329.72 526.684 1330.9 Q524.693 1332.08 524.693 1334.93 Q524.693 1337.2 526.174 1338.54 Q527.679 1339.86 530.248 1339.86 Q533.79 1339.86 535.92 1337.36 Q538.072 1334.84 538.072 1330.67 L538.072 1329.72 L533.836 1329.72 M542.332 1327.96 L542.332 1342.76 L538.072 1342.76 L538.072 1338.82 Q536.614 1341.18 534.438 1342.32 Q532.262 1343.43 529.114 1343.43 Q525.133 1343.43 522.772 1341.2 Q520.434 1338.96 520.434 1335.21 Q520.434 1330.83 523.35 1328.61 Q526.29 1326.39 532.1 1326.39 L538.072 1326.39 L538.072 1325.97 Q538.072 1323.03 536.128 1321.44 Q534.207 1319.82 530.711 1319.82 Q528.489 1319.82 526.383 1320.35 Q524.276 1320.88 522.332 1321.95 L522.332 1318.01 Q524.67 1317.11 526.869 1316.67 Q529.068 1316.2 531.151 1316.2 Q536.776 1316.2 539.554 1319.12 Q542.332 1322.04 542.332 1327.96 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M569.762 1317.82 L569.762 1321.81 Q567.957 1320.81 566.128 1320.32 Q564.322 1319.82 562.47 1319.82 Q558.327 1319.82 556.035 1322.45 Q553.744 1325.07 553.744 1329.82 Q553.744 1334.56 556.035 1337.2 Q558.327 1339.82 562.47 1339.82 Q564.322 1339.82 566.128 1339.33 Q567.957 1338.82 569.762 1337.82 L569.762 1341.76 Q567.98 1342.59 566.058 1343.01 Q564.16 1343.43 562.008 1343.43 Q556.151 1343.43 552.702 1339.75 Q549.253 1336.07 549.253 1329.82 Q549.253 1323.47 552.725 1319.84 Q556.221 1316.2 562.285 1316.2 Q564.253 1316.2 566.128 1316.62 Q568.003 1317.01 569.762 1317.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip140)" d="M595.827 1317.82 L595.827 1321.81 Q594.021 1320.81 592.193 1320.32 Q590.387 1319.82 588.535 1319.82 Q584.392 1319.82 582.1 1322.45 Q579.808 1325.07 579.808 1329.82 Q579.808 1334.56 582.1 1337.2 Q584.392 1339.82 588.535 1339.82 Q590.387 1339.82 592.193 1339.33 Q594.021 1338.82 595.827 1337.82 L595.827 1341.76 Q594.044 1342.59 592.123 1343.01 Q590.225 1343.43 588.072 1343.43 Q582.216 1343.43 578.767 1339.75 Q575.318 1336.07 575.318 1329.82 Q575.318 1323.47 578.79 1319.84 Q582.285 1316.2 588.35 1316.2 Q590.318 1316.2 592.193 1316.62 Q594.068 1317.01 595.827 1317.82 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /></svg><pre><code class="language-julia hljs"></code></pre><h2 id="ResNeXt"><a class="docs-heading-anchor" href="#ResNeXt">ResNeXt</a><a id="ResNeXt-1"></a><a class="docs-heading-anchor-permalink" href="#ResNeXt" title="Permalink"></a></h2><p>:label:<code>subsec_resnext</code></p><p>One of the challenges one encounters in the design of ResNet is the trade-off between nonlinearity and dimensionality within a given block. That is, we could add more nonlinearity by increasing the number of layers, or by increasing the width of the convolutions. An alternative strategy is to increase the number of channels that can carry information between blocks. Unfortunately, the latter comes with a quadratic penalty since the computational cost of ingesting <span>$c_\textrm{i}$</span> channels and emitting <span>$c_\textrm{o}$</span> channels is proportional to <span>$\mathcal{O}(c_\textrm{i} \cdot c_\textrm{o})$</span> (see our discussion in :numref:<code>sec_channels</code>). </p><p>We can take some inspiration from the Inception block of :numref:<code>fig_inception</code> which has information flowing through the block in separate groups. Applying the idea of multiple independent groups to the ResNet block of :numref:<code>fig_resnet_block</code> led to the design of ResNeXt :cite:<code>Xie.Girshick.Dollar.ea.2017</code>. Different from the smorgasbord of transformations in Inception,  ResNeXt adopts the <em>same</em> transformation in all branches, thus minimizing the need for manual tuning of each branch. </p><p><img src="../../img/resnext-block.svg" alt="The ResNeXt block. The use of grouped convolution with \$\\mathit{g}\$ groups is \$\\mathit{g}\$ times faster than a dense convolution. It is a bottleneck residual block when the number of intermediate channels \$\\mathit{b}\$ is less than \$\\mathit{c}\$."/> :label:<code>fig_resnext_block</code></p><p>Breaking up a convolution from <span>$c_\textrm{i}$</span> to <span>$c_\textrm{o}$</span> channels into one of <span>$g$</span> groups of size <span>$c_\textrm{i}/g$</span> generating <span>$g$</span> outputs of size <span>$c_\textrm{o}/g$</span> is called, quite fittingly, a <em>grouped convolution</em>. The computational cost (proportionally) is reduced from <span>$\mathcal{O}(c_\textrm{i} \cdot c_\textrm{o})$</span> to <span>$\mathcal{O}(g \cdot (c_\textrm{i}/g) \cdot (c_\textrm{o}/g)) = \mathcal{O}(c_\textrm{i} \cdot c_\textrm{o} / g)$</span>, i.e., it is <span>$g$</span> times faster. Even better, the number of parameters needed to generate the output is also reduced from a <span>$c_\textrm{i} \times c_\textrm{o}$</span> matrix to <span>$g$</span> smaller matrices of size <span>$(c_\textrm{i}/g) \times (c_\textrm{o}/g)$</span>, again a <span>$g$</span> times reduction. In what follows we assume that both <span>$c_\textrm{i}$</span> and <span>$c_\textrm{o}$</span> are divisible by <span>$g$</span>. </p><p>The only challenge in this design is that no information is exchanged between the <span>$g$</span> groups. The ResNeXt block of  :numref:<code>fig_resnext_block</code> amends this in two ways: the grouped convolution with a <span>$3 \times 3$</span> kernel is sandwiched in between two <span>$1 \times 1$</span> convolutions. The second one serves double duty in changing the number of channels back. The benefit is that we only pay the <span>$\mathcal{O}(c \cdot b)$</span> cost for <span>$1 \times 1$</span> kernels and can make do with an <span>$\mathcal{O}(b^2 / g)$</span> cost for <span>$3 \times 3$</span> kernels. Similar to the residual block implementation in :numref:<code>subsec_residual-blks</code>, the residual connection is replaced (thus generalized) by a <span>$1 \times 1$</span> convolution.</p><p>The right-hand figure in :numref:<code>fig_resnext_block</code> provides a much more concise summary of the resulting network block. It will also play a major role in the design of generic modern CNNs in :numref:<code>sec_cnn-design</code>. Note that the idea of grouped convolutions dates back to the implementation of AlexNet :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code>. When distributing the network across two GPUs with limited memory, the implementation treated each GPU as its own channel with no ill effects. </p><p>The following implementation of the <code>ResNeXtBlock</code> class takes as argument <code>groups</code> (<span>$g$</span>), with  <code>bot_channels</code> (<span>$b$</span>) intermediate (bottleneck) channels. Lastly, when we need to reduce the height and width of the representation, we add a stride of <span>$2$</span> by setting <code>use_1x1conv=True, strides=2</code>.</p><pre><code class="language-julia hljs">struct ResNeXtBlock{N} &lt;: AbstractClassifier 
    net::N
end

function ResNeXtBlock(channel_in::Int, groups::Int, bot_mul; num_channels = channel_in, 
        
        use_1x1conv = !isequal(channel_in, num_channels),
        stride = 1)
    bot_channels = Int(round(num_channels*bot_mul))
    bottleneck_net = Chain(
        Conv((1,1), channel_in =&gt; bot_channels),
        BatchNorm(bot_channels, relu),
        Conv((3,3), bot_channels =&gt; bot_channels, pad = 1, stride = stride, groups = groups),
        BatchNorm(bot_channels, relu),
        Conv((1,1), bot_channels =&gt; num_channels, stride = 1),
        BatchNorm(num_channels, relu)
    )
    net = if !use_1x1conv
        Parallel(+, Flux.identity, bottleneck_net)
    else
        sidenet = Chain(
            Conv((1,1), channel_in =&gt; num_channels, stride = stride),
            BatchNorm(num_channels, relu)
        )
        Parallel(+, sidenet, bottleneck_net)
    end
    ResNeXtBlock(net)
end
Flux.@layer ResNeXtBlock 
(r::ResNeXtBlock)(x) = r.net(x)</code></pre><p>Its use is entirely analogous to that of the <code>ResNetBlock</code> discussed previously. For instance, when using (<code>use_1x1conv=False, strides=1</code>), the input and output are of the same shape. Alternatively, setting <code>use_1x1conv=True, strides=2</code> halves the output height and width.</p><pre><code class="language-julia hljs">blk = ResNeXtBlock(32, 16, 1)</code></pre><pre><code class="nohighlight hljs">ResNeXtBlock(
  Parallel(
    +,
    identity,
    Chain(
      Conv((1, 1), 32 =&gt; 32),           # 1_056 parameters
      BatchNorm(32, relu),              # 64 parameters, plus 64
      Conv((3, 3), 32 =&gt; 32, pad=1, groups=16),  # 608 parameters
      BatchNorm(32, relu),              # 64 parameters, plus 64
      Conv((1, 1), 32 =&gt; 32),           # 1_056 parameters
      BatchNorm(32, relu),              # 64 parameters, plus 64
    ),
  ),
)         # Total: 12 trainable arrays, 2_912 parameters,
          # plus 6 non-trainable, 192 parameters, summarysize 13.344 KiB.</code></pre><pre><code class="language-julia hljs">blk = ResNeXtBlock(32, 16, 2; num_channels = 64)</code></pre><pre><code class="nohighlight hljs">ResNeXtBlock(
  Parallel(
    +,
    Chain(
      Conv((1, 1), 32 =&gt; 64),           # 2_112 parameters
      BatchNorm(64, relu),              # 128 parameters, plus 128
    ),
    Chain(
      Conv((1, 1), 32 =&gt; 128),          # 4_224 parameters
      BatchNorm(128, relu),             # 256 parameters, plus 256
      Conv((3, 3), 128 =&gt; 128, pad=1, groups=16),  # 9_344 parameters
      BatchNorm(128, relu),             # 256 parameters, plus 256
      Conv((1, 1), 128 =&gt; 64),          # 8_256 parameters
      BatchNorm(64, relu),              # 128 parameters, plus 128
    ),
  ),
)         # Total: 16 trainable arrays, 24_704 parameters,
          # plus 8 non-trainable, 768 parameters, summarysize 101.125 KiB.</code></pre><h2 id="Summary-and-Discussion"><a class="docs-heading-anchor" href="#Summary-and-Discussion">Summary and Discussion</a><a id="Summary-and-Discussion-1"></a><a class="docs-heading-anchor-permalink" href="#Summary-and-Discussion" title="Permalink"></a></h2><p>Nested function classes are desirable since they allow us to obtain strictly <em>more powerful</em> rather than also subtly <em>different</em> function classes when adding capacity. One way of accomplishing this is by letting additional layers to simply pass through the input to the output. Residual connections allow for this. As a consequence, this changes the inductive bias from simple functions being of the form <span>$f(\mathbf{x}) = 0$</span> to simple functions looking like <span>$f(\mathbf{x}) = \mathbf{x}$</span>. </p><p>The residual mapping can learn the identity function more easily, such as pushing parameters in the weight layer to zero. We can train an effective <em>deep</em> neural network by having residual blocks. Inputs can forward propagate faster through the residual connections across layers. As a consequence, we can thus train much deeper networks. For instance, the original ResNet paper :cite:<code>He.Zhang.Ren.ea.2016</code> allowed for up to 152 layers. Another benefit of residual networks is that it allows us to add layers, initialized as the identity function, <em>during</em> the training process. After all, the default behavior of a layer is to let the data pass through unchanged. This can accelerate the training of very large networks in some cases. </p><p>Prior to residual connections, bypassing paths with gating units were introduced to effectively train highway networks with over 100 layers :cite:<code>srivastava2015highway</code>. Using identity functions as bypassing paths, ResNet performed remarkably well on multiple computer vision tasks. Residual connections had a major influence on the design of subsequent deep neural networks, of either convolutional or sequential nature. As we will introduce later, the Transformer architecture :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code> adopts residual connections (together with other design choices) and is pervasive in areas as diverse as language, vision, speech, and reinforcement learning.</p><p>ResNeXt is an example for how the design of convolutional neural networks has evolved over time: by being more frugal with computation and trading it off against the size of the activations (number of channels), it allows for faster and more accurate networks at lower cost. An alternative way of viewing grouped convolutions is to think of a block-diagonal matrix for the convolutional weights. Note that there are quite a few such &quot;tricks&quot; that lead to more efficient networks. For instance, ShiftNet :cite:<code>wu2018shift</code> mimicks the effects of a <span>$3 \times 3$</span> convolution, simply by adding shifted activations to the channels, offering increased function complexity, this time without any computational cost. </p><p>A common feature of the designs we have discussed so far is that the network design is fairly manual, primarily relying on the ingenuity of the designer to find the &quot;right&quot; network hyperparameters. While clearly feasible, it is also very costly in terms of human time and there is no guarantee that the outcome is optimal in any sense. In :numref:<code>sec_cnn-design</code> we will discuss a number of strategies for obtaining high quality networks in a more automated fashion. In particular, we will review the notion of <em>network design spaces</em> that led to the RegNetX/Y models :cite:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li>What are the major differences between the Inception block in :numref:<code>fig_inception</code> and the residual block? How do they compare in terms of computation, accuracy, and the classes of functions they can describe?</li><li>Refer to Table 1 in the ResNet paper :cite:<code>He.Zhang.Ren.ea.2016</code> to implement different variants of the network. </li><li>For deeper networks, ResNet introduces a &quot;bottleneck&quot; architecture to reduce model complexity. Try to implement it.</li><li>In subsequent versions of ResNet, the authors changed the &quot;convolution, batch normalization, and activation&quot; structure to the &quot;batch normalization, activation, and convolution&quot; structure. Make this improvement yourself. See Figure 1 in :citet:<code>He.Zhang.Ren.ea.2016*1</code> for details.</li><li>Why can&#39;t we just increase the complexity of functions without bound, even if the function classes are nested?</li></ol><pre><code class="language-julia hljs">## 3.
struct ResNetBottleNeck{N} &lt;: AbstractModel
    net::N 
end

Flux.@layer ResNetBottleNeck 

(rbn::ResNetBottleNeck)(x) = r.net(x)


function ResNetBottleNeck(channel_in, bot_mul)
    bottled_channels = Int(round(channel_in*bot_mul))
    main_net = Chain(
        Conv((1,1), channel_in =&gt; bottled_channels),
        BatchNorm(bottled_channels, relu),
        Conv((3,3), bottled_channels =&gt; bottled_channels, pad = 1, stride = 1),
        BatchNorm(bottled_channels, relu),
        Conv((1,1), bottled_channels =&gt; channel_in),
        BatchNorm(channel_in, relu),
    )
    net = Parallel(+, Flux.identity, main_net)
    ResNetBottleNeck(net)
end

b_neck = ResNetBottleNeck(64, 0.25)</code></pre><pre><code class="nohighlight hljs">ResNetBottleNeck(
  Parallel(
    +,
    identity,
    Chain(
      Conv((1, 1), 64 =&gt; 16),           # 1_040 parameters
      BatchNorm(16, relu),              # 32 parameters, plus 32
      Conv((3, 3), 16 =&gt; 16, pad=1),    # 2_320 parameters
      BatchNorm(16, relu),              # 32 parameters, plus 32
      Conv((1, 1), 16 =&gt; 64),           # 1_088 parameters
      BatchNorm(64, relu),              # 128 parameters, plus 128
    ),
  ),
)         # Total: 12 trainable arrays, 4_640 parameters,
          # plus 6 non-trainable, 192 parameters, summarysize 20.094 KiB.</code></pre><pre><code class="language-julia hljs">Flux.outputsize(b_neck.net, (96, 96, 64, 1))</code></pre><pre><code class="nohighlight hljs">(96, 96, 64, 1)</code></pre><pre><code class="language-julia hljs">## 4.
struct ResidualBlockNew{N} &lt;: AbstractModel
    net::N 
end
Flux.@layer ResidualBlockNew
(rbn::ResidualBlockNew)(x) = rbn.net(x)

function ResidualBlockNew(channel_in::Int; num_channels = channel_in, use_1x1conv = !isequal(channel_in, num_channels), stride = 1)
    main_net = Chain(
        BatchNorm(channel_in, relu),
        Conv((3,3), channel_in =&gt; num_channels, stride = stride, pad = 1),
        BatchNorm(num_channels, relu),
        Conv((3,3), num_channels =&gt; num_channels, stride = 1, pad = 1),
    )
    shortcut_connection = if use_1x1conv
        Conv((1,1), channel_in =&gt; num_channels, stride = stride)
    else
        Flux.identity
    end
    net = Parallel(+, shortcut_connection, main_net)
    ResidualBlockNew(net)
end

rb_new = ResidualBlockNew(64; num_channels = 128, stride = 2)</code></pre><pre><code class="nohighlight hljs">ResidualBlockNew(
  Parallel(
    +,
    Conv((1, 1), 64 =&gt; 128, stride=2),  # 8_320 parameters
    Chain(
      BatchNorm(64, relu),              # 128 parameters, plus 128
      Conv((3, 3), 64 =&gt; 128, pad=1, stride=2),  # 73_856 parameters
      BatchNorm(128, relu),             # 256 parameters, plus 256
      Conv((3, 3), 128 =&gt; 128, pad=1),  # 147_584 parameters
    ),
  ),
)         # Total: 10 trainable arrays, 230_144 parameters,
          # plus 4 non-trainable, 384 parameters, summarysize 901.500 KiB.</code></pre><pre><code class="language-julia hljs">Flux.outputsize(rb_new.net, (96, 96, 64, 1))</code></pre><pre><code class="nohighlight hljs">(48, 48, 128, 1)</code></pre><pre><code class="language-julia hljs"></code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MCNN_5/">« -</a><a class="docs-footer-nextpage" href="../MCNN_7/">Densely Connected Networks (DenseNet) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.12.0 on <span class="colophon-date" title="Sunday 15 June 2025 19:32">Sunday 15 June 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
